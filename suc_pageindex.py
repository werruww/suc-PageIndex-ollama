# -*- coding: utf-8 -*-
"""suc_PageIndex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ATgFXnnY_WVA7uVV6nkoPxUM_QGL_v-U
"""









"""### https://github.com/VectifyAI/PageIndex/tree/main

### https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb
"""

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &


!ollama pull llama3

!nohup ollama serve &

!ollama list

# Commented out IPython magic to ensure Python compatibility.
# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

# Commented out IPython magic to ensure Python compatibility.
# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a very basic way to simulate a tree from raw text for demonstration.
        # In a real PageIndex setup, this is a sophisticated process.
        # Here, we'll just make the entire document a single node for simplicity
        # or split by major headings if available (e.g., from PDF parsing).

        # For the provided academic paper structure, we'll try to mimic it slightly.
        # This is a placeholder and would ideally use a more robust parsing.

        # Example: Try to split by common section headers for academic papers
        sections = []
        lines = text_content.split('\n')
        current_section = []
        section_title = "Document Overview"
        node_counter = 0

        # Attempt to find common academic paper headings
        # This is a heuristic and might not work perfectly for all PDFs
        section_patterns = [
            "Abstract", "Contents", "1. Introduction", "2. Approach", "3. Experiment",
            "4. Discussion", "5. Conclusion, Limitations, and Future Work",
            "References", "Appendix"
        ]

        # Use a more robust (though still simple) parsing for structure
        parsed_sections = []
        current_section_title = None
        current_section_content = []
        current_page_index = 1 # Simplified page index

        for line in lines:
            stripped_line = line.strip()
            is_new_section = False
            for pattern in section_patterns:
                if stripped_line.startswith(pattern) and len(stripped_line) < 100: # Heuristic for title
                    if current_section_title and current_section_content:
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                    current_section_title = pattern
                    current_section_content = [stripped_line]
                    is_new_section = True
                    # A very rough way to simulate page breaks for a PDF
                    if "Conclusion" in pattern or "References" in pattern:
                        current_page_index += 5 # Simulate a jump
                    else:
                        current_page_index += 1
                    break
            if not is_new_section:
                current_section_content.append(line)

        if current_section_title and current_section_content:
             parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Convert simple list of sections into a nested tree structure
        # For simplicity, we'll just make it a flat list under a root for this dummy.
        tree_nodes = []
        node_map_for_subnodes = {} # To easily find parents

        root_node = {
            "title": "DeepSeek-R1: Incentivizing Reasoning Cap...",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# DeepSeek-R1: Incentivizing Reasoning C...",
            "nodes": [],
            "page_index": 1 # Root starts at page 1
        }

        for section in parsed_sections:
            # Generate a simple summary (first few words)
            summary = section["text"].split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            # Assign a more structured node_id for consistency with original notebook
            # This is still simplified. Real PageIndex would use more robust IDing.
            original_node_id_map = {
                "Abstract": "0001", "Contents": "0002", "1. Introduction": "0003",
                "1.1. Contributions": "0004", "1.2. Summary of Evaluation Results": "0005",
                "2. Approach": "0006", "2.1. Overview": "0007", "2.2. DeepSeek-R1-Zero: Reinforcement Lea...": "0008",
                "2.2.1. Reinforcement Learning Algorithm": "0009", "2.2.2. Reward Modeling": "0010",
                "2.2.3. Training Template": "0011", "2.2.4. Performance, Self-evolution Proce...": "0012",
                "2.3. DeepSeek-R1: Reinforcement Learning...": "0013", "2.4. Distillation: Empower Small Models ...": "0014",
                "3. Experiment": "0015", "3.1. DeepSeek-R1 Evaluation": "0016", "3.2. Distilled Model Evaluation": "0017",
                "4. Discussion": "0018", "5. Conclusion, Limitations, and Future Work": "0019",
                "References": "0020", "Appendix": "0021", "A. Contributions and Acknowledgments": "0022"
            }
            node_id_suffix = original_node_id_map.get(section["title"], f"{node_counter:04d}")

            node_entry = {
                "title": section["title"],
                "node_id": node_id_suffix,
                "summary": summary,
                "text": section["text"],
                "page_index": section["page_index"]
            }

            # Simple nesting logic based on common patterns
            if section["title"].startswith("1.") and len(section["title"]) > 10 and not section["title"].startswith("1.1.") and not section["title"].startswith("1.2."): # This is a bit weak, just for demo
                 node_map_for_subnodes["1. Introduction"] = node_entry
                 root_node["nodes"].append(node_entry)
            elif section["title"].startswith("1.1.") or section["title"].startswith("1.2."):
                if "1. Introduction" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["1. Introduction"]:
                    node_map_for_subnodes["1. Introduction"]["nodes"] = []
                if "1. Introduction" in node_map_for_subnodes:
                    node_map_for_subnodes["1. Introduction"]["nodes"].append(node_entry)
            elif section["title"].startswith("2.") and len(section["title"]) > 10 and not section["title"].startswith("2.1.") and not section["title"].startswith("2.2.") and not section["title"].startswith("2.3.") and not section["title"].startswith("2.4."):
                node_map_for_subnodes["2. Approach"] = node_entry
                root_node["nodes"].append(node_entry)
            elif section["title"].startswith("2.1.") or section["title"].startswith("2.2.") or section["title"].startswith("2.3.") or section["title"].startswith("2.4."):
                if "2. Approach" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["2. Approach"]:
                    node_map_for_subnodes["2. Approach"]["nodes"] = []
                if "2. Approach" in node_map_for_subnodes:
                    node_map_for_subnodes["2. Approach"]["nodes"].append(node_entry)
            elif section["title"].startswith("3.") and len(section["title"]) > 10 and not section["title"].startswith("3.1.") and not section["title"].startswith("3.2."):
                 node_map_for_subnodes["3. Experiment"] = node_entry
                 root_node["nodes"].append(node_entry)
            elif section["title"].startswith("3.1.") or section["title"].startswith("3.2."):
                if "3. Experiment" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["3. Experiment"]:
                    node_map_for_subnodes["3. Experiment"]["nodes"] = []
                if "3. Experiment" in node_map_for_subnodes:
                    node_map_for_subnodes["3. Experiment"]["nodes"].append(node_entry)
            elif section["title"].startswith("A."):
                if "Appendix" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["Appendix"]:
                    node_map_for_subnodes["Appendix"]["nodes"] = []
                if "Appendix" in node_map_for_subnodes:
                    node_map_for_subnodes["Appendix"]["nodes"].append(node_entry)
            else:
                root_node["nodes"].append(node_entry) # Add as top-level if no specific parent

        return [root_node] # Return as a list with the root node

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
pdf_url = "https://arxiv.org/pdf/2501.12948.pdf"
pdf_filename = pdf_url.split('/')[-1]
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

# Download the PDF content
print(f"Downloading {pdf_url}...")
response = requests.get(pdf_url)
response.raise_for_status() # Check for request errors

# Convert PDF content to text (simplified for local demo)
# In a real PageIndex client, this would involve robust PDF parsing.
# For this example, we'll try to get text from an existing HTML version if possible
# or just note that a real PDF parser is needed.
# Since the original notebook downloaded a PDF and then submitted it, we'll simulate
# having the text content. A full PDF-to-text conversion is outside this script's scope,
# but for demonstration, we'll assume we have the text content extracted.

# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.
# Since we are focusing on *how* to integrate with Ollama once text is available,
# let's use a placeholder.
# For the purpose of making this runnable, let's create a dummy text file
# representing the content of the PDF.

# Fetch the abstract and intro from arXiv for a more realistic text
arxiv_id = pdf_filename.replace('.pdf', '')
arxiv_html_url = f"https://arxiv.org/html/{arxiv_id}v1.html" # Assuming v1 exists

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    # A more robust solution would be to use a PDF parser.
    # For now, let's grab all paragraphs and headings.
    extracted_text_parts = []
    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    dummy_text_content = """
    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models
    ## Abstract
    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.
    ## 1. Introduction
    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.
    ### 1.1. Contributions
    We propose novel RL algorithms and a distillation process.
    ## 5. Conclusion, Limitations, and Future Work
    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
query = "What are the conclusions in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

https://arxiv.org/pdf/2501.12948

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a very basic way to simulate a tree from raw text for demonstration.
        # In a real PageIndex setup, this is a sophisticated process.
        # Here, we'll just make the entire document a single node for simplicity
        # or split by major headings if available (e.g., from PDF parsing).

        # For the provided academic paper structure, we'll try to mimic it slightly.
        # This is a placeholder and would ideally use a more robust parsing.

        # Example: Try to split by common section headers for academic papers
        sections = []
        lines = text_content.split('\n')
        current_section = []
        section_title = "Document Overview"
        node_counter = 0

        # Attempt to find common academic paper headings
        # This is a heuristic and might not work perfectly for all PDFs
        section_patterns = [
            "Abstract", "Contents", "1. Introduction", "2. Approach", "3. Experiment",
            "4. Discussion", "5. Conclusion, Limitations, and Future Work",
            "References", "Appendix"
        ]

        # Use a more robust (though still simple) parsing for structure
        parsed_sections = []
        current_section_title = None
        current_section_content = []
        current_page_index = 1 # Simplified page index

        for line in lines:
            stripped_line = line.strip()
            is_new_section = False
            for pattern in section_patterns:
                if stripped_line.startswith(pattern) and len(stripped_line) < 100: # Heuristic for title
                    if current_section_title and current_section_content:
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                    current_section_title = pattern
                    current_section_content = [stripped_line]
                    is_new_section = True
                    # A very rough way to simulate page breaks for a PDF
                    if "Conclusion" in pattern or "References" in pattern:
                        current_page_index += 5 # Simulate a jump
                    else:
                        current_page_index += 1
                    break
            if not is_new_section:
                current_section_content.append(line)

        if current_section_title and current_section_content:
             parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Convert simple list of sections into a nested tree structure
        # For simplicity, we'll just make it a flat list under a root for this dummy.
        tree_nodes = []
        node_map_for_subnodes = {} # To easily find parents

        root_node = {
            "title": "DeepSeek-R1: Incentivizing Reasoning Cap...",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# DeepSeek-R1: Incentivizing Reasoning C...",
            "nodes": [],
            "page_index": 1 # Root starts at page 1
        }

        for section in parsed_sections:
            # Generate a simple summary (first few words)
            summary = section["text"].split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            # Assign a more structured node_id for consistency with original notebook
            # This is still simplified. Real PageIndex would use more robust IDing.
            original_node_id_map = {
                "Abstract": "0001", "Contents": "0002", "1. Introduction": "0003",
                "1.1. Contributions": "0004", "1.2. Summary of Evaluation Results": "0005",
                "2. Approach": "0006", "2.1. Overview": "0007", "2.2. DeepSeek-R1-Zero: Reinforcement Lea...": "0008",
                "2.2.1. Reinforcement Learning Algorithm": "0009", "2.2.2. Reward Modeling": "0010",
                "2.2.3. Training Template": "0011", "2.2.4. Performance, Self-evolution Proce...": "0012",
                "2.3. DeepSeek-R1: Reinforcement Learning...": "0013", "2.4. Distillation: Empower Small Models ...": "0014",
                "3. Experiment": "0015", "3.1. DeepSeek-R1 Evaluation": "0016", "3.2. Distilled Model Evaluation": "0017",
                "4. Discussion": "0018", "5. Conclusion, Limitations, and Future Work": "0019",
                "References": "0020", "Appendix": "0021", "A. Contributions and Acknowledgments": "0022"
            }
            node_id_suffix = original_node_id_map.get(section["title"], f"{node_counter:04d}")

            node_entry = {
                "title": section["title"],
                "node_id": node_id_suffix,
                "summary": summary,
                "text": section["text"],
                "page_index": section["page_index"]
            }

            # Simple nesting logic based on common patterns
            if section["title"].startswith("1.") and len(section["title"]) > 10 and not section["title"].startswith("1.1.") and not section["title"].startswith("1.2."): # This is a bit weak, just for demo
                 node_map_for_subnodes["1. Introduction"] = node_entry
                 root_node["nodes"].append(node_entry)
            elif section["title"].startswith("1.1.") or section["title"].startswith("1.2."):
                if "1. Introduction" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["1. Introduction"]:
                    node_map_for_subnodes["1. Introduction"]["nodes"] = []
                if "1. Introduction" in node_map_for_subnodes:
                    node_map_for_subnodes["1. Introduction"]["nodes"].append(node_entry)
            elif section["title"].startswith("2.") and len(section["title"]) > 10 and not section["title"].startswith("2.1.") and not section["title"].startswith("2.2.") and not section["title"].startswith("2.3.") and not section["title"].startswith("2.4."):
                node_map_for_subnodes["2. Approach"] = node_entry
                root_node["nodes"].append(node_entry)
            elif section["title"].startswith("2.1.") or section["title"].startswith("2.2.") or section["title"].startswith("2.3.") or section["title"].startswith("2.4."):
                if "2. Approach" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["2. Approach"]:
                    node_map_for_subnodes["2. Approach"]["nodes"] = []
                if "2. Approach" in node_map_for_subnodes:
                    node_map_for_subnodes["2. Approach"]["nodes"].append(node_entry)
            elif section["title"].startswith("3.") and len(section["title"]) > 10 and not section["title"].startswith("3.1.") and not section["title"].startswith("3.2."):
                 node_map_for_subnodes["3. Experiment"] = node_entry
                 root_node["nodes"].append(node_entry)
            elif section["title"].startswith("3.1.") or section["title"].startswith("3.2."):
                if "3. Experiment" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["3. Experiment"]:
                    node_map_for_subnodes["3. Experiment"]["nodes"] = []
                if "3. Experiment" in node_map_for_subnodes:
                    node_map_for_subnodes["3. Experiment"]["nodes"].append(node_entry)
            elif section["title"].startswith("A."):
                if "Appendix" in node_map_for_subnodes and "nodes" not in node_map_for_subnodes["Appendix"]:
                    node_map_for_subnodes["Appendix"]["nodes"] = []
                if "Appendix" in node_map_for_subnodes:
                    node_map_for_subnodes["Appendix"]["nodes"].append(node_entry)
            else:
                root_node["nodes"].append(node_entry) # Add as top-level if no specific parent

        return [root_node] # Return as a list with the root node

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
pdf_url = "https://arxiv.org/pdf/2501.12948"
pdf_filename = pdf_url.split('/')[-1]
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

# Download the PDF content
print(f"Downloading {pdf_url}...")
response = requests.get(pdf_url)
response.raise_for_status() # Check for request errors

# Convert PDF content to text (simplified for local demo)
# In a real PageIndex client, this would involve robust PDF parsing.
# For this example, we'll try to get text from an existing HTML version if possible
# or just note that a real PDF parser is needed.
# Since the original notebook downloaded a PDF and then submitted it, we'll simulate
# having the text content. A full PDF-to-text conversion is outside this script's scope,
# but for demonstration, we'll assume we have the text content extracted.

# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.
# Since we are focusing on *how* to integrate with Ollama once text is available,
# let's use a placeholder.
# For the purpose of making this runnable, let's create a dummy text file
# representing the content of the PDF.

# Fetch the abstract and intro from arXiv for a more realistic text
arxiv_id = pdf_filename.replace('.pdf', '')
arxiv_html_url = f"https://arxiv.org/html/{arxiv_id}v1.html" # Assuming v1 exists

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    # A more robust solution would be to use a PDF parser.
    # For now, let's grab all paragraphs and headings.
    extracted_text_parts = []
    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    dummy_text_content = """
    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models
    ## Abstract
    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.
    ## 1. Introduction
    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.
    ### 1.1. Contributions
    We propose novel RL algorithms and a distillation process.
    ## 5. Conclusion, Limitations, and Future Work
    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
query = "What are the conclusions in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a very basic way to simulate a tree from raw text for demonstration.
        # In a real PageIndex setup, this is a sophisticated process.

        tree_nodes = []
        root_node = {
            "title": "DeepSeek-R1: Incentivizing Reasoning Cap...",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# DeepSeek-R1: Incentivizing Reasoning C...",
            "nodes": [],
            "page_index": 1 # Root starts at page 1
        }

        # Explicitly parse the minimal dummy text for distinct sections
        sections = {
            "Abstract": [],
            "1. Introduction": [],
            "1.1. Contributions": [],
            "5. Conclusion, Limitations, and Future Work": [],
        }

        current_section = None
        lines = text_content.split('\n')

        # Simple state machine to parse the dummy text
        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith("## Abstract"):
                current_section = "Abstract"
                continue
            elif stripped_line.startswith("## 1. Introduction"):
                current_section = "1. Introduction"
                continue
            elif stripped_line.startswith("### 1.1. Contributions"):
                current_section = "1.1. Contributions"
                continue
            elif stripped_line.startswith("## 5. Conclusion, Limitations, and Future Work"):
                current_section = "5. Conclusion, Limitations, and Future Work"
                continue

            if current_section and stripped_line:
                sections[current_section].append(stripped_line)

        node_counter = 0
        node_map_for_nesting = {} # To hold references for nesting

        # Create nodes for the explicitly parsed sections
        for title_key, content_lines in sections.items():
            if not content_lines:
                continue

            node_counter += 1
            node_id = f"{doc_id}_{node_counter:04d}"
            text = "\n".join(content_lines)
            summary = text.split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            node = {
                "title": title_key,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": 1 # Simplified page index for dummy text
            }

            # Simple nesting logic based on title prefixes
            if title_key == "Abstract":
                root_node["nodes"].append(node)
            elif title_key == "1. Introduction":
                root_node["nodes"].append(node)
                node_map_for_nesting["1. Introduction"] = node
            elif title_key == "1.1. Contributions":
                if "1. Introduction" in node_map_for_nesting:
                    if "nodes" not in node_map_for_nesting["1. Introduction"]:
                        node_map_for_nesting["1. Introduction"]["nodes"] = []
                    node_map_for_nesting["1. Introduction"]["nodes"].append(node)
                else: # Fallback to root if parent not found
                    root_node["nodes"].append(node)
            elif title_key == "5. Conclusion, Limitations, and Future Work":
                root_node["nodes"].append(node)
            else:
                root_node["nodes"].append(node)

        return [root_node] # Return as a list with the root node

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}


    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a very basic way to simulate a tree from raw text for demonstration.
        # In a real PageIndex setup, this is a sophisticated process.

        tree_nodes = []
        root_node = {
            "title": "DeepSeek-R1: Incentivizing Reasoning Cap...",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# DeepSeek-R1: Incentivizing Reasoning C...",
            "nodes": [],
            "page_index": 1 # Root starts at page 1
        }

        # Explicitly parse the minimal dummy text for distinct sections
        sections = {
            "Abstract": [],
            "1. Introduction": [],
            "1.1. Contributions": [],
            "5. Conclusion, Limitations, and Future Work": [],
        }

        current_section = None
        lines = text_content.split('\n')

        # Simple state machine to parse the dummy text
        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith("## Abstract"):
                current_section = "Abstract"
                continue
            elif stripped_line.startswith("## 1. Introduction"):
                current_section = "1. Introduction"
                continue
            elif stripped_line.startswith("### 1.1. Contributions"):
                current_section = "1.1. Contributions"
                continue
            elif stripped_line.startswith("## 5. Conclusion, Limitations, and Future Work"):
                current_section = "5. Conclusion, Limitations, and Future Work"
                continue

            if current_section and stripped_line:
                sections[current_section].append(stripped_line)

        node_counter = 0
        node_map_for_nesting = {} # To hold references for nesting

        # Create nodes for the explicitly parsed sections
        for title_key, content_lines in sections.items():
            if not content_lines:
                continue

            node_counter += 1
            node_id = f"{doc_id}_{node_counter:04d}"
            text = "\n".join(content_lines)
            summary = text.split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            node = {
                "title": title_key,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": 1 # Simplified page index for dummy text
            }

            # Simple nesting logic based on title prefixes
            if title_key == "Abstract":
                root_node["nodes"].append(node)
            elif title_key == "1. Introduction":
                root_node["nodes"].append(node)
                node_map_for_nesting["1. Introduction"] = node
            elif title_key == "1.1. Contributions":
                if "1. Introduction" in node_map_for_nesting:
                    if "nodes" not in node_map_for_nesting["1. Introduction"]:
                        node_map_for_nesting["1. Introduction"]["nodes"] = []
                    node_map_for_nesting["1. Introduction"]["nodes"].append(node)
                else: # Fallback to root if parent not found
                    root_node["nodes"].append(node)
            elif title_key == "5. Conclusion, Limitations, and Future Work":
                root_node["nodes"].append(node)
            else:
                root_node["nodes"].append(node)

        return [root_node] # Return as a list with the root node

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
pdf_url = "https://arxiv.org/pdf/2501.12948"
pdf_filename = pdf_url.split('/')[-1]
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

# Download the PDF content
print(f"Downloading {pdf_url}...")
response = requests.get(pdf_url)
response.raise_for_status() # Check for request errors

# Convert PDF content to text (simplified for local demo)
# In a real PageIndex client, this would involve robust PDF parsing.
# For this example, we'll try to get text from an existing HTML version if possible
# or just note that a real PDF parser is needed.
# Since the original notebook downloaded a PDF and then submitted it, we'll simulate
# having the text content. A full PDF-to-text conversion is outside this script's scope,
# but for demonstration, we'll assume we have the text content extracted.

# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.
# Since we are focusing on *how* to integrate with Ollama once text is available,
# let's use a placeholder.
# For the purpose of making this runnable, let's create a dummy text file
# representing the content of the PDF.

# Fetch the abstract and intro from arXiv for a more realistic text
arxiv_id = pdf_filename.replace('.pdf', '')
arxiv_html_url = f"https://arxiv.org/html/{arxiv_id}v1.html" # Assuming v1 exists

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    # A more robust solution would be to use a PDF parser.
    # For now, let's grab all paragraphs and headings.
    extracted_text_parts = []
    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    dummy_text_content = """
    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models
    ## Abstract
    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.
    ## 1. Introduction
    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.
    ### 1.1. Contributions
    We propose novel RL algorithms and a distillation process.
    ## 5. Conclusion, Limitations, and Future Work
    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
query = "What are the conclusions in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

/content/data/2501.12948



    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models
    ## Abstract
    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.
    ## 1. Introduction
    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.
    ### 1.1. Contributions
    We propose novel RL algorithms and a distillation process.
    ## 5. Conclusion, Limitations, and Future Work
    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.

https://arxiv.org/html/2508.21069v1

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}


    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a very basic way to simulate a tree from raw text for demonstration.
        # In a real PageIndex setup, this is a sophisticated process.

        tree_nodes = []
        root_node = {
            "title": "DeepSeek-R1: Incentivizing Reasoning Cap...",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# DeepSeek-R1: Incentivizing Reasoning C...",
            "nodes": [],
            "page_index": 1 # Root starts at page 1
        }

        # Explicitly parse the minimal dummy text for distinct sections
        sections = {
            "Abstract": [],
            "1. Introduction": [],
            "1.1. Contributions": [],
            "5. Conclusion, Limitations, and Future Work": [],
        }

        current_section = None
        lines = text_content.split('\n')

        # Simple state machine to parse the dummy text
        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith("## Abstract"):
                current_section = "Abstract"
                continue
            elif stripped_line.startswith("## 1. Introduction"):
                current_section = "1. Introduction"
                continue
            elif stripped_line.startswith("### 1.1. Contributions"):
                current_section = "1.1. Contributions"
                continue
            elif stripped_line.startswith("## 5. Conclusion, Limitations, and Future Work"):
                current_section = "5. Conclusion, Limitations, and Future Work"
                continue

            if current_section and stripped_line:
                sections[current_section].append(stripped_line)

        node_counter = 0
        node_map_for_nesting = {} # To hold references for nesting

        # Create nodes for the explicitly parsed sections
        for title_key, content_lines in sections.items():
            if not content_lines:
                continue

            node_counter += 1
            node_id = f"{doc_id}_{node_counter:04d}"
            text = "\n".join(content_lines)
            summary = text.split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            node = {
                "title": title_key,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": 1 # Simplified page index for dummy text
            }

            # Simple nesting logic based on title prefixes
            if title_key == "Abstract":
                root_node["nodes"].append(node)
            elif title_key == "1. Introduction":
                root_node["nodes"].append(node)
                node_map_for_nesting["1. Introduction"] = node
            elif title_key == "1.1. Contributions":
                if "1. Introduction" in node_map_for_nesting:
                    if "nodes" not in node_map_for_nesting["1. Introduction"]:
                        node_map_for_nesting["1. Introduction"]["nodes"] = []
                    node_map_for_nesting["1. Introduction"]["nodes"].append(node)
                else: # Fallback to root if parent not found
                    root_node["nodes"].append(node)
            elif title_key == "5. Conclusion, Limitations, and Future Work":
                root_node["nodes"].append(node)
            else:
                root_node["nodes"].append(node)

        return [root_node] # Return as a list with the root node

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
pdf_url = "https://arxiv.org/pdf/2501.12948"
pdf_filename = pdf_url.split('/')[-1]
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

# Download the PDF content
print(f"Downloading {pdf_url}...")
response = requests.get(pdf_url)
response.raise_for_status() # Check for request errors

# Convert PDF content to text (simplified for local demo)
# In a real PageIndex client, this would involve robust PDF parsing.
# For this example, we'll try to get text from an existing HTML version if possible
# or just note that a real PDF parser is needed.
# Since the original notebook downloaded a PDF and then submitted it, we'll simulate
# having the text content. A full PDF-to-text conversion is outside this script's scope,
# but for demonstration, we'll assume we have the text content extracted.

# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.
# Since we are focusing on *how* to integrate with Ollama once text is available,
# let's use a placeholder.
# For the purpose of making this runnable, let's create a dummy text file
# representing the content of the PDF.

# Fetch the abstract and intro from arXiv for a more realistic text
arxiv_id = pdf_filename.replace('.pdf', '')
arxiv_html_url = f"https://arxiv.org/html/{arxiv_id}v1.html" # Assuming v1 exists

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    # A more robust solution would be to use a PDF parser.
    # For now, let's grab all paragraphs and headings.
    extracted_text_parts = []
    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    dummy_text_content = """
    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models
    ## Abstract
    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.
    ## 1. Introduction
    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.
    ### 1.1. Contributions
    We propose novel RL algorithms and a distillation process.
    ## 5. Conclusion, Limitations, and Future Work
    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
query = "What are the conclusions in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

"""###     """

# Commented out IPython magic to ensure Python compatibility.
# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
#        .

#  PageIndex (    )
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  #      OpenAI  
# %pip install -q requests beautifulsoup4  #   

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient   (   API )
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} #   
        self.trees = {} #      

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') #    
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        #         .
        #        .
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        #          .
        #   PageIndex    .

        tree_nodes = []
        root_node = {
            "title": "DeepSeek-R1: Incentivizing Reasoning Cap...",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# DeepSeek-R1: Incentivizing Reasoning C...",
            "nodes": [],
            "page_index": 1 #      1
        }

        #        
        sections = {
            "Abstract": [],
            "1. Introduction": [],
            "1.1. Contributions": [],
            "5. Conclusion, Limitations, and Future Work": [],
        }

        current_section = None
        lines = text_content.split('\n')

        #      
        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith("## Abstract"):
                current_section = "Abstract"
                continue
            elif stripped_line.startswith("## 1. Introduction"):
                current_section = "1. Introduction"
                continue
            elif stripped_line.startswith("### 1.1. Contributions"):
                current_section = "1.1. Contributions"
                continue
            elif stripped_line.startswith("## 5. Conclusion, Limitations, and Future Work"):
                current_section = "5. Conclusion, Limitations, and Future Work"
                continue

            if current_section and stripped_line:
                sections[current_section].append(stripped_line)

        node_counter = 0
        node_map_for_nesting = {} #     

        #        
        for title_key, content_lines in sections.items():
            if not content_lines:
                continue

            node_counter += 1
            #   node_id        
            if title_key == "Abstract": node_id_suffix = "0001"
            elif title_key == "1. Introduction": node_id_suffix = "0003"
            elif title_key == "1.1. Contributions": node_id_suffix = "0004"
            elif title_key == "5. Conclusion, Limitations, and Future Work": node_id_suffix = "0019"
            else: node_id_suffix = f"{node_counter:04d}" # fallback

            node_id = f"{doc_id}_{node_id_suffix}"
            text = "\n".join(content_lines)
            summary = text.split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            node = {
                "title": title_key,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": 1 #     
            }

            #       
            if title_key == "Abstract":
                root_node["nodes"].append(node)
            elif title_key == "1. Introduction":
                root_node["nodes"].append(node)
                node_map_for_nesting["1. Introduction"] = node
            elif title_key == "1.1. Contributions":
                if "1. Introduction" in node_map_for_nesting:
                    if "nodes" not in node_map_for_nesting["1. Introduction"]:
                        node_map_for_nesting["1. Introduction"]["nodes"] = []
                    node_map_for_nesting["1. Introduction"]["nodes"].append(node)
                else: # Fallback        
                    root_node["nodes"].append(node)
            elif title_key == "5. Conclusion, Limitations, and Future Work":
                root_node["nodes"].append(node)
            else:
                root_node["nodes"].append(node)

        return [root_node] #     

#     ( pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

#  Ollama LLM  
import httpx

async def call_llm(prompt, model="llama3", temperature=0):
    #     Ollama    
    # : ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None #       
        )
        response.raise_for_status() #    HTTP
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1     PageIndex ()
pdf_url = "https://arxiv.org/pdf/2501.12948.pdf"
pdf_filename = pdf_url.split('/')[-1]
pdf_path = os.path.join("data", pdf_filename) #   'data'

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

#   PDF
print(f"Downloading {pdf_url}...")
response = requests.get(pdf_url)
response.raise_for_status() #    

#   PDF   (  )
#   PageIndex       PDF.
#         HTML   
#         PDF .
#        PDF   
#   .  PDF       
#        .

#   PDF      PyPDF2  pdfminer.six  pypopper.
#        Ollama   
#    .
#        
#    PDF.

#     arXiv     
arxiv_id = pdf_filename.replace('.pdf', '')
arxiv_html_url = f"https://arxiv.org/html/{arxiv_id}v1.html" #   v1 

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    #    -       HTML   arXiv
    #        PDF.
    #        .
    extracted_text_parts = []
    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    dummy_text_content = """
    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models
    ## Abstract
    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.
    ## 1. Introduction
    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.
    ### 1.1. Contributions
    We propose novel RL algorithms and a distillation process.
    ## 5. Conclusion, Limitations, and Future Work
    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2     PageIndex   
#         LocalPageIndexClient  
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1  LLM            
query = "What are the conclusions in this document?"

#   'text'       LLM   
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3") #  llama3    

# 2.2     
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        #    LLM    '0019'    doc_id      
        #        "doc_id_0019"
        #       LLM   
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1       
#  node_map      
#    LLM       
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2      
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3") #  llama3    
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---







# Commented out IPython magic to ensure Python compatibility.
# 0.0    
#         .

#  PageIndex (    )
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai
# %pip install -q requests beautifulsoup4 PyPDF2

import os
import json
import PyPDF2
import httpx
import asyncio
from bs4 import BeautifulSoup

# Client   PageIndexClient (  API )
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} #   
        self.trees = {} #    

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_')
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        #      
        sections = []
        lines = text_content.split('\n')
        current_section = []
        section_title = "   "
        node_counter = 0

        section_patterns = [
            "", "", "", "", "",
            "", "", ""
        ]

        parsed_sections = []
        current_section_title = None
        current_section_content = []
        current_page_index = 1

        for line in lines:
            stripped_line = line.strip()
            is_new_section = False
            for pattern in section_patterns:
                if stripped_line.startswith(pattern) and len(stripped_line) < 100:
                    if current_section_title and current_section_content:
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                    current_section_title = pattern
                    current_section_content = [stripped_line]
                    is_new_section = True
                    current_page_index += 1
                    break
            if not is_new_section:
                current_section_content.append(line)

        if current_section_title and current_section_content:
             parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        tree_nodes = []
        node_map_for_subnodes = {}

        root_node = {
            "title": "  ",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "#   ",
            "nodes": [],
            "page_index": 1
        }

        for section in parsed_sections:
            summary = section["text"].split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            node_entry = {
                "title": section["title"],
                "node_id": f"{node_counter:04d}",
                "summary": summary,
                "text": section["text"],
                "page_index": section["page_index"]
            }

            root_node["nodes"].append(node_entry)

        return [root_node]

# utils   
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client  
async def call_llm(prompt, model="llama3", temperature=0):
    #     Ollama   
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None #     
        )
        response.raise_for_status() #    HTTP
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

#    
book_path = "/content/_.pdf"  #     

#    
if not os.path.exists(book_path):
    raise FileNotFoundError(f"     : {book_path}")

# 1.1     PageIndex ()
try:
    #  PyPDF2    PDF 
    with open(book_path, 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)
        extracted_text_parts = []

        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text = page.extract_text()
            if text:
                extracted_text_parts.append(f"---  {page_num+1} ---")
                extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)

    text_path = book_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)

    print(f"    {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('  :', doc_id)

except Exception as e:
    print(f"     PDF: {e}")
    #      
    dummy_text_content = """
    #    
    ## 
          .          .
    ##  :  
              .
    ##  :  
           .
    ## 
           .
    """
    text_path = book_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"      {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('  :', doc_id)

# 1.2     PageIndex 
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\n   :')
    utils.print_tree(tree)
else:
    print("       ...")

# 2.1  LLM            
query = "     "

#   'text'       LLM   
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
      .
         .
             .

: {query}

  :
{json.dumps(tree_without_text, indent=2)}

   JSON :
{{
    "thinking": "<      >",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
   JSON  .     .
"""

print("\n  LLM     (    )...")
tree_search_result = await call_llm(search_prompt, model="llama3") #  llama3     

# 2.2     
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\n :')
    utils.print_wrapped(tree_search_result_json.get('thinking', '  LLM  .'))

    print('\n :')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("  LLM  .")
    for node_id_key in retrieved_node_ids:
        #    LLM    '0019'    doc_id      
        #         "doc_id_0019"
        #       LLM   
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f" : {node['node_id']}\t : {node.get('page_index', 'N/A')}\t : {node['title']}")
        else:
            print(f" : {actual_node_id} (    -   LLM       )")
except json.JSONDecodeError:
    print(f"\n:   LLM JSON     :\n{tree_search_result}")
except Exception as e:
    print(f"\n       : {e}")

# 3.1       
#  node_map      
#    LLM       
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\n :\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("           LLM.")

# 3.2      
if relevant_content_str:
    answer_prompt = f"""
         :\n\n: {query}\n: {relevant_content_str}\n\n        .
    """
    print('\n  LLM   ...')
    answer = await call_llm(answer_prompt, model="llama3") #  llama3     
    print('\n :\n')
    utils.print_wrapped(answer)
else:
    print("\n           .")

""" :

:       

 :          

  PDF:  PyPDF2   PDF 

 :         

 :      

:      

               .      -          .
"""

# Commented out IPython magic to ensure Python compatibility.
# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai
# %pip install -q requests beautifulsoup4 PyPDF2

import os
import json
import PyPDF2
import httpx
import asyncio
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_')
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # Simple implementation to create a tree from text
        sections = []
        lines = text_content.split('\n')
        current_section = []
        section_title = "Document Overview"
        node_counter = 0

        section_patterns = [
            "Introduction", "Chapter", "Section", "Conclusion", "Summary",
            "References", "Appendix", "Index", "Abstract"
        ]

        parsed_sections = []
        current_section_title = None
        current_section_content = []
        current_page_index = 1

        for line in lines:
            stripped_line = line.strip()
            is_new_section = False
            for pattern in section_patterns:
                if stripped_line.startswith(pattern) and len(stripped_line) < 100:
                    if current_section_title and current_section_content:
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                    current_section_title = pattern
                    current_section_content = [stripped_line]
                    is_new_section = True
                    current_page_index += 1
                    break
            if not is_new_section:
                current_section_content.append(line)

        if current_section_title and current_section_content:
             parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        tree_nodes = []
        node_map_for_subnodes = {}

        root_node = {
            "title": "Python Learning Book",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "# Python Learning Book",
            "nodes": [],
            "page_index": 1
        }

        for section in parsed_sections:
            summary = section["text"].split(' ', 20)
            summary = ' '.join(summary[:20]) + "..." if len(summary) > 20 else ' '.join(summary)

            node_entry = {
                "title": section["title"],
                "node_id": f"{node_counter:04d}",
                "summary": summary,
                "text": section["text"],
                "page_index": section["page_index"]
            }

            root_node["nodes"].append(node_entry)

        return [root_node]

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# Specify local book path
book_path = "/content/data/Understanding_Climate_Change.pdf"  # Change this path as needed

# Verify book exists
if not os.path.exists(book_path):
    raise FileNotFoundError(f"Book not found at specified path: {book_path}")

# 1.1 Submit a document for generating PageIndex tree (locally)
try:
    # Use PyPDF2 to extract text from local PDF
    with open(book_path, 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)
        extracted_text_parts = []

        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text = page.extract_text()
            if text:
                extracted_text_parts.append(f"--- Page {page_num+1} ---")
                extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)

    text_path = book_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)

    print(f"Content extracted to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from PDF: {e}")
    # Use default content if extraction fails
    dummy_text_content = """
    # Python Learning Book
    ## Introduction
    Python is a powerful and easy-to-learn programming language. This book aims to help you learn the basics and advanced concepts.
    ## Chapter 1: Python Basics
    In this chapter we will learn about variables, basic types, and basic operations in Python.
    ## Chapter 2: Control Structures
    We will learn about conditions, loops, and iteration in this chapter.
    ## Conclusion
    Python is an excellent language for both beginners and professionals.
    """
    text_path = book_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used default text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

# 1.2 Get the generated PageIndex tree structure
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
query = "What are the causes of climate change in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")

# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")

# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")



Climate_Change

"""##       """

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This function attempts to parse the text content into a hierarchical structure
        # based on common academic paper headings.

        root_node = {
            "title": "Document Root", # General root title
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        # Define common academic section patterns to look for
        section_patterns = [
            ("## Abstract", "Abstract"),
            ("## 1 Introduction", "1. Introduction"),
            ("## 1.1", "1.1"),
            ("## 2 Related Work", "2. Related Work"),
            ("## 3 Method", "3. Method"),
            ("## 3.1", "3.1"),
            ("## 4 Experiments", "4. Experiments"),
            ("## 4.1", "4.1"),
            ("## 5 Conclusion and Future Work", "5. Conclusion and Future Work"),
            ("## References", "References"),
            ("## Appendix", "Appendix")
        ]

        parsed_sections = []
        current_section_title = "Document Root Content" # Default for un-sectioned content
        current_section_content = []
        node_counter = 0
        current_page_index = 1

        lines = text_content.split('\n')

        # First pass: Identify all main sections and their content
        for line in lines:
            stripped_line = line.strip()
            found_new_section = False
            for pattern_prefix, canonical_title in section_patterns:
                if stripped_line.startswith(pattern_prefix):
                    if current_section_content and current_section_title != "Document Root Content":
                        # Save the previous section
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                        current_page_index += 1 # Simulate page increment for new section

                    current_section_title = canonical_title
                    current_section_content = [stripped_line]
                    found_new_section = True
                    break

            if not found_new_section:
                current_section_content.append(line)

        # Add the last section
        if current_section_content:
            parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Second pass: Build the hierarchical tree
        node_map_for_nesting = {root_node["node_id"]: root_node} # Map to easily find parents

        for section_data in parsed_sections:
            node_id = section_data["node_id"]
            title = section_data["title"]
            text = section_data["text"]

            summary_words = text.split(' ', 20)
            summary = ' '.join(summary_words[:20]) + "..." if len(summary_words) > 20 else ' '.join(summary_words)

            node_entry = {
                "title": title,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": section_data["page_index"]
            }
            node_map_for_nesting[node_id] = node_entry # Add to map for potential children

            # Simple heuristic for nesting (based on "1." "1.1." "2." etc.)
            parent_added = False
            if title.startswith("1.") and len(title.split('.')) == 2: # e.g., "1. Introduction"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("1.1") or title.startswith("1.2"): # e.g., "1.1. Contributions"
                intro_node = next((n for n in root_node["nodes"] if n["title"].startswith("1.") and len(n["title"].split('.')) == 2), None)
                if intro_node:
                    if "nodes" not in intro_node: intro_node["nodes"] = []
                    intro_node["nodes"].append(node_entry)
                    parent_added = True
            elif title.startswith("2.") and len(title.split('.')) == 2: # e.g., "2. Related Work"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("3.") and len(title.split('.')) == 2: # e.g., "3. Method"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("4.") and len(title.split('.')) == 2: # e.g., "4. Experiments"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("5.") and len(title.split('.')) == 2: # e.g., "5. Conclusion..."
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title == "Abstract" or title == "References" or title == "Appendix":
                root_node["nodes"].append(node_entry)
                parent_added = True

            if not parent_added and title != "Document Root Content":
                # If not specifically nested, add to root
                root_node["nodes"].append(node_entry)

        # Ensure no duplicate entries in root_node if some sections were implicitly added twice
        final_root_nodes = []
        seen_node_ids = set()
        for node in root_node["nodes"]:
            if node["node_id"] not in seen_node_ids:
                final_root_nodes.append(node)
                seen_node_ids.add(node["node_id"])
        root_node["nodes"] = final_root_nodes

        return [root_node]


# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
# CHANGED: New URL for the document
pdf_url = "https://arxiv.org/pdf/2508.21069.pdf" # This is a placeholder as .pdf might not exist for future
arxiv_html_url = "https://arxiv.org/html/2508.21069v1.html" # Direct HTML link

pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)


print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    extracted_text_parts = []
    # Look for common article structure elements
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    # UPDATED dummy text for the new URL if HTML extraction fails
    dummy_text_content = """
    # Deep Learning for Climate Model Emulation
    ## Abstract
    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.
    ## 1 Introduction
    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.
    ## 2 Related Work
    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.
    ## 3 Method
    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.
    ## 4 Experiments
    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.
    ## 5 Conclusion and Future Work
    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.
    ## References
    [1] Smith et al. Climate Modeling.
    [2] Jones et al. Deep Learning for Earth Systems.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
# CHANGED: New query
query = "What are the main findings of this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

     .
  Ctrl + / (   )  Cmd + / ( macOS).
    #     .              #.

    !          .

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This function attempts to parse the text content into a hierarchical structure
        # based on common academic paper headings.

        root_node = {
            "title": "Document Root", # General root title
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        # Define common academic section patterns to look for
        section_patterns = [
            ("## Abstract", "Abstract"),
            ("## 1 Introduction", "1. Introduction"),
            ("## 1.1", "1.1"),
            ("## 2 Related Work", "2. Related Work"),
            ("## 3 Method", "3. Method"),
            ("## 3.1", "3.1"),
            ("## 4 Experiments", "4. Experiments"),
            ("## 4.1", "4.1"),
            ("## 5 Conclusion and Future Work", "5. Conclusion and Future Work"),
            ("## References", "References"),
            ("## Appendix", "Appendix")
        ]

        parsed_sections = []
        current_section_title = "Document Root Content" # Default for un-sectioned content
        current_section_content = []
        node_counter = 0
        current_page_index = 1

        lines = text_content.split('\n')

        # First pass: Identify all main sections and their content
        for line in lines:
            stripped_line = line.strip()
            found_new_section = False
            for pattern_prefix, canonical_title in section_patterns:
                if stripped_line.startswith(pattern_prefix):
                    if current_section_content and current_section_title != "Document Root Content":
                        # Save the previous section
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                        current_page_index += 1 # Simulate page increment for new section

                    current_section_title = canonical_title
                    current_section_content = [stripped_line]
                    found_new_section = True
                    break

            if not found_new_section:
                current_section_content.append(line)

        # Add the last section
        if current_section_content:
            parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Second pass: Build the hierarchical tree
        node_map_for_nesting = {root_node["node_id"]: root_node} # Map to easily find parents

        for section_data in parsed_sections:
            node_id = section_data["node_id"]
            title = section_data["title"]
            text = section_data["text"]

            summary_words = text.split(' ', 20)
            summary = ' '.join(summary_words[:20]) + "..." if len(summary_words) > 20 else ' '.join(summary_words)

            node_entry = {
                "title": title,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": section_data["page_index"]
            }
            node_map_for_nesting[node_id] = node_entry # Add to map for potential children

            # Simple heuristic for nesting (based on "1." "1.1." "2." etc.)
            parent_added = False
            if title.startswith("1.") and len(title.split('.')) == 2: # e.g., "1. Introduction"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("1.1") or title.startswith("1.2"): # e.g., "1.1. Contributions"
                intro_node = next((n for n in root_node["nodes"] if n["title"].startswith("1.") and len(n["title"].split('.')) == 2), None)
                if intro_node:
                    if "nodes" not in intro_node: intro_node["nodes"] = []
                    intro_node["nodes"].append(node_entry)
                    parent_added = True
            elif title.startswith("2.") and len(title.split('.')) == 2: # e.g., "2. Related Work"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("3.") and len(title.split('.')) == 2: # e.g., "3. Method"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("4.") and len(title.split('.')) == 2: # e.g., "4. Experiments"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("5.") and len(title.split('.')) == 2: # e.g., "5. Conclusion..."
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title == "Abstract" or title == "References" or title == "Appendix":
                root_node["nodes"].append(node_entry)
                parent_added = True

            if not parent_added and title != "Document Root Content":
                # If not specifically nested, add to root
                root_node["nodes"].append(node_entry)

        # Ensure no duplicate entries in root_node if some sections were implicitly added twice
        final_root_nodes = []
        seen_node_ids = set()
        for node in root_node["nodes"]:
            if node["node_id"] not in seen_node_ids:
                final_root_nodes.append(node)
                seen_node_ids.add(node["node_id"])
        root_node["nodes"] = final_root_nodes

        return [root_node]


# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
# CHANGED: New URL for the document
pdf_url = "https://arxiv.org/pdf/2508.21069" # This is a placeholder as .pdf might not exist for future
arxiv_html_url = "https://arxiv.org/html/2508.21069v1" # Direct HTML link

pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)


print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    extracted_text_parts = []
    # Look for common article structure elements
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    # UPDATED dummy text for the new URL if HTML extraction fails
    dummy_text_content = """
    # Deep Learning for Climate Model Emulation
    ## Abstract
    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.
    ## 1 Introduction
    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.
    ## 2 Related Work
    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.
    ## 3 Method
    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.
    ## 4 Experiments
    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.
    ## 5 Conclusion and Future Work
    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.
    ## References
    [1] Smith et al. Climate Modeling.
    [2] Jones et al. Deep Learning for Earth Systems.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
# CHANGED: New query
query = "What are the main findings of this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This function attempts to parse the text content into a hierarchical structure
        # based on common academic paper headings.

        root_node = {
            "title": "Document Root", # General root title
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        # Define common academic section patterns to look for
        section_patterns = [
            ("## Abstract", "Abstract"),
            ("## 1 Introduction", "1. Introduction"),
            ("## 1.1", "1.1"),
            ("## 2 Related Work", "2. Related Work"),
            ("## 3 Method", "3. Method"),
            ("## 3.1", "3.1"),
            ("## 4 Experiments", "4. Experiments"),
            ("## 4.1", "4.1"),
            ("## 5 Conclusion and Future Work", "5. Conclusion and Future Work"),
            ("## References", "References"),
            ("## Appendix", "Appendix")
        ]

        parsed_sections = []
        current_section_title = "Document Root Content" # Default for un-sectioned content
        current_section_content = []
        node_counter = 0
        current_page_index = 1

        lines = text_content.split('\n')

        # First pass: Identify all main sections and their content
        for line in lines:
            stripped_line = line.strip()
            found_new_section = False
            for pattern_prefix, canonical_title in section_patterns:
                if stripped_line.startswith(pattern_prefix):
                    if current_section_content and current_section_title != "Document Root Content":
                        # Save the previous section
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                        current_page_index += 1 # Simulate page increment for new section

                    current_section_title = canonical_title
                    current_section_content = [stripped_line]
                    found_new_section = True
                    break

            if not found_new_section:
                current_section_content.append(line)

        # Add the last section
        if current_section_content:
            parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Second pass: Build the hierarchical tree
        node_map_for_nesting = {root_node["node_id"]: root_node} # Map to easily find parents

        for section_data in parsed_sections:
            node_id = section_data["node_id"]
            title = section_data["title"]
            text = section_data["text"]

            summary_words = text.split(' ', 20)
            summary = ' '.join(summary_words[:20]) + "..." if len(summary_words) > 20 else ' '.join(summary_words)

            node_entry = {
                "title": title,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": section_data["page_index"]
            }
            node_map_for_nesting[node_id] = node_entry # Add to map for potential children

            # Simple heuristic for nesting (based on "1." "1.1." "2." etc.)
            parent_added = False
            if title.startswith("1.") and len(title.split('.')) == 2: # e.g., "1. Introduction"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("1.1") or title.startswith("1.2"): # e.g., "1.1. Contributions"
                intro_node = next((n for n in root_node["nodes"] if n["title"].startswith("1.") and len(n["title"].split('.')) == 2), None)
                if intro_node:
                    if "nodes" not in intro_node: intro_node["nodes"] = []
                    intro_node["nodes"].append(node_entry)
                    parent_added = True
            elif title.startswith("2.") and len(title.split('.')) == 2: # e.g., "2. Related Work"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("3.") and len(title.split('.')) == 2: # e.g., "3. Method"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("4.") and len(title.split('.')) == 2: # e.g., "4. Experiments"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("5.") and len(title.split('.')) == 2: # e.g., "5. Conclusion..."
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title == "Abstract" or title == "References" or title == "Appendix":
                root_node["nodes"].append(node_entry)
                parent_added = True

            if not parent_added and title != "Document Root Content":
                # If not specifically nested, add to root
                root_node["nodes"].append(node_entry)

        # Ensure no duplicate entries in root_node if some sections were implicitly added twice
        final_root_nodes = []
        seen_node_ids = set()
        for node in root_node["nodes"]:
            if node["node_id"] not in seen_node_ids:
                final_root_nodes.append(node)
                seen_node_ids.add(node["node_id"])
        root_node["nodes"] = final_root_nodes

        return [root_node]


# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
# CHANGED: New URL for the document
pdf_url = "https://arxiv.org/pdf/2508.21069" # This is a placeholder as .pdf might not exist for future
arxiv_html_url = "https://arxiv.org/html/2508.21069v1" # Direct HTML link

pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)


print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    extracted_text_parts = []
    # Look for common article structure elements
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    # UPDATED dummy text for the new URL if HTML extraction fails
    dummy_text_content = """
    # Deep Learning for Climate Model Emulation
    ## Abstract
    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.
    ## 1 Introduction
    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.
    ## 2 Related Work
    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.
    ## 3 Method
    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.
    ## 4 Experiments
    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.
    ## 5 Conclusion and Future Work
    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.
    ## References
    [1] Smith et al. Climate Modeling.
    [2] Jones et al. Deep Learning for Earth Systems.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
# CHANGED: New query
query = "What are the main findings of this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

What are the conclusions in this document?

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This function attempts to parse the text content into a hierarchical structure
        # based on common academic paper headings.

        root_node = {
            "title": "Document Root", # General root title
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        # Define common academic section patterns to look for
        section_patterns = [
            ("## Abstract", "Abstract"),
            ("## 1 Introduction", "1. Introduction"),
            ("## 1.1", "1.1"),
            ("## 2 Related Work", "2. Related Work"),
            ("## 3 Method", "3. Method"),
            ("## 3.1", "3.1"),
            ("## 4 Experiments", "4. Experiments"),
            ("## 4.1", "4.1"),
            ("## 5 Conclusion and Future Work", "5. Conclusion and Future Work"),
            ("## References", "References"),
            ("## Appendix", "Appendix")
        ]

        parsed_sections = []
        current_section_title = "Document Root Content" # Default for un-sectioned content
        current_section_content = []
        node_counter = 0
        current_page_index = 1

        lines = text_content.split('\n')

        # First pass: Identify all main sections and their content
        for line in lines:
            stripped_line = line.strip()
            found_new_section = False
            for pattern_prefix, canonical_title in section_patterns:
                if stripped_line.startswith(pattern_prefix):
                    if current_section_content and current_section_title != "Document Root Content":
                        # Save the previous section
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                        current_page_index += 1 # Simulate page increment for new section

                    current_section_title = canonical_title
                    current_section_content = [stripped_line]
                    found_new_section = True
                    break

            if not found_new_section:
                current_section_content.append(line)

        # Add the last section
        if current_section_content:
            parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Second pass: Build the hierarchical tree
        node_map_for_nesting = {root_node["node_id"]: root_node} # Map to easily find parents

        for section_data in parsed_sections:
            node_id = section_data["node_id"]
            title = section_data["title"]
            text = section_data["text"]

            summary_words = text.split(' ', 20)
            summary = ' '.join(summary_words[:20]) + "..." if len(summary_words) > 20 else ' '.join(summary_words)

            node_entry = {
                "title": title,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": section_data["page_index"]
            }
            node_map_for_nesting[node_id] = node_entry # Add to map for potential children

            # Simple heuristic for nesting (based on "1." "1.1." "2." etc.)
            parent_added = False
            if title.startswith("1.") and len(title.split('.')) == 2: # e.g., "1. Introduction"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("1.1") or title.startswith("1.2"): # e.g., "1.1. Contributions"
                intro_node = next((n for n in root_node["nodes"] if n["title"].startswith("1.") and len(n["title"].split('.')) == 2), None)
                if intro_node:
                    if "nodes" not in intro_node: intro_node["nodes"] = []
                    intro_node["nodes"].append(node_entry)
                    parent_added = True
            elif title.startswith("2.") and len(title.split('.')) == 2: # e.g., "2. Related Work"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("3.") and len(title.split('.')) == 2: # e.g., "3. Method"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("4.") and len(title.split('.')) == 2: # e.g., "4. Experiments"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("5.") and len(title.split('.')) == 2: # e.g., "5. Conclusion..."
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title == "Abstract" or title == "References" or title == "Appendix":
                root_node["nodes"].append(node_entry)
                parent_added = True

            if not parent_added and title != "Document Root Content":
                # If not specifically nested, add to root
                root_node["nodes"].append(node_entry)

        # Ensure no duplicate entries in root_node if some sections were implicitly added twice
        final_root_nodes = []
        seen_node_ids = set()
        for node in root_node["nodes"]:
            if node["node_id"] not in seen_node_ids:
                final_root_nodes.append(node)
                seen_node_ids.add(node["node_id"])
        root_node["nodes"] = final_root_nodes

        return [root_node]


# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
# CHANGED: New URL for the document
pdf_url = "https://arxiv.org/pdf/2508.21069" # This is a placeholder as .pdf might not exist for future
arxiv_html_url = "https://arxiv.org/html/2508.21069v1" # Direct HTML link

pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)


print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    extracted_text_parts = []
    # Look for common article structure elements
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    # UPDATED dummy text for the new URL if HTML extraction fails
    dummy_text_content = """
    # Deep Learning for Climate Model Emulation
    ## Abstract
    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.
    ## 1 Introduction
    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.
    ## 2 Related Work
    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.
    ## 3 Method
    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.
    ## 4 Experiments
    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.
    ## 5 Conclusion and Future Work
    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.
    ## References
    [1] Smith et al. Climate Modeling.
    [2] Jones et al. Deep Learning for Earth Systems.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
# CHANGED: New query
query = "What are the conclusions in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

Conclusion

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This function attempts to parse the text content into a hierarchical structure
        # based on common academic paper headings.

        root_node = {
            "title": "Document Root", # General root title
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        # Define common academic section patterns to look for
        section_patterns = [
            ("## Abstract", "Abstract"),
            ("## 1 Introduction", "1. Introduction"),
            ("## 1.1", "1.1"),
            ("## 2 Related Work", "2. Related Work"),
            ("## 3 Method", "3. Method"),
            ("## 3.1", "3.1"),
            ("## 4 Experiments", "4. Experiments"),
            ("## 4.1", "4.1"),
            ("## 5 Conclusion and Future Work", "5. Conclusion and Future Work"),
            ("## References", "References"),
            ("## Appendix", "Appendix")
        ]

        parsed_sections = []
        current_section_title = "Document Root Content" # Default for un-sectioned content
        current_section_content = []
        node_counter = 0
        current_page_index = 1

        lines = text_content.split('\n')

        # First pass: Identify all main sections and their content
        for line in lines:
            stripped_line = line.strip()
            found_new_section = False
            for pattern_prefix, canonical_title in section_patterns:
                if stripped_line.startswith(pattern_prefix):
                    if current_section_content and current_section_title != "Document Root Content":
                        # Save the previous section
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                        current_page_index += 1 # Simulate page increment for new section

                    current_section_title = canonical_title
                    current_section_content = [stripped_line]
                    found_new_section = True
                    break

            if not found_new_section:
                current_section_content.append(line)

        # Add the last section
        if current_section_content:
            parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Second pass: Build the hierarchical tree
        node_map_for_nesting = {root_node["node_id"]: root_node} # Map to easily find parents

        for section_data in parsed_sections:
            node_id = section_data["node_id"]
            title = section_data["title"]
            text = section_data["text"]

            summary_words = text.split(' ', 20)
            summary = ' '.join(summary_words[:20]) + "..." if len(summary_words) > 20 else ' '.join(summary_words)

            node_entry = {
                "title": title,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": section_data["page_index"]
            }
            node_map_for_nesting[node_id] = node_entry # Add to map for potential children

            # Simple heuristic for nesting (based on "1." "1.1." "2." etc.)
            parent_added = False
            if title.startswith("1.") and len(title.split('.')) == 2: # e.g., "1. Introduction"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("1.1") or title.startswith("1.2"): # e.g., "1.1. Contributions"
                intro_node = next((n for n in root_node["nodes"] if n["title"].startswith("1.") and len(n["title"].split('.')) == 2), None)
                if intro_node:
                    if "nodes" not in intro_node: intro_node["nodes"] = []
                    intro_node["nodes"].append(node_entry)
                    parent_added = True
            elif title.startswith("2.") and len(title.split('.')) == 2: # e.g., "2. Related Work"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("3.") and len(title.split('.')) == 2: # e.g., "3. Method"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("4.") and len(title.split('.')) == 2: # e.g., "4. Experiments"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("5.") and len(title.split('.')) == 2: # e.g., "5. Conclusion..."
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title == "Abstract" or title == "References" or title == "Appendix":
                root_node["nodes"].append(node_entry)
                parent_added = True

            if not parent_added and title != "Document Root Content":
                # If not specifically nested, add to root
                root_node["nodes"].append(node_entry)

        # Ensure no duplicate entries in root_node if some sections were implicitly added twice
        final_root_nodes = []
        seen_node_ids = set()
        for node in root_node["nodes"]:
            if node["node_id"] not in seen_node_ids:
                final_root_nodes.append(node)
                seen_node_ids.add(node["node_id"])
        root_node["nodes"] = final_root_nodes

        return [root_node]


# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
# CHANGED: New URL for the document
pdf_url = "https://arxiv.org/pdf/2508.21069" # This is a placeholder as .pdf might not exist for future
arxiv_html_url = "https://arxiv.org/html/2508.21069v1" # Direct HTML link

pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)


print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    extracted_text_parts = []
    # Look for common article structure elements
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    # UPDATED dummy text for the new URL if HTML extraction fails
    dummy_text_content = """
    # Deep Learning for Climate Model Emulation
    ## Abstract
    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.
    ## 1 Introduction
    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.
    ## 2 Related Work
    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.
    ## 3 Method
    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.
    ## 4 Experiments
    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.
    ## 5 Conclusion and Future Work
    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.
    ## References
    [1] Smith et al. Climate Modeling.
    [2] Jones et al. Deep Learning for Earth Systems.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
# CHANGED: New query
query = "What are the Conclusion in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        # In a real scenario, you'd process the document here to generate a tree.
        # For this example, we'll manually create a simplified tree for demonstration.
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This function attempts to parse the text content into a hierarchical structure
        # based on common academic paper headings.

        root_node = {
            "title": "Document Root", # General root title
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        # Define common academic section patterns to look for
        section_patterns = [
            ("## Abstract", "Abstract"),
            ("## 1 Introduction", "1. Introduction"),
            ("## 1.1", "1.1"),
            ("## 2 Related Work", "2. Related Work"),
            ("## 3 Method", "3. Method"),
            ("## 3.1", "3.1"),
            ("## 4 Experiments", "4. Experiments"),
            ("## 4.1", "4.1"),
            ("## 5 Conclusion and Future Work", "5. Conclusion and Future Work"),
            ("## References", "References"),
            ("## Appendix", "Appendix")
        ]

        parsed_sections = []
        current_section_title = "Document Root Content" # Default for un-sectioned content
        current_section_content = []
        node_counter = 0
        current_page_index = 1

        lines = text_content.split('\n')

        # First pass: Identify all main sections and their content
        for line in lines:
            stripped_line = line.strip()
            found_new_section = False
            for pattern_prefix, canonical_title in section_patterns:
                if stripped_line.startswith(pattern_prefix):
                    if current_section_content and current_section_title != "Document Root Content":
                        # Save the previous section
                        parsed_sections.append({
                            "title": current_section_title,
                            "text": "\n".join(current_section_content).strip(),
                            "node_id": f"{doc_id}_{node_counter:04d}",
                            "page_index": current_page_index
                        })
                        node_counter += 1
                        current_page_index += 1 # Simulate page increment for new section

                    current_section_title = canonical_title
                    current_section_content = [stripped_line]
                    found_new_section = True
                    break

            if not found_new_section:
                current_section_content.append(line)

        # Add the last section
        if current_section_content:
            parsed_sections.append({
                "title": current_section_title,
                "text": "\n".join(current_section_content).strip(),
                "node_id": f"{doc_id}_{node_counter:04d}",
                "page_index": current_page_index
            })

        # Second pass: Build the hierarchical tree
        node_map_for_nesting = {root_node["node_id"]: root_node} # Map to easily find parents

        for section_data in parsed_sections:
            node_id = section_data["node_id"]
            title = section_data["title"]
            text = section_data["text"]

            summary_words = text.split(' ', 20)
            summary = ' '.join(summary_words[:20]) + "..." if len(summary_words) > 20 else ' '.join(summary_words)

            node_entry = {
                "title": title,
                "node_id": node_id,
                "summary": summary,
                "text": text,
                "page_index": section_data["page_index"]
            }
            node_map_for_nesting[node_id] = node_entry # Add to map for potential children

            # Simple heuristic for nesting (based on "1." "1.1." "2." etc.)
            parent_added = False
            if title.startswith("1.") and len(title.split('.')) == 2: # e.g., "1. Introduction"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("1.1") or title.startswith("1.2"): # e.g., "1.1. Contributions"
                intro_node = next((n for n in root_node["nodes"] if n["title"].startswith("1.") and len(n["title"].split('.')) == 2), None)
                if intro_node:
                    if "nodes" not in intro_node: intro_node["nodes"] = []
                    intro_node["nodes"].append(node_entry)
                    parent_added = True
            elif title.startswith("2.") and len(title.split('.')) == 2: # e.g., "2. Related Work"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("3.") and len(title.split('.')) == 2: # e.g., "3. Method"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("4.") and len(title.split('.')) == 2: # e.g., "4. Experiments"
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title.startswith("5.") and len(title.split('.')) == 2: # e.g., "5. Conclusion..."
                root_node["nodes"].append(node_entry)
                parent_added = True
            elif title == "Abstract" or title == "References" or title == "Appendix":
                root_node["nodes"].append(node_entry)
                parent_added = True

            if not parent_added and title != "Document Root Content":
                # If not specifically nested, add to root
                root_node["nodes"].append(node_entry)

        # Ensure no duplicate entries in root_node if some sections were implicitly added twice
        final_root_nodes = []
        seen_node_ids = set()
        for node in root_node["nodes"]:
            if node["node_id"] not in seen_node_ids:
                final_root_nodes.append(node)
                seen_node_ids.add(node["node_id"])
        root_node["nodes"] = final_root_nodes

        return [root_node]


# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3:latest", temperature=0):
    # Ensure Ollama server is running and model is pulled
    # Example: ollama run llama3
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None # Disable timeout for potentially long responses
        )
        response.raise_for_status() # Raise an exception for HTTP errors
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}")
        print("Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').")
        return "ERROR: Could not connect to Ollama. Please check your Ollama setup."
    except httpx.HTTPStatusError as e:
        print(f"Error from Ollama server: {e.response.status_code} - {e.response.text}")
        return f"ERROR: Ollama server responded with an error: {e.response.status_code}"
    except json.JSONDecodeError:
        print(f"Error decoding JSON from Ollama: {response.text}")
        return "ERROR: Invalid JSON response from Ollama."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return f"ERROR: An unexpected error occurred: {e}"

# 1.1 Submit a document for generating PageIndex tree (locally)
# CHANGED: New URL for the document
pdf_url = "https://arxiv.org/pdf/2508.21069" # This is a placeholder as .pdf might not exist for future
arxiv_html_url = "https://arxiv.org/html/2508.21069v1" # Direct HTML link

pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html
pdf_path = os.path.join("data", pdf_filename) # Use a 'data' directory

os.makedirs(os.path.dirname(pdf_path), exist_ok=True)


print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    # Extract main content - this will be highly dependent on arXiv's HTML structure
    extracted_text_parts = []
    # Look for common article structure elements
    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            extracted_text_parts.append(text)

    dummy_text_content = "\n\n".join(extracted_text_parts)
    if not dummy_text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    print("Falling back to a very minimal dummy text for tree generation.")
    # UPDATED dummy text for the new URL if HTML extraction fails
    dummy_text_content = """
    # Deep Learning for Climate Model Emulation
    ## Abstract
    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.
    ## 1 Introduction
    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.
    ## 2 Related Work
    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.
    ## 3 Method
    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.
    ## 4 Experiments
    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.
    ## 5 Conclusion and Future Work
    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.
    ## References
    [1] Smith et al. Climate Modeling.
    [2] Jones et al. Deep Learning for Earth Systems.
    """
    text_path = pdf_path.replace('.pdf', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(dummy_text_content)
    print(f"Used minimal dummy text and saved to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)


# 1.2 Get the generated PageIndex tree structure
# This will use the simplified tree generated by our LocalPageIndexClient
if pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("Processing document, please try again later...")

# 2.1 Use LLM for tree search and identify nodes that might contain relevant context
# CHANGED: New query
query = "What are the conclusion in this document?"

# Remove the 'text' field to avoid sending too much data to the LLM for tree search
tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""

print("\nCalling local LLM for tree search (this might take a moment)...")
tree_search_result = await call_llm(search_prompt, model="llama3:latest") # Use llama3 or your preferred local model

# 2.2 Print retrieved nodes and reasoning process
try:
    node_map = utils.create_node_mapping(tree)
    tree_search_result_json = json.loads(tree_search_result)

    print('\nReasoning Process:')
    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))

    print('\nRetrieved Nodes:')
    retrieved_node_ids = tree_search_result_json.get("node_list", [])
    if not retrieved_node_ids:
        print("No nodes retrieved by LLM.")
    for node_id_key in retrieved_node_ids:
        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store
        # For this dummy client, the node_ids are like "doc_id_0019"
        # Let's adjust for the dummy structure if LLM outputs just the number
        actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key

        node = node_map.get(actual_node_id)
        if node:
            print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
        else:
            print(f"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)")
except json.JSONDecodeError:
    print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
except Exception as e:
    print(f"\nAn error occurred during tree search result processing: {e}")


# 3.1 Extract relevant context from retrieved nodes
# Use the node_map to get the full text of the identified nodes
# Re-parse LLM result in case of error in previous block
try:
    retrieved_node_ids = json.loads(tree_search_result).get("node_list", [])
except json.JSONDecodeError:
    retrieved_node_ids = []

relevant_content = []
for node_id_key in retrieved_node_ids:
    actual_node_id = f"{doc_id}_{node_id_key}" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key
    node = node_map.get(actual_node_id)
    if node and 'text' in node:
        relevant_content.append(node['text'])

relevant_content_str = "\n\n".join(relevant_content)

print('\nRetrieved Context:\n')
if relevant_content_str:
    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)
else:
    print("No relevant context found based on LLM's node list.")


# 3.2 Generate answer based on retrieved context
if relevant_content_str:
    answer_prompt = f"""
    Answer the question based on the context:\n\nQuestion: {query}\nContext: {relevant_content_str}\n\nProvide a clear, concise answer based only on the context provided.
    """
    print('\nCalling local LLM for answer generation...')
    answer = await call_llm(answer_prompt, model="llama3:latest") # Use llama3 or your preferred local model
    print('\nGenerated Answer:\n')
    utils.print_wrapped(answer)
else:
    print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---



""":         https://arxiv.org/html/2508.21069v1.  !
:      _generate_simple_tree_from_text.     " "      ##                  .     ""          .
  (LLM):            ""   ()   (_0003)           .
 :           (    )           .
:
     _generate_simple_tree_from_text            .         ( "Abstract", "Introduction", "Conclusion")        ##.
                     .
   :
   .      LocalPageIndexClient   _generate_simple_tree_from_text.
code
Python

"""

# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    # --- START OF MAJOR CORRECTION ---
    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a more robust parser for the actual scraped HTML text.
        root_node = {
            "title": "Document Root",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        parsed_sections = []
        current_title = "Preamble"  # Content before the first proper header
        current_content = []

        # More flexible headers to look for in the scraped text
        # These are keywords that often start a section title.
        header_keywords = [
            "Abstract", "Introduction", "Related Work", "Method",
            "Experiments", "Conclusion", "References", "Appendix"
        ]

        lines = text_content.split('\n')
        for line in lines:
            stripped = line.strip()
            if not stripped:
                continue

            found_header = None
            # Heuristic: A line is likely a header if it's short and starts with a keyword or a number.
            if len(stripped.split()) < 10:
                for keyword in header_keywords:
                    # Check for "1 Introduction", "5. Conclusion", "Abstract", etc.
                    if (stripped.startswith(keyword) or (len(stripped) > 1 and stripped[0].isdigit() and keyword in stripped)):
                        found_header = stripped
                        break

            if found_header:
                # If we found a new header, save the previous section's content
                if current_content:
                    parsed_sections.append({
                        "title": current_title,
                        "text": "\n".join(current_content).strip()
                    })
                # Start the new section
                current_title = found_header
                current_content = [] # Reset content buffer
            else:
                # This line is content for the current section
                current_content.append(line)

        # After the loop, save the very last section
        if current_content:
            parsed_sections.append({
                "title": current_title,
                "text": "\n".join(current_content).strip()
            })

        # Now, build the tree from the correctly parsed sections
        node_counter = 1
        for i, section in enumerate(parsed_sections):
            text = section["text"]
            summary = ' '.join(text.split()[:20]) + '...' if len(text.split()) > 20 else text

            node_entry = {
                "title": section["title"],
                "node_id": f"{doc_id}_{i+1:04d}", # Simple, sequential node IDs
                "summary": summary,
                "text": text,
                "page_index": i + 1
            }
            root_node["nodes"].append(node_entry)

        return [root_node]
    # --- END OF MAJOR CORRECTION ---

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3", temperature=0):
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None
        )
        response.raise_for_status()
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}\nPlease ensure Ollama is running.")
        return '{"thinking": "Error: Could not connect to Ollama.", "node_list": []}'
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return '{"thinking": "An unexpected error occurred.", "node_list": []}'

# 1.1 Submit a document for generating PageIndex tree (locally)
arxiv_html_url = "https://arxiv.org/html/2508.21069v1"
doc_name = arxiv_html_url.split('/')[-1]
doc_path = os.path.join("data", doc_name)

os.makedirs(os.path.dirname(doc_path), exist_ok=True)

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    article_body = soup.find('div', class_='ltx_page_content')
    if article_body:
        text_content = article_body.get_text(separator='\n', strip=True)
    else: # Fallback if specific class not found
        text_content = soup.get_text(separator='\n', strip=True)

    if not text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = doc_path.replace('.html', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    doc_id = None # Ensure doc_id is None if setup fails

# 1.2 Get the generated PageIndex tree structure
if 'doc_id' in locals() and doc_id and pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("\nSkipping further steps because document processing failed.")
    tree = None

if tree:
    # 2.1 Use LLM for tree search
    query = "What are the main findings of this document?"
    tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

    search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id and a title. Your task is to find all nodes that are likely to contain the answer to the question. Focus on sections like 'Abstract', 'Experiments', 'Results', or 'Conclusion'.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply ONLY with a JSON object in the following format, with no other text:
{{
    "thinking": "<Your brief thinking process on which nodes are relevant>",
    "node_list": ["<full_node_id_1>", "<full_node_id_2>"]
}}
"""

    print("\nCalling local LLM for tree search (this might take a moment)...")
    tree_search_result = await call_llm(search_prompt, model="llama3:latest")

    # 2.2 Print retrieved nodes and reasoning process
    try:
        node_map = utils.create_node_mapping(tree)
        tree_search_result_json = json.loads(tree_search_result)

        print('\nReasoning Process:')
        utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided.'))

        print('\nRetrieved Nodes:')
        retrieved_node_ids = tree_search_result_json.get("node_list", [])
        if not retrieved_node_ids:
            print("No nodes retrieved by LLM.")

        for node_id in retrieved_node_ids:
            node = node_map.get(node_id)
            if node:
                print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
            else:
                print(f"Node ID: {node_id} (Not found in map - LLM may have hallucinated or ID format is mismatched)")

    except json.JSONDecodeError:
        print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
        retrieved_node_ids = []
    except Exception as e:
        print(f"\nAn error occurred during tree search result processing: {e}")
        retrieved_node_ids = []

    # 3.1 Extract relevant context
    relevant_content = []
    if retrieved_node_ids:
        for node_id in retrieved_node_ids:
            node = node_map.get(node_id)
            if node and 'text' in node:
                relevant_content.append(f"--- From Section: {node['title']} ---\n{node['text']}")

    relevant_content_str = "\n\n".join(relevant_content)

    print('\nRetrieved Context:\n')
    if relevant_content_str:
        utils.print_wrapped(relevant_content_str[:1500] + '...' if len(relevant_content_str) > 1500 else relevant_content_str)
    else:
        print("No relevant context found based on LLM's node list.")

    # 3.2 Generate answer
    if relevant_content_str:
        answer_prompt = f"Based ONLY on the following context, what are the main findings of the document?\n\nContext:\n{relevant_content_str}\n\nAnswer:"

        print('\nCalling local LLM for answer generation...')
        answer = await call_llm(answer_prompt, model="llama3:latest")
        print('\nGenerated Answer:\n')
        utils.print_wrapped(answer)
    else:
        print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

!rm -rf /content/data



# --- START OF COLAB CELL ---

# 0.0 Setup Environment and Install Dependencies
# This cell will install necessary packages and prepare the environment.

# Install PageIndex (if not already installed)
# %pip install -q --upgrade pageindex
# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers
# %pip install -q requests beautifulsoup4  # For document downloading and parsing

import os
import requests
import json
from bs4 import BeautifulSoup

# Dummy PageIndexClient for local execution (no actual API calls)
class LocalPageIndexClient:
    def __init__(self, api_key=None):
        print("Using dummy LocalPageIndexClient for local execution.")
        self.documents = {} # Store document content
        self.trees = {} # Store generated tree structures

    def submit_document(self, file_path):
        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents[doc_id] = f.read()
        print(f"Document submitted locally: {doc_id}")
        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)
        return {"doc_id": doc_id}

    def is_retrieval_ready(self, doc_id):
        return doc_id in self.trees

    def get_tree(self, doc_id, node_summary=True):
        return {"result": self.trees.get(doc_id, [])}

    # --- START OF MAJOR CORRECTION ---
    def _generate_simple_tree_from_text(self, text_content, doc_id):
        # This is a more robust parser for the actual scraped HTML text.
        root_node = {
            "title": "Document Root",
            "node_id": f"{doc_id}_0000",
            "prefix_summary": "Root of the document structure.",
            "nodes": [],
            "page_index": 1
        }

        parsed_sections = []
        current_title = "Preamble"  # Content before the first proper header
        current_content = []

        # More flexible headers to look for in the scraped text
        # These are keywords that often start a section title.
        header_keywords = [
            "Abstract", "Introduction", "Related Work", "Method",
            "Experiments", "Conclusion", "References", "Appendix"
        ]

        lines = text_content.split('\n')
        for line in lines:
            stripped = line.strip()
            if not stripped:
                continue

            found_header = None
            # Heuristic: A line is likely a header if it's short and starts with a keyword or a number.
            if len(stripped.split()) < 10:
                for keyword in header_keywords:
                    # Check for "1 Introduction", "5. Conclusion", "Abstract", etc.
                    if (stripped.startswith(keyword) or (len(stripped) > 1 and stripped[0].isdigit() and keyword in stripped)):
                        found_header = stripped
                        break

            if found_header:
                # If we found a new header, save the previous section's content
                if current_content:
                    parsed_sections.append({
                        "title": current_title,
                        "text": "\n".join(current_content).strip()
                    })
                # Start the new section
                current_title = found_header
                current_content = [] # Reset content buffer
            else:
                # This line is content for the current section
                current_content.append(line)

        # After the loop, save the very last section
        if current_content:
            parsed_sections.append({
                "title": current_title,
                "text": "\n".join(current_content).strip()
            })

        # Now, build the tree from the correctly parsed sections
        node_counter = 1
        for i, section in enumerate(parsed_sections):
            text = section["text"]
            summary = ' '.join(text.split()[:20]) + '...' if len(text.split()) > 20 else text

            node_entry = {
                "title": section["title"],
                "node_id": f"{doc_id}_{i+1:04d}", # Simple, sequential node IDs
                "summary": summary,
                "text": text,
                "page_index": i + 1
            }
            root_node["nodes"].append(node_entry)

        return [root_node]
    # --- END OF MAJOR CORRECTION ---

# Dummy utils for local execution (mimicking pageindex.utils)
class LocalUtils:
    @staticmethod
    def print_tree(tree, indent=0):
        for node in tree:
            prefix = "  " * indent
            print(f"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})")
            if "nodes" in node:
                LocalUtils.print_tree(node["nodes"], indent + 1)

    @staticmethod
    def remove_fields(obj, fields):
        if isinstance(obj, dict):
            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}
        elif isinstance(obj, list):
            return [LocalUtils.remove_fields(elem, fields) for elem in obj]
        else:
            return obj

    @staticmethod
    def create_node_mapping(tree, node_map=None):
        if node_map is None:
            node_map = {}
        for node in tree:
            node_map[node.get('node_id')] = node
            if "nodes" in node:
                LocalUtils.create_node_mapping(node["nodes"], node_map)
        return node_map

    @staticmethod
    def print_wrapped(text, width=80):
        import textwrap
        print(textwrap.fill(text, width=width))

pi_client = LocalPageIndexClient()
utils = LocalUtils()

# Ollama LLM client for local inference
import httpx

async def call_llm(prompt, model="llama3", temperature=0):
    client = httpx.AsyncClient(base_url="http://localhost:11434")
    try:
        response = await client.post(
            "/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {"temperature": temperature}
            },
            timeout=None
        )
        response.raise_for_status()
        return response.json()["message"]["content"].strip()
    except httpx.ConnectError as e:
        print(f"Error connecting to Ollama: {e}\nPlease ensure Ollama is running.")
        return '{"thinking": "Error: Could not connect to Ollama.", "node_list": []}'
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return '{"thinking": "An unexpected error occurred.", "node_list": []}'

# 1.1 Submit a document for generating PageIndex tree (locally)
arxiv_html_url = "https://arxiv.org/html/2508.21069v1"
doc_name = arxiv_html_url.split('/')[-1]
doc_path = os.path.join("data", doc_name)

os.makedirs(os.path.dirname(doc_path), exist_ok=True)

print(f"Attempting to fetch text from {arxiv_html_url} for content simulation...")
try:
    html_response = requests.get(arxiv_html_url)
    html_response.raise_for_status()
    soup = BeautifulSoup(html_response.text, 'html.parser')

    article_body = soup.find('div', class_='ltx_page_content')
    if article_body:
        text_content = article_body.get_text(separator='\n', strip=True)
    else: # Fallback if specific class not found
        text_content = soup.get_text(separator='\n', strip=True)

    if not text_content.strip():
        raise ValueError("Could not extract significant text from arXiv HTML.")

    text_path = doc_path.replace('.html', '.txt')
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(text_content)
    print(f"Extracted content to {text_path}")
    doc_id = pi_client.submit_document(text_path)["doc_id"]
    print('Document Submitted:', doc_id)

except Exception as e:
    print(f"Failed to extract text from arXiv HTML or save to file: {e}")
    doc_id = None # Ensure doc_id is None if setup fails

# 1.2 Get the generated PageIndex tree structure
if 'doc_id' in locals() and doc_id and pi_client.is_retrieval_ready(doc_id):
    tree = pi_client.get_tree(doc_id, node_summary=True)['result']
    print('\nSimplified Tree Structure of the Document:')
    utils.print_tree(tree)
else:
    print("\nSkipping further steps because document processing failed.")
    tree = None

if tree:
    # 2.1 Use LLM for tree search
    query = "What are the main findings of this document?"
    tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])

    search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id and a title. Your task is to find all nodes that are likely to contain the answer to the question. Focus on sections like 'Abstract', 'Experiments', 'Results', or 'Conclusion'.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2)}

Please reply ONLY with a JSON object in the following format, with no other text:
{{
    "thinking": "<Your brief thinking process on which nodes are relevant>",
    "node_list": ["<full_node_id_1>", "<full_node_id_2>"]
}}
"""

    print("\nCalling local LLM for tree search (this might take a moment)...")
    tree_search_result = await call_llm(search_prompt, model="llama3:latest")

    # 2.2 Print retrieved nodes and reasoning process
    try:
        node_map = utils.create_node_mapping(tree)
        tree_search_result_json = json.loads(tree_search_result)

        print('\nReasoning Process:')
        utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided.'))

        print('\nRetrieved Nodes:')
        retrieved_node_ids = tree_search_result_json.get("node_list", [])
        if not retrieved_node_ids:
            print("No nodes retrieved by LLM.")

        for node_id in retrieved_node_ids:
            node = node_map.get(node_id)
            if node:
                print(f"Node ID: {node['node_id']}\t Page: {node.get('page_index', 'N/A')}\t Title: {node['title']}")
            else:
                print(f"Node ID: {node_id} (Not found in map - LLM may have hallucinated or ID format is mismatched)")

    except json.JSONDecodeError:
        print(f"\nError: LLM did not return valid JSON for tree search result:\n{tree_search_result}")
        retrieved_node_ids = []
    except Exception as e:
        print(f"\nAn error occurred during tree search result processing: {e}")
        retrieved_node_ids = []

    # 3.1 Extract relevant context
    relevant_content = []
    if retrieved_node_ids:
        for node_id in retrieved_node_ids:
            node = node_map.get(node_id)
            if node and 'text' in node:
                relevant_content.append(f"--- From Section: {node['title']} ---\n{node['text']}")

    relevant_content_str = "\n\n".join(relevant_content)

    print('\nRetrieved Context:\n')
    if relevant_content_str:
        utils.print_wrapped(relevant_content_str[:1500] + '...' if len(relevant_content_str) > 1500 else relevant_content_str)
    else:
        print("No relevant context found based on LLM's node list.")

    # 3.2 Generate answer
    if relevant_content_str:
        answer_prompt = f"Based ONLY on the following context, what are the main findings of the document?\n\nContext:\n{relevant_content_str}\n\nAnswer:"

        print('\nCalling local LLM for answer generation...')
        answer = await call_llm(answer_prompt, model="llama3:latest")
        print('\nGenerated Answer:\n')
        utils.print_wrapped(answer)
    else:
        print("\nCannot generate an answer as no relevant context was retrieved.")

# --- END OF COLAB CELL ---

