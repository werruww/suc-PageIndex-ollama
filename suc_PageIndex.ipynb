{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5R16F98AlI-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9o6vBMLKPvxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCa4B7CsFjke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6bSqKmKdFjm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/VectifyAI/PageIndex/tree/main"
      ],
      "metadata": {
        "id": "hrCTmuTkPutw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb"
      ],
      "metadata": {
        "id": "TkD_f2dDPrCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "nohup ollama serve &\n",
        "ollama pull llama3:8b\n",
        "ollama list"
      ],
      "metadata": {
        "id": "yvDb2yB7Fjo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndY5QvpSFlSU",
        "outputId": "7ed07f34-45cf-480f-ed0c-d26cd6503dfc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "\n",
        "\n",
        "!ollama pull llama3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYX32P8FmAO",
        "outputId": "57ae415a-ffa7-4e2c-c176-ea29a075a5e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa_S40-HIxSd",
        "outputId": "80fc5d1e-07c7-41bf-9762-720bdc2978f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0HAx12BGPR0",
        "outputId": "746da057-7072-47b1-8839-e4a0e8701204"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME             ID              SIZE      MODIFIED       \n",
            "llama3:latest    365c0bd3c000    4.7 GB    13 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PageIndex (if not already installed)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "%pip install -q requests beautifulsoup4  # For document downloading and parsing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1JdmtJDF7Zp",
        "outputId": "027696a9-48c3-49fd-9d77-f56dd9465d9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/812.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m522.2/812.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "%pip install -q requests beautifulsoup4  # For document downloading and parsing"
      ],
      "metadata": {
        "id": "aBrAwrgjGNUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "        # Here, we'll just make the entire document a single node for simplicity\n",
        "        # or split by major headings if available (e.g., from PDF parsing).\n",
        "\n",
        "        # For the provided academic paper structure, we'll try to mimic it slightly.\n",
        "        # This is a placeholder and would ideally use a more robust parsing.\n",
        "\n",
        "        # Example: Try to split by common section headers for academic papers\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"Document Overview\"\n",
        "        node_counter = 0\n",
        "\n",
        "        # Attempt to find common academic paper headings\n",
        "        # This is a heuristic and might not work perfectly for all PDFs\n",
        "        section_patterns = [\n",
        "            \"Abstract\", \"Contents\", \"1. Introduction\", \"2. Approach\", \"3. Experiment\",\n",
        "            \"4. Discussion\", \"5. Conclusion, Limitations, and Future Work\",\n",
        "            \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        # Use a more robust (though still simple) parsing for structure\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1 # Simplified page index\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100: # Heuristic for title\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    # A very rough way to simulate page breaks for a PDF\n",
        "                    if \"Conclusion\" in pattern or \"References\" in pattern:\n",
        "                        current_page_index += 5 # Simulate a jump\n",
        "                    else:\n",
        "                        current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Convert simple list of sections into a nested tree structure\n",
        "        # For simplicity, we'll just make it a flat list under a root for this dummy.\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {} # To easily find parents\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            # Generate a simple summary (first few words)\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            # Assign a more structured node_id for consistency with original notebook\n",
        "            # This is still simplified. Real PageIndex would use more robust IDing.\n",
        "            original_node_id_map = {\n",
        "                \"Abstract\": \"0001\", \"Contents\": \"0002\", \"1. Introduction\": \"0003\",\n",
        "                \"1.1. Contributions\": \"0004\", \"1.2. Summary of Evaluation Results\": \"0005\",\n",
        "                \"2. Approach\": \"0006\", \"2.1. Overview\": \"0007\", \"2.2. DeepSeek-R1-Zero: Reinforcement Lea...\": \"0008\",\n",
        "                \"2.2.1. Reinforcement Learning Algorithm\": \"0009\", \"2.2.2. Reward Modeling\": \"0010\",\n",
        "                \"2.2.3. Training Template\": \"0011\", \"2.2.4. Performance, Self-evolution Proce...\": \"0012\",\n",
        "                \"2.3. DeepSeek-R1: Reinforcement Learning...\": \"0013\", \"2.4. Distillation: Empower Small Models ...\": \"0014\",\n",
        "                \"3. Experiment\": \"0015\", \"3.1. DeepSeek-R1 Evaluation\": \"0016\", \"3.2. Distilled Model Evaluation\": \"0017\",\n",
        "                \"4. Discussion\": \"0018\", \"5. Conclusion, Limitations, and Future Work\": \"0019\",\n",
        "                \"References\": \"0020\", \"Appendix\": \"0021\", \"A. Contributions and Acknowledgments\": \"0022\"\n",
        "            }\n",
        "            node_id_suffix = original_node_id_map.get(section[\"title\"], f\"{node_counter:04d}\")\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": node_id_suffix,\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on common patterns\n",
        "            if section[\"title\"].startswith(\"1.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"1.1.\") and not section[\"title\"].startswith(\"1.2.\"): # This is a bit weak, just for demo\n",
        "                 node_map_for_subnodes[\"1. Introduction\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"1.1.\") or section[\"title\"].startswith(\"1.2.\"):\n",
        "                if \"1. Introduction\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"1. Introduction\"]:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"] = []\n",
        "                if \"1. Introduction\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"2.1.\") and not section[\"title\"].startswith(\"2.2.\") and not section[\"title\"].startswith(\"2.3.\") and not section[\"title\"].startswith(\"2.4.\"):\n",
        "                node_map_for_subnodes[\"2. Approach\"] = node_entry\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.1.\") or section[\"title\"].startswith(\"2.2.\") or section[\"title\"].startswith(\"2.3.\") or section[\"title\"].startswith(\"2.4.\"):\n",
        "                if \"2. Approach\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"2. Approach\"]:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"] = []\n",
        "                if \"2. Approach\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"3.1.\") and not section[\"title\"].startswith(\"3.2.\"):\n",
        "                 node_map_for_subnodes[\"3. Experiment\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.1.\") or section[\"title\"].startswith(\"3.2.\"):\n",
        "                if \"3. Experiment\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"3. Experiment\"]:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"] = []\n",
        "                if \"3. Experiment\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"A.\"):\n",
        "                if \"Appendix\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"Appendix\"]:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"] = []\n",
        "                if \"Appendix\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"].append(node_entry)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node_entry) # Add as top-level if no specific parent\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4lRFxGOGHVN",
        "outputId": "125b058a-594b-46b9-c566-61232b4e8f66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948.pdf...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948.txt\n",
            "Document submitted locally: 2501_12948_txt\n",
            "Document Submitted: 2501_12948_txt\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_txt_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in this document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_txt_0000\t Page: 1\t Title: DeepSeek-R1: Incentivizing Reasoning Cap...\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://arxiv.org/pdf/2501.12948"
      ],
      "metadata": {
        "id": "WQ3WWIjrIPkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "        # Here, we'll just make the entire document a single node for simplicity\n",
        "        # or split by major headings if available (e.g., from PDF parsing).\n",
        "\n",
        "        # For the provided academic paper structure, we'll try to mimic it slightly.\n",
        "        # This is a placeholder and would ideally use a more robust parsing.\n",
        "\n",
        "        # Example: Try to split by common section headers for academic papers\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"Document Overview\"\n",
        "        node_counter = 0\n",
        "\n",
        "        # Attempt to find common academic paper headings\n",
        "        # This is a heuristic and might not work perfectly for all PDFs\n",
        "        section_patterns = [\n",
        "            \"Abstract\", \"Contents\", \"1. Introduction\", \"2. Approach\", \"3. Experiment\",\n",
        "            \"4. Discussion\", \"5. Conclusion, Limitations, and Future Work\",\n",
        "            \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        # Use a more robust (though still simple) parsing for structure\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1 # Simplified page index\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100: # Heuristic for title\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    # A very rough way to simulate page breaks for a PDF\n",
        "                    if \"Conclusion\" in pattern or \"References\" in pattern:\n",
        "                        current_page_index += 5 # Simulate a jump\n",
        "                    else:\n",
        "                        current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Convert simple list of sections into a nested tree structure\n",
        "        # For simplicity, we'll just make it a flat list under a root for this dummy.\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {} # To easily find parents\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            # Generate a simple summary (first few words)\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            # Assign a more structured node_id for consistency with original notebook\n",
        "            # This is still simplified. Real PageIndex would use more robust IDing.\n",
        "            original_node_id_map = {\n",
        "                \"Abstract\": \"0001\", \"Contents\": \"0002\", \"1. Introduction\": \"0003\",\n",
        "                \"1.1. Contributions\": \"0004\", \"1.2. Summary of Evaluation Results\": \"0005\",\n",
        "                \"2. Approach\": \"0006\", \"2.1. Overview\": \"0007\", \"2.2. DeepSeek-R1-Zero: Reinforcement Lea...\": \"0008\",\n",
        "                \"2.2.1. Reinforcement Learning Algorithm\": \"0009\", \"2.2.2. Reward Modeling\": \"0010\",\n",
        "                \"2.2.3. Training Template\": \"0011\", \"2.2.4. Performance, Self-evolution Proce...\": \"0012\",\n",
        "                \"2.3. DeepSeek-R1: Reinforcement Learning...\": \"0013\", \"2.4. Distillation: Empower Small Models ...\": \"0014\",\n",
        "                \"3. Experiment\": \"0015\", \"3.1. DeepSeek-R1 Evaluation\": \"0016\", \"3.2. Distilled Model Evaluation\": \"0017\",\n",
        "                \"4. Discussion\": \"0018\", \"5. Conclusion, Limitations, and Future Work\": \"0019\",\n",
        "                \"References\": \"0020\", \"Appendix\": \"0021\", \"A. Contributions and Acknowledgments\": \"0022\"\n",
        "            }\n",
        "            node_id_suffix = original_node_id_map.get(section[\"title\"], f\"{node_counter:04d}\")\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": node_id_suffix,\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on common patterns\n",
        "            if section[\"title\"].startswith(\"1.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"1.1.\") and not section[\"title\"].startswith(\"1.2.\"): # This is a bit weak, just for demo\n",
        "                 node_map_for_subnodes[\"1. Introduction\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"1.1.\") or section[\"title\"].startswith(\"1.2.\"):\n",
        "                if \"1. Introduction\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"1. Introduction\"]:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"] = []\n",
        "                if \"1. Introduction\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"2.1.\") and not section[\"title\"].startswith(\"2.2.\") and not section[\"title\"].startswith(\"2.3.\") and not section[\"title\"].startswith(\"2.4.\"):\n",
        "                node_map_for_subnodes[\"2. Approach\"] = node_entry\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.1.\") or section[\"title\"].startswith(\"2.2.\") or section[\"title\"].startswith(\"2.3.\") or section[\"title\"].startswith(\"2.4.\"):\n",
        "                if \"2. Approach\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"2. Approach\"]:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"] = []\n",
        "                if \"2. Approach\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"3.1.\") and not section[\"title\"].startswith(\"3.2.\"):\n",
        "                 node_map_for_subnodes[\"3. Experiment\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.1.\") or section[\"title\"].startswith(\"3.2.\"):\n",
        "                if \"3. Experiment\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"3. Experiment\"]:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"] = []\n",
        "                if \"3. Experiment\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"A.\"):\n",
        "                if \"Appendix\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"Appendix\"]:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"] = []\n",
        "                if \"Appendix\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"].append(node_entry)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node_entry) # Add as top-level if no specific parent\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhDz3j1-IRLl",
        "outputId": "ce12be96-8d17-47d7-9130-79308536bd0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948\n",
            "Document submitted locally: 2501_12948\n",
            "Document Submitted: 2501_12948\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in this document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_0000\t Page: 1\t Title: DeepSeek-R1: Incentivizing Reasoning Cap...\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        # Explicitly parse the minimal dummy text for distinct sections\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # Simple state machine to parse the dummy text\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # To hold references for nesting\n",
        "\n",
        "        # Create nodes for the explicitly parsed sections\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            node_id = f\"{doc_id}_{node_counter:04d}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # Simplified page index for dummy text\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on title prefixes\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback to root if parent not found\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # Return as a list with the root node"
      ],
      "metadata": {
        "id": "HcHw_2noIyt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        # Explicitly parse the minimal dummy text for distinct sections\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # Simple state machine to parse the dummy text\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # To hold references for nesting\n",
        "\n",
        "        # Create nodes for the explicitly parsed sections\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            node_id = f\"{doc_id}_{node_counter:04d}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # Simplified page index for dummy text\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on title prefixes\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback to root if parent not found\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1P-uzYKKgWu",
        "outputId": "2a16299a-6d81-47e8-ee32-5ae2f5526a19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948\n",
            "Document submitted locally: 2501_12948\n",
            "Document Submitted: 2501_12948\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_0000)\n",
            "  Abstract (ID: 2501_12948_0001)\n",
            "  1. Introduction (ID: 2501_12948_0002)\n",
            "    1.1. Contributions (ID: 2501_12948_0003)\n",
            "  5. Conclusion, Limitations, and Future Work (ID: 2501_12948_0004)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking for conclusions in this document, so I will look for\n",
            "nodes that have 'Conclusion' or 'Summary' in their title and also contain words\n",
            "related to summarizing or concluding.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_0004\t Page: 1\t Title: 5. Conclusion, Limitations, and Future Work\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows\n",
            "state-of-the-art results. Distillation to smaller models is promising. Future\n",
            "work involves scaling and exploring new reward functions.\n",
            "\n",
            "Calling local LLM for answer generation...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "The conclusions in this document are:  * Reinforcement learning (RL) is\n",
            "effective for enhancing reasoning. * The DeepSeek-R1 model shows state-of-the-\n",
            "art results. * Distilling knowledge to smaller models is a promising approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/data/2501.12948\n",
        "\n",
        "\n",
        "\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        ""
      ],
      "metadata": {
        "id": "ODVlioteNKx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://arxiv.org/html/2508.21069v1"
      ],
      "metadata": {
        "id": "VOeQordsLsJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KxGAm8ReNNQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        # Explicitly parse the minimal dummy text for distinct sections\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # Simple state machine to parse the dummy text\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # To hold references for nesting\n",
        "\n",
        "        # Create nodes for the explicitly parsed sections\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            node_id = f\"{doc_id}_{node_counter:04d}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # Simplified page index for dummy text\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on title prefixes\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback to root if parent not found\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "id": "MtJeKDuJM4OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###     "
      ],
      "metadata": {
        "id": "-HoLCB2LPjTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "#        .\n",
        "\n",
        "#  PageIndex (    )\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  #      OpenAI  \n",
        "%pip install -q requests beautifulsoup4  #   \n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient   (   API )\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} #   \n",
        "        self.trees = {} #      \n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') #    \n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        #         .\n",
        "        #        .\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        #          .\n",
        "        #   PageIndex    .\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 #      1\n",
        "        }\n",
        "\n",
        "        #        \n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        #      \n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} #     \n",
        "\n",
        "        #        \n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            #   node_id        \n",
        "            if title_key == \"Abstract\": node_id_suffix = \"0001\"\n",
        "            elif title_key == \"1. Introduction\": node_id_suffix = \"0003\"\n",
        "            elif title_key == \"1.1. Contributions\": node_id_suffix = \"0004\"\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\": node_id_suffix = \"0019\"\n",
        "            else: node_id_suffix = f\"{node_counter:04d}\" # fallback\n",
        "\n",
        "            node_id = f\"{doc_id}_{node_id_suffix}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 #     \n",
        "            }\n",
        "\n",
        "            #       \n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback        \n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] #     \n",
        "\n",
        "#     ( pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "#  Ollama LLM  \n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    #     Ollama    \n",
        "    # : ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None #       \n",
        "        )\n",
        "        response.raise_for_status() #    HTTP\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1     PageIndex ()\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) #   'data'\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "#   PDF\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() #    \n",
        "\n",
        "#   PDF   (  )\n",
        "#   PageIndex       PDF.\n",
        "#         HTML   \n",
        "#         PDF .\n",
        "#        PDF   \n",
        "#   .  PDF       \n",
        "#        .\n",
        "\n",
        "#   PDF      PyPDF2  pdfminer.six  pypopper.\n",
        "#        Ollama   \n",
        "#    .\n",
        "#        \n",
        "#    PDF.\n",
        "\n",
        "#     arXiv     \n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" #   v1 \n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    #    -       HTML   arXiv\n",
        "    #        PDF.\n",
        "    #        .\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2     PageIndex   \n",
        "#         LocalPageIndexClient  \n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1  LLM            \n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "#   'text'       LLM   \n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") #  llama3    \n",
        "\n",
        "# 2.2     \n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        #    LLM    '0019'    doc_id      \n",
        "        #        \"doc_id_0019\"\n",
        "        #       LLM   \n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1       \n",
        "#  node_map      \n",
        "#    LLM       \n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2      \n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3\") #  llama3    \n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSMfARVCNPWW",
        "outputId": "dd941481-8a0f-417d-a5a4-0502ec5380d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948.pdf...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948.txt\n",
            "Document submitted locally: 2501_12948_txt\n",
            "Document Submitted: 2501_12948_txt\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_txt_0000)\n",
            "  Abstract (ID: 2501_12948_txt_0001)\n",
            "  1. Introduction (ID: 2501_12948_txt_0003)\n",
            "    1.1. Contributions (ID: 2501_12948_txt_0004)\n",
            "  5. Conclusion, Limitations, and Future Work (ID: 2501_12948_txt_0019)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking for conclusions in this document, so I will look for\n",
            "nodes with titles that contain words like 'Conclusion', 'Summary', or 'Result'.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_txt_0019\t Page: 1\t Title: 5. Conclusion, Limitations, and Future Work\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows\n",
            "state-of-the-art results. Distillation to smaller models is promising. Future\n",
            "work involves scaling and exploring new reward functions.\n",
            "\n",
            "Calling local LLM for answer generation...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "The conclusions in this document are:  * Reinforcement learning (RL) is\n",
            "effective for enhancing reasoning. * The DeepSeek-R1 model shows state-of-the-\n",
            "art results. * Distilling knowledge to smaller models is a promising approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTQyVpmYPaEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAwhcqd7P9j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYNJ7inDP9iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.0    \n",
        "#         .\n",
        "\n",
        "#  PageIndex (    )\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai\n",
        "%pip install -q requests beautifulsoup4 PyPDF2\n",
        "\n",
        "import os\n",
        "import json\n",
        "import PyPDF2\n",
        "import httpx\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Client   PageIndexClient (  API )\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} #   \n",
        "        self.trees = {} #    \n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_')\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        #      \n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"   \"\n",
        "        node_counter = 0\n",
        "\n",
        "        section_patterns = [\n",
        "            \"\", \"\", \"\", \"\", \"\",\n",
        "            \"\", \"\", \"\"\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100:\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {}\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"  \",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"#   \",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{node_counter:04d}\",\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "# utils   \n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client  \n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    #     Ollama   \n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None #     \n",
        "        )\n",
        "        response.raise_for_status() #    HTTP\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "#    \n",
        "book_path = \"/content/_.pdf\"  #     \n",
        "\n",
        "#    \n",
        "if not os.path.exists(book_path):\n",
        "    raise FileNotFoundError(f\"     : {book_path}\")\n",
        "\n",
        "# 1.1     PageIndex ()\n",
        "try:\n",
        "    #  PyPDF2    PDF \n",
        "    with open(book_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        extracted_text_parts = []\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                extracted_text_parts.append(f\"---  {page_num+1} ---\")\n",
        "                extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "\n",
        "    print(f\"    {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('  :', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"     PDF: {e}\")\n",
        "    #      \n",
        "    dummy_text_content = \"\"\"\n",
        "    #    \n",
        "    ## \n",
        "          .          .\n",
        "    ##  :  \n",
        "              .\n",
        "    ##  :  \n",
        "           .\n",
        "    ## \n",
        "           .\n",
        "    \"\"\"\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"      {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('  :', doc_id)\n",
        "\n",
        "# 1.2     PageIndex \n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\n   :')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"       ...\")\n",
        "\n",
        "# 2.1  LLM            \n",
        "query = \"     \"\n",
        "\n",
        "#   'text'       LLM   \n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "      .\n",
        "         .\n",
        "             .\n",
        "\n",
        ": {query}\n",
        "\n",
        "  :\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "   JSON :\n",
        "{{\n",
        "    \"thinking\": \"<      >\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "   JSON  .     .\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n  LLM     (    )...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") #  llama3     \n",
        "\n",
        "# 2.2     \n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\n :')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', '  LLM  .'))\n",
        "\n",
        "    print('\\n :')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"  LLM  .\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        #    LLM    '0019'    doc_id      \n",
        "        #         \"doc_id_0019\"\n",
        "        #       LLM   \n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\" : {node['node_id']}\\t : {node.get('page_index', 'N/A')}\\t : {node['title']}\")\n",
        "        else:\n",
        "            print(f\" : {actual_node_id} (    -   LLM       )\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\n:   LLM JSON     :\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n       : {e}\")\n",
        "\n",
        "# 3.1       \n",
        "#  node_map      \n",
        "#    LLM       \n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\n :\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"           LLM.\")\n",
        "\n",
        "# 3.2      \n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "         :\\n\\n: {query}\\n: {relevant_content_str}\\n\\n        .\n",
        "    \"\"\"\n",
        "    print('\\n  LLM   ...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3\") #  llama3     \n",
        "    print('\\n :\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\n           .\")"
      ],
      "metadata": {
        "id": "MEwtDB3XP9fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " :\n",
        "\n",
        ":       \n",
        "\n",
        " :          \n",
        "\n",
        "  PDF:  PyPDF2   PDF \n",
        "\n",
        " :         \n",
        "\n",
        " :      \n",
        "\n",
        ":      \n",
        "\n",
        "               .      -          ."
      ],
      "metadata": {
        "id": "JuxsoQ1aS5tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai\n",
        "%pip install -q requests beautifulsoup4 PyPDF2\n",
        "\n",
        "import os\n",
        "import json\n",
        "import PyPDF2\n",
        "import httpx\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_')\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # Simple implementation to create a tree from text\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"Document Overview\"\n",
        "        node_counter = 0\n",
        "\n",
        "        section_patterns = [\n",
        "            \"Introduction\", \"Chapter\", \"Section\", \"Conclusion\", \"Summary\",\n",
        "            \"References\", \"Appendix\", \"Index\", \"Abstract\"\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100:\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {}\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Python Learning Book\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# Python Learning Book\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{node_counter:04d}\",\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# Specify local book path\n",
        "book_path = \"/content/data/Understanding_Climate_Change.pdf\"  # Change this path as needed\n",
        "\n",
        "# Verify book exists\n",
        "if not os.path.exists(book_path):\n",
        "    raise FileNotFoundError(f\"Book not found at specified path: {book_path}\")\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "try:\n",
        "    # Use PyPDF2 to extract text from local PDF\n",
        "    with open(book_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        extracted_text_parts = []\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                extracted_text_parts.append(f\"--- Page {page_num+1} ---\")\n",
        "                extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "\n",
        "    print(f\"Content extracted to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from PDF: {e}\")\n",
        "    # Use default content if extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Python Learning Book\n",
        "    ## Introduction\n",
        "    Python is a powerful and easy-to-learn programming language. This book aims to help you learn the basics and advanced concepts.\n",
        "    ## Chapter 1: Python Basics\n",
        "    In this chapter we will learn about variables, basic types, and basic operations in Python.\n",
        "    ## Chapter 2: Control Structures\n",
        "    We will learn about conditions, loops, and iteration in this chapter.\n",
        "    ## Conclusion\n",
        "    Python is an excellent language for both beginners and professionals.\n",
        "    \"\"\"\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used default text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the causes of climate change in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baEuiibUQ-_R",
        "outputId": "9f846984-70ad-45d9-8716-8ee918c56aef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m194.6/232.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing dummy LocalPageIndexClient for local execution.\n",
            "Content extracted to /content/data/Understanding_Climate_Change.txt\n",
            "Document submitted locally: Understanding_Climate_Change_txt\n",
            "Document Submitted: Understanding_Climate_Change_txt\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Python Learning Book (ID: Understanding_Climate_Change_txt_0000)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about the causes of climate change, so I will look for\n",
            "nodes that have 'causes' or 'greenhouse gases' in their summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: Chapter_2: Causes of Climate Change (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: Chapter_5: The Role of Technology in Climate Change (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIEgFHTqQ-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Climate_Change"
      ],
      "metadata": {
        "id": "wZb8ZTKNQ-4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##       "
      ],
      "metadata": {
        "id": "64BXVs4QR8xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069.pdf\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1.html\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the main findings of this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "id": "JVWiJO3fR51p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "     .\n",
        "  Ctrl + / (   )  Cmd + / ( macOS).\n",
        "    #     .              #.\n",
        "\n",
        "    !          .\n",
        "\n"
      ],
      "metadata": {
        "id": "avGq2FKxUBMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "%pip install -q requests beautifulsoup4  # For document downloading and parsing"
      ],
      "metadata": {
        "id": "DS7LYIWeTvRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the main findings of this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_nl1ceUXDfC",
        "outputId": "6d1e900f-f372-4edc-d69a-bbf546d508bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "Error connecting to Ollama: All connection attempts failed\n",
            "Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\n",
            "\n",
            "Error: LLM did not return valid JSON for tree search result:\n",
            "ERROR: Could not connect to Ollama. Please check your Ollama setup.\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the main findings of this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxUy6PlcXg9L",
        "outputId": "ef9d7062-553b-4268-8a0b-4c131ebe6ef1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking for main findings, so I will look for nodes that have\n",
            "'findings' or 'results' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "What are the conclusions in this document?"
      ],
      "metadata": {
        "id": "uzEgtEJKX84C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z82jWQcLYLja",
        "outputId": "455dc24d-1324-4d9f-a225-fcd137e60836"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in this document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: 2508_21069v1_0004 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Conclusion"
      ],
      "metadata": {
        "id": "LPWdd0SAZJ6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the Conclusion in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9QuMgoBZL6S",
        "outputId": "5e7a26ac-0baa-473d-a91b-174e1a39d9e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "Since we're looking for 'Conclusion' in this document, I'll focus on nodes that\n",
            "have a title or summary containing the word 'Conclusion'.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the conclusion in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvzKH8RjahUC",
        "outputId": "9eeb2d12-0daf-4532-82fa-be522fca38af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in the document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "auQIFmorbUN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ":         https://arxiv.org/html/2508.21069v1.  !\n",
        ":      _generate_simple_tree_from_text.     \" \"      ##                  .     \"\"          .\n",
        "  (LLM):            \"\"   ()   (_0003)           .\n",
        " :           (    )           .\n",
        ":\n",
        "     _generate_simple_tree_from_text            .         ( \"Abstract\", \"Introduction\", \"Conclusion\")        ##.\n",
        "                     .\n",
        "   :\n",
        "   .      LocalPageIndexClient   _generate_simple_tree_from_text.\n",
        "code\n",
        "Python\n"
      ],
      "metadata": {
        "id": "nzgD7mKrbNE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    # --- START OF MAJOR CORRECTION ---\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a more robust parser for the actual scraped HTML text.\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_title = \"Preamble\"  # Content before the first proper header\n",
        "        current_content = []\n",
        "\n",
        "        # More flexible headers to look for in the scraped text\n",
        "        # These are keywords that often start a section title.\n",
        "        header_keywords = [\n",
        "            \"Abstract\", \"Introduction\", \"Related Work\", \"Method\",\n",
        "            \"Experiments\", \"Conclusion\", \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if not stripped:\n",
        "                continue\n",
        "\n",
        "            found_header = None\n",
        "            # Heuristic: A line is likely a header if it's short and starts with a keyword or a number.\n",
        "            if len(stripped.split()) < 10:\n",
        "                for keyword in header_keywords:\n",
        "                    # Check for \"1 Introduction\", \"5. Conclusion\", \"Abstract\", etc.\n",
        "                    if (stripped.startswith(keyword) or (len(stripped) > 1 and stripped[0].isdigit() and keyword in stripped)):\n",
        "                        found_header = stripped\n",
        "                        break\n",
        "\n",
        "            if found_header:\n",
        "                # If we found a new header, save the previous section's content\n",
        "                if current_content:\n",
        "                    parsed_sections.append({\n",
        "                        \"title\": current_title,\n",
        "                        \"text\": \"\\n\".join(current_content).strip()\n",
        "                    })\n",
        "                # Start the new section\n",
        "                current_title = found_header\n",
        "                current_content = [] # Reset content buffer\n",
        "            else:\n",
        "                # This line is content for the current section\n",
        "                current_content.append(line)\n",
        "\n",
        "        # After the loop, save the very last section\n",
        "        if current_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_title,\n",
        "                \"text\": \"\\n\".join(current_content).strip()\n",
        "            })\n",
        "\n",
        "        # Now, build the tree from the correctly parsed sections\n",
        "        node_counter = 1\n",
        "        for i, section in enumerate(parsed_sections):\n",
        "            text = section[\"text\"]\n",
        "            summary = ' '.join(text.split()[:20]) + '...' if len(text.split()) > 20 else text\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{doc_id}_{i+1:04d}\", # Simple, sequential node IDs\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": i + 1\n",
        "            }\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "    # --- END OF MAJOR CORRECTION ---\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\\nPlease ensure Ollama is running.\")\n",
        "        return '{\"thinking\": \"Error: Could not connect to Ollama.\", \"node_list\": []}'\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return '{\"thinking\": \"An unexpected error occurred.\", \"node_list\": []}'\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\"\n",
        "doc_name = arxiv_html_url.split('/')[-1]\n",
        "doc_path = os.path.join(\"data\", doc_name)\n",
        "\n",
        "os.makedirs(os.path.dirname(doc_path), exist_ok=True)\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', class_='ltx_page_content')\n",
        "    if article_body:\n",
        "        text_content = article_body.get_text(separator='\\n', strip=True)\n",
        "    else: # Fallback if specific class not found\n",
        "        text_content = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    if not text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = doc_path.replace('.html', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    doc_id = None # Ensure doc_id is None if setup fails\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "if 'doc_id' in locals() and doc_id and pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"\\nSkipping further steps because document processing failed.\")\n",
        "    tree = None\n",
        "\n",
        "if tree:\n",
        "    # 2.1 Use LLM for tree search\n",
        "    query = \"What are the main findings of this document?\"\n",
        "    tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "    search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id and a title. Your task is to find all nodes that are likely to contain the answer to the question. Focus on sections like 'Abstract', 'Experiments', 'Results', or 'Conclusion'.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply ONLY with a JSON object in the following format, with no other text:\n",
        "{{\n",
        "    \"thinking\": \"<Your brief thinking process on which nodes are relevant>\",\n",
        "    \"node_list\": [\"<full_node_id_1>\", \"<full_node_id_2>\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "    tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\")\n",
        "\n",
        "    # 2.2 Print retrieved nodes and reasoning process\n",
        "    try:\n",
        "        node_map = utils.create_node_mapping(tree)\n",
        "        tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "        print('\\nReasoning Process:')\n",
        "        utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided.'))\n",
        "\n",
        "        print('\\nRetrieved Nodes:')\n",
        "        retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "        if not retrieved_node_ids:\n",
        "            print(\"No nodes retrieved by LLM.\")\n",
        "\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node:\n",
        "                print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "            else:\n",
        "                print(f\"Node ID: {node_id} (Not found in map - LLM may have hallucinated or ID format is mismatched)\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "        retrieved_node_ids = []\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "        retrieved_node_ids = []\n",
        "\n",
        "    # 3.1 Extract relevant context\n",
        "    relevant_content = []\n",
        "    if retrieved_node_ids:\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node and 'text' in node:\n",
        "                relevant_content.append(f\"--- From Section: {node['title']} ---\\n{node['text']}\")\n",
        "\n",
        "    relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "    print('\\nRetrieved Context:\\n')\n",
        "    if relevant_content_str:\n",
        "        utils.print_wrapped(relevant_content_str[:1500] + '...' if len(relevant_content_str) > 1500 else relevant_content_str)\n",
        "    else:\n",
        "        print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "    # 3.2 Generate answer\n",
        "    if relevant_content_str:\n",
        "        answer_prompt = f\"Based ONLY on the following context, what are the main findings of the document?\\n\\nContext:\\n{relevant_content_str}\\n\\nAnswer:\"\n",
        "\n",
        "        print('\\nCalling local LLM for answer generation...')\n",
        "        answer = await call_llm(answer_prompt, model=\"llama3:latest\")\n",
        "        print('\\nGenerated Answer:\\n')\n",
        "        utils.print_wrapped(answer)\n",
        "    else:\n",
        "        print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HpFzX6Ehahrt",
        "outputId": "6ae6556d-9efc-4a27-824d-ace551646e41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "  Preamble (ID: 2508_21069v1_0001)\n",
            "  Abstract (ID: 2508_21069v1_0002)\n",
            "  Introduction (ID: 2508_21069v1_0003)\n",
            "  Methods (ID: 2508_21069v1_0004)\n",
            "  Conclusion (ID: 2508_21069v1_0005)\n",
            "  References (ID: 2508_21069v1_0006)\n",
            "  Appendix A (ID: 2508_21069v1_0007)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about the main findings of this document, so I'm looking\n",
            "for sections that summarize or conclude the results. This suggests 'Abstract',\n",
            "'Conclusion', and possibly 'Results' if it exists in the tree.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002\t Page: 2\t Title: Abstract\n",
            "Node ID: 2508_21069v1_0005\t Page: 5\t Title: Conclusion\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "--- From Section: Abstract --- We present constraints on the reionization\n",
            "optical depth,  \\tau , obtained using several independent methods. First, we\n",
            "perform a non-parametric reconstruction of the reionization history, using\n",
            "Lyman-  \\alpha constraints on the evolution of the volume-averaged neutral\n",
            "hydrogen fraction, x HI  ( z ) x_{\\mathrm{HI}}(z) , including recent results\n",
            "from the James Webb Space Telescope . When combined with baryon acoustic\n",
            "oscillation (BAO) measurements from DESI and Big Bang nucleosynthesis\n",
            "constraints, these data imply a rapid reionization history ( z mid = 7.00  0.18\n",
            "+ 0.12 z_{\\mathrm{mid}}=7.00^{+0.12}_{-0.18} and   z 50 = 1.12  0.29 + 0.12\n",
            "\\Delta z_{50}=1.12^{+0.12}_{-0.29} ) and a value of  = 0.0492  0.0030 + 0.0014\n",
            "\\tau=0.0492^{+0.0014}_{-0.0030} , which is largely insensitive to the assumed\n",
            "cosmological model and independent of cosmic microwave background (CMB) data.\n",
            "The optical depth can also be measured from large-scale (  < 30 ) (\\ell<30) CMB\n",
            "polarization data, yielding constraints that are similarly model-insensitive and\n",
            "consistent with the Ly  \\alpha bound. Third,  \\tau may be constrained from the\n",
            "attenuation of small-scale (  > 30 ) (\\ell>30) CMB anisotropies, but the\n",
            "results are sensitive to the choice of cosmological model. Assuming  \\Lambda\n",
            "CDM and combining small-scale CMB data with CMB lensing and type 1a supernovae\n",
            "(SNe) yields tight constraints that are compatible with the Ly  \\alpha bound.\n",
            "Adding galaxy clustering a...\n",
            "\n",
            "Calling local LLM for answer generation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-762829457.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCalling local LLM for answer generation...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nGenerated Answer:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-762829457.py\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(prompt, model, temperature)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http://localhost:11434\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         response = await client.post(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;34m\"/api/chat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             json={\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \"\"\"\n\u001b[0;32m-> 1859\u001b[0;31m         return await self.request(\n\u001b[0m\u001b[1;32m   1860\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         )\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0masynccontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = await self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = await self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             ):\n\u001b[1;32m   1253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/locks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data"
      ],
      "metadata": {
        "id": "lfKLB1pzb9tc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m4CMFCvQcCxc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bvZluHBhj2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    # --- START OF MAJOR CORRECTION ---\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a more robust parser for the actual scraped HTML text.\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_title = \"Preamble\"  # Content before the first proper header\n",
        "        current_content = []\n",
        "\n",
        "        # More flexible headers to look for in the scraped text\n",
        "        # These are keywords that often start a section title.\n",
        "        header_keywords = [\n",
        "            \"Abstract\", \"Introduction\", \"Related Work\", \"Method\",\n",
        "            \"Experiments\", \"Conclusion\", \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if not stripped:\n",
        "                continue\n",
        "\n",
        "            found_header = None\n",
        "            # Heuristic: A line is likely a header if it's short and starts with a keyword or a number.\n",
        "            if len(stripped.split()) < 10:\n",
        "                for keyword in header_keywords:\n",
        "                    # Check for \"1 Introduction\", \"5. Conclusion\", \"Abstract\", etc.\n",
        "                    if (stripped.startswith(keyword) or (len(stripped) > 1 and stripped[0].isdigit() and keyword in stripped)):\n",
        "                        found_header = stripped\n",
        "                        break\n",
        "\n",
        "            if found_header:\n",
        "                # If we found a new header, save the previous section's content\n",
        "                if current_content:\n",
        "                    parsed_sections.append({\n",
        "                        \"title\": current_title,\n",
        "                        \"text\": \"\\n\".join(current_content).strip()\n",
        "                    })\n",
        "                # Start the new section\n",
        "                current_title = found_header\n",
        "                current_content = [] # Reset content buffer\n",
        "            else:\n",
        "                # This line is content for the current section\n",
        "                current_content.append(line)\n",
        "\n",
        "        # After the loop, save the very last section\n",
        "        if current_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_title,\n",
        "                \"text\": \"\\n\".join(current_content).strip()\n",
        "            })\n",
        "\n",
        "        # Now, build the tree from the correctly parsed sections\n",
        "        node_counter = 1\n",
        "        for i, section in enumerate(parsed_sections):\n",
        "            text = section[\"text\"]\n",
        "            summary = ' '.join(text.split()[:20]) + '...' if len(text.split()) > 20 else text\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{doc_id}_{i+1:04d}\", # Simple, sequential node IDs\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": i + 1\n",
        "            }\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "    # --- END OF MAJOR CORRECTION ---\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\\nPlease ensure Ollama is running.\")\n",
        "        return '{\"thinking\": \"Error: Could not connect to Ollama.\", \"node_list\": []}'\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return '{\"thinking\": \"An unexpected error occurred.\", \"node_list\": []}'\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\"\n",
        "doc_name = arxiv_html_url.split('/')[-1]\n",
        "doc_path = os.path.join(\"data\", doc_name)\n",
        "\n",
        "os.makedirs(os.path.dirname(doc_path), exist_ok=True)\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', class_='ltx_page_content')\n",
        "    if article_body:\n",
        "        text_content = article_body.get_text(separator='\\n', strip=True)\n",
        "    else: # Fallback if specific class not found\n",
        "        text_content = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    if not text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = doc_path.replace('.html', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    doc_id = None # Ensure doc_id is None if setup fails\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "if 'doc_id' in locals() and doc_id and pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"\\nSkipping further steps because document processing failed.\")\n",
        "    tree = None\n",
        "\n",
        "if tree:\n",
        "    # 2.1 Use LLM for tree search\n",
        "    query = \"What are the main findings of this document?\"\n",
        "    tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "    search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id and a title. Your task is to find all nodes that are likely to contain the answer to the question. Focus on sections like 'Abstract', 'Experiments', 'Results', or 'Conclusion'.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply ONLY with a JSON object in the following format, with no other text:\n",
        "{{\n",
        "    \"thinking\": \"<Your brief thinking process on which nodes are relevant>\",\n",
        "    \"node_list\": [\"<full_node_id_1>\", \"<full_node_id_2>\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "    tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\")\n",
        "\n",
        "    # 2.2 Print retrieved nodes and reasoning process\n",
        "    try:\n",
        "        node_map = utils.create_node_mapping(tree)\n",
        "        tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "        print('\\nReasoning Process:')\n",
        "        utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided.'))\n",
        "\n",
        "        print('\\nRetrieved Nodes:')\n",
        "        retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "        if not retrieved_node_ids:\n",
        "            print(\"No nodes retrieved by LLM.\")\n",
        "\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node:\n",
        "                print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "            else:\n",
        "                print(f\"Node ID: {node_id} (Not found in map - LLM may have hallucinated or ID format is mismatched)\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "        retrieved_node_ids = []\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "        retrieved_node_ids = []\n",
        "\n",
        "    # 3.1 Extract relevant context\n",
        "    relevant_content = []\n",
        "    if retrieved_node_ids:\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node and 'text' in node:\n",
        "                relevant_content.append(f\"--- From Section: {node['title']} ---\\n{node['text']}\")\n",
        "\n",
        "    relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "    print('\\nRetrieved Context:\\n')\n",
        "    if relevant_content_str:\n",
        "        utils.print_wrapped(relevant_content_str[:1500] + '...' if len(relevant_content_str) > 1500 else relevant_content_str)\n",
        "    else:\n",
        "        print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "    # 3.2 Generate answer\n",
        "    if relevant_content_str:\n",
        "        answer_prompt = f\"Based ONLY on the following context, what are the main findings of the document?\\n\\nContext:\\n{relevant_content_str}\\n\\nAnswer:\"\n",
        "\n",
        "        print('\\nCalling local LLM for answer generation...')\n",
        "        answer = await call_llm(answer_prompt, model=\"llama3:latest\")\n",
        "        print('\\nGenerated Answer:\\n')\n",
        "        utils.print_wrapped(answer)\n",
        "    else:\n",
        "        print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b4Xj6BUhjzj",
        "outputId": "1c24ae40-6dc1-4d10-c6e0-ac6ad2519506"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "  Preamble (ID: 2508_21069v1_0001)\n",
            "  Abstract (ID: 2508_21069v1_0002)\n",
            "  Introduction (ID: 2508_21069v1_0003)\n",
            "  Methods (ID: 2508_21069v1_0004)\n",
            "  Conclusion (ID: 2508_21069v1_0005)\n",
            "  References (ID: 2508_21069v1_0006)\n",
            "  Appendix A (ID: 2508_21069v1_0007)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about the main findings of this document, so I'm looking\n",
            "for sections that summarize or conclude the results. This suggests 'Abstract',\n",
            "'Conclusion', and possibly 'Results' if it exists in the tree.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002\t Page: 2\t Title: Abstract\n",
            "Node ID: 2508_21069v1_0005\t Page: 5\t Title: Conclusion\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "--- From Section: Abstract --- We present constraints on the reionization\n",
            "optical depth,  \\tau , obtained using several independent methods. First, we\n",
            "perform a non-parametric reconstruction of the reionization history, using\n",
            "Lyman-  \\alpha constraints on the evolution of the volume-averaged neutral\n",
            "hydrogen fraction, x HI  ( z ) x_{\\mathrm{HI}}(z) , including recent results\n",
            "from the James Webb Space Telescope . When combined with baryon acoustic\n",
            "oscillation (BAO) measurements from DESI and Big Bang nucleosynthesis\n",
            "constraints, these data imply a rapid reionization history ( z mid = 7.00  0.18\n",
            "+ 0.12 z_{\\mathrm{mid}}=7.00^{+0.12}_{-0.18} and   z 50 = 1.12  0.29 + 0.12\n",
            "\\Delta z_{50}=1.12^{+0.12}_{-0.29} ) and a value of  = 0.0492  0.0030 + 0.0014\n",
            "\\tau=0.0492^{+0.0014}_{-0.0030} , which is largely insensitive to the assumed\n",
            "cosmological model and independent of cosmic microwave background (CMB) data.\n",
            "The optical depth can also be measured from large-scale (  < 30 ) (\\ell<30) CMB\n",
            "polarization data, yielding constraints that are similarly model-insensitive and\n",
            "consistent with the Ly  \\alpha bound. Third,  \\tau may be constrained from the\n",
            "attenuation of small-scale (  > 30 ) (\\ell>30) CMB anisotropies, but the\n",
            "results are sensitive to the choice of cosmological model. Assuming  \\Lambda\n",
            "CDM and combining small-scale CMB data with CMB lensing and type 1a supernovae\n",
            "(SNe) yields tight constraints that are compatible with the Ly  \\alpha bound.\n",
            "Adding galaxy clustering a...\n",
            "\n",
            "Calling local LLM for answer generation...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "Based on the provided context, the main findings of the document are:  1.\n",
            "Constraints on the reionization optical depth () obtained using several\n",
            "independent methods:         * Non-parametric reconstruction of the reionization\n",
            "history from Lyman- and Lyman- constraints.         * Baryon acoustic\n",
            "oscillation (BAO) measurements from DESI and Big Bang nucleosynthesis\n",
            "constraints.         * Large-scale CMB polarization data. 2. The optical depth\n",
            "is found to be:         *  = 0.0492  0.0014 (mainly insensitive to the assumed\n",
            "cosmological model). 3. A rapid and late reionization history is implied, with a\n",
            "midpoint at zmid = 7.00  0.12 and a duration of z50 = 1.12  0.29. 4. The\n",
            "constraints from DESI BAO and CMB data imply a larger optical depth ( = 0.094 \n",
            "0.011), which is in tension with the Lyman- bound. 5. Alternative cosmological\n",
            "models, such as those with dynamical dark energy or non-standard reionization\n",
            "scenarios, can reconcile some but not all constraints.  These findings suggest\n",
            "that there are tensions between different observational datasets and theoretical\n",
            "models, highlighting the need for further research to understand the early\n",
            "universe and its evolution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyp67cDvh1ee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}