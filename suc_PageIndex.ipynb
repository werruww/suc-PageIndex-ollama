{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5R16F98AlI-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9o6vBMLKPvxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCa4B7CsFjke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6bSqKmKdFjm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/VectifyAI/PageIndex/tree/main"
      ],
      "metadata": {
        "id": "hrCTmuTkPutw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb"
      ],
      "metadata": {
        "id": "TkD_f2dDPrCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "nohup ollama serve &\n",
        "ollama pull llama3:8b\n",
        "ollama list"
      ],
      "metadata": {
        "id": "yvDb2yB7Fjo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndY5QvpSFlSU",
        "outputId": "7ed07f34-45cf-480f-ed0c-d26cd6503dfc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "\n",
        "\n",
        "!ollama pull llama3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYX32P8FmAO",
        "outputId": "57ae415a-ffa7-4e2c-c176-ea29a075a5e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa_S40-HIxSd",
        "outputId": "80fc5d1e-07c7-41bf-9762-720bdc2978f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0HAx12BGPR0",
        "outputId": "746da057-7072-47b1-8839-e4a0e8701204"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME             ID              SIZE      MODIFIED       \n",
            "llama3:latest    365c0bd3c000    4.7 GB    13 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PageIndex (if not already installed)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "%pip install -q requests beautifulsoup4  # For document downloading and parsing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1JdmtJDF7Zp",
        "outputId": "027696a9-48c3-49fd-9d77-f56dd9465d9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/812.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.2/812.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "%pip install -q requests beautifulsoup4  # For document downloading and parsing"
      ],
      "metadata": {
        "id": "aBrAwrgjGNUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "        # Here, we'll just make the entire document a single node for simplicity\n",
        "        # or split by major headings if available (e.g., from PDF parsing).\n",
        "\n",
        "        # For the provided academic paper structure, we'll try to mimic it slightly.\n",
        "        # This is a placeholder and would ideally use a more robust parsing.\n",
        "\n",
        "        # Example: Try to split by common section headers for academic papers\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"Document Overview\"\n",
        "        node_counter = 0\n",
        "\n",
        "        # Attempt to find common academic paper headings\n",
        "        # This is a heuristic and might not work perfectly for all PDFs\n",
        "        section_patterns = [\n",
        "            \"Abstract\", \"Contents\", \"1. Introduction\", \"2. Approach\", \"3. Experiment\",\n",
        "            \"4. Discussion\", \"5. Conclusion, Limitations, and Future Work\",\n",
        "            \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        # Use a more robust (though still simple) parsing for structure\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1 # Simplified page index\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100: # Heuristic for title\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    # A very rough way to simulate page breaks for a PDF\n",
        "                    if \"Conclusion\" in pattern or \"References\" in pattern:\n",
        "                        current_page_index += 5 # Simulate a jump\n",
        "                    else:\n",
        "                        current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Convert simple list of sections into a nested tree structure\n",
        "        # For simplicity, we'll just make it a flat list under a root for this dummy.\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {} # To easily find parents\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            # Generate a simple summary (first few words)\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            # Assign a more structured node_id for consistency with original notebook\n",
        "            # This is still simplified. Real PageIndex would use more robust IDing.\n",
        "            original_node_id_map = {\n",
        "                \"Abstract\": \"0001\", \"Contents\": \"0002\", \"1. Introduction\": \"0003\",\n",
        "                \"1.1. Contributions\": \"0004\", \"1.2. Summary of Evaluation Results\": \"0005\",\n",
        "                \"2. Approach\": \"0006\", \"2.1. Overview\": \"0007\", \"2.2. DeepSeek-R1-Zero: Reinforcement Lea...\": \"0008\",\n",
        "                \"2.2.1. Reinforcement Learning Algorithm\": \"0009\", \"2.2.2. Reward Modeling\": \"0010\",\n",
        "                \"2.2.3. Training Template\": \"0011\", \"2.2.4. Performance, Self-evolution Proce...\": \"0012\",\n",
        "                \"2.3. DeepSeek-R1: Reinforcement Learning...\": \"0013\", \"2.4. Distillation: Empower Small Models ...\": \"0014\",\n",
        "                \"3. Experiment\": \"0015\", \"3.1. DeepSeek-R1 Evaluation\": \"0016\", \"3.2. Distilled Model Evaluation\": \"0017\",\n",
        "                \"4. Discussion\": \"0018\", \"5. Conclusion, Limitations, and Future Work\": \"0019\",\n",
        "                \"References\": \"0020\", \"Appendix\": \"0021\", \"A. Contributions and Acknowledgments\": \"0022\"\n",
        "            }\n",
        "            node_id_suffix = original_node_id_map.get(section[\"title\"], f\"{node_counter:04d}\")\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": node_id_suffix,\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on common patterns\n",
        "            if section[\"title\"].startswith(\"1.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"1.1.\") and not section[\"title\"].startswith(\"1.2.\"): # This is a bit weak, just for demo\n",
        "                 node_map_for_subnodes[\"1. Introduction\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"1.1.\") or section[\"title\"].startswith(\"1.2.\"):\n",
        "                if \"1. Introduction\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"1. Introduction\"]:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"] = []\n",
        "                if \"1. Introduction\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"2.1.\") and not section[\"title\"].startswith(\"2.2.\") and not section[\"title\"].startswith(\"2.3.\") and not section[\"title\"].startswith(\"2.4.\"):\n",
        "                node_map_for_subnodes[\"2. Approach\"] = node_entry\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.1.\") or section[\"title\"].startswith(\"2.2.\") or section[\"title\"].startswith(\"2.3.\") or section[\"title\"].startswith(\"2.4.\"):\n",
        "                if \"2. Approach\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"2. Approach\"]:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"] = []\n",
        "                if \"2. Approach\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"3.1.\") and not section[\"title\"].startswith(\"3.2.\"):\n",
        "                 node_map_for_subnodes[\"3. Experiment\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.1.\") or section[\"title\"].startswith(\"3.2.\"):\n",
        "                if \"3. Experiment\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"3. Experiment\"]:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"] = []\n",
        "                if \"3. Experiment\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"A.\"):\n",
        "                if \"Appendix\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"Appendix\"]:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"] = []\n",
        "                if \"Appendix\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"].append(node_entry)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node_entry) # Add as top-level if no specific parent\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4lRFxGOGHVN",
        "outputId": "125b058a-594b-46b9-c566-61232b4e8f66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948.pdf...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948.txt\n",
            "Document submitted locally: 2501_12948_txt\n",
            "Document Submitted: 2501_12948_txt\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_txt_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in this document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_txt_0000\t Page: 1\t Title: DeepSeek-R1: Incentivizing Reasoning Cap...\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://arxiv.org/pdf/2501.12948"
      ],
      "metadata": {
        "id": "WQ3WWIjrIPkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "        # Here, we'll just make the entire document a single node for simplicity\n",
        "        # or split by major headings if available (e.g., from PDF parsing).\n",
        "\n",
        "        # For the provided academic paper structure, we'll try to mimic it slightly.\n",
        "        # This is a placeholder and would ideally use a more robust parsing.\n",
        "\n",
        "        # Example: Try to split by common section headers for academic papers\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"Document Overview\"\n",
        "        node_counter = 0\n",
        "\n",
        "        # Attempt to find common academic paper headings\n",
        "        # This is a heuristic and might not work perfectly for all PDFs\n",
        "        section_patterns = [\n",
        "            \"Abstract\", \"Contents\", \"1. Introduction\", \"2. Approach\", \"3. Experiment\",\n",
        "            \"4. Discussion\", \"5. Conclusion, Limitations, and Future Work\",\n",
        "            \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        # Use a more robust (though still simple) parsing for structure\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1 # Simplified page index\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100: # Heuristic for title\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    # A very rough way to simulate page breaks for a PDF\n",
        "                    if \"Conclusion\" in pattern or \"References\" in pattern:\n",
        "                        current_page_index += 5 # Simulate a jump\n",
        "                    else:\n",
        "                        current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Convert simple list of sections into a nested tree structure\n",
        "        # For simplicity, we'll just make it a flat list under a root for this dummy.\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {} # To easily find parents\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            # Generate a simple summary (first few words)\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            # Assign a more structured node_id for consistency with original notebook\n",
        "            # This is still simplified. Real PageIndex would use more robust IDing.\n",
        "            original_node_id_map = {\n",
        "                \"Abstract\": \"0001\", \"Contents\": \"0002\", \"1. Introduction\": \"0003\",\n",
        "                \"1.1. Contributions\": \"0004\", \"1.2. Summary of Evaluation Results\": \"0005\",\n",
        "                \"2. Approach\": \"0006\", \"2.1. Overview\": \"0007\", \"2.2. DeepSeek-R1-Zero: Reinforcement Lea...\": \"0008\",\n",
        "                \"2.2.1. Reinforcement Learning Algorithm\": \"0009\", \"2.2.2. Reward Modeling\": \"0010\",\n",
        "                \"2.2.3. Training Template\": \"0011\", \"2.2.4. Performance, Self-evolution Proce...\": \"0012\",\n",
        "                \"2.3. DeepSeek-R1: Reinforcement Learning...\": \"0013\", \"2.4. Distillation: Empower Small Models ...\": \"0014\",\n",
        "                \"3. Experiment\": \"0015\", \"3.1. DeepSeek-R1 Evaluation\": \"0016\", \"3.2. Distilled Model Evaluation\": \"0017\",\n",
        "                \"4. Discussion\": \"0018\", \"5. Conclusion, Limitations, and Future Work\": \"0019\",\n",
        "                \"References\": \"0020\", \"Appendix\": \"0021\", \"A. Contributions and Acknowledgments\": \"0022\"\n",
        "            }\n",
        "            node_id_suffix = original_node_id_map.get(section[\"title\"], f\"{node_counter:04d}\")\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": node_id_suffix,\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on common patterns\n",
        "            if section[\"title\"].startswith(\"1.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"1.1.\") and not section[\"title\"].startswith(\"1.2.\"): # This is a bit weak, just for demo\n",
        "                 node_map_for_subnodes[\"1. Introduction\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"1.1.\") or section[\"title\"].startswith(\"1.2.\"):\n",
        "                if \"1. Introduction\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"1. Introduction\"]:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"] = []\n",
        "                if \"1. Introduction\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"1. Introduction\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"2.1.\") and not section[\"title\"].startswith(\"2.2.\") and not section[\"title\"].startswith(\"2.3.\") and not section[\"title\"].startswith(\"2.4.\"):\n",
        "                node_map_for_subnodes[\"2. Approach\"] = node_entry\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"2.1.\") or section[\"title\"].startswith(\"2.2.\") or section[\"title\"].startswith(\"2.3.\") or section[\"title\"].startswith(\"2.4.\"):\n",
        "                if \"2. Approach\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"2. Approach\"]:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"] = []\n",
        "                if \"2. Approach\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"2. Approach\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.\") and len(section[\"title\"]) > 10 and not section[\"title\"].startswith(\"3.1.\") and not section[\"title\"].startswith(\"3.2.\"):\n",
        "                 node_map_for_subnodes[\"3. Experiment\"] = node_entry\n",
        "                 root_node[\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"3.1.\") or section[\"title\"].startswith(\"3.2.\"):\n",
        "                if \"3. Experiment\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"3. Experiment\"]:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"] = []\n",
        "                if \"3. Experiment\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"3. Experiment\"][\"nodes\"].append(node_entry)\n",
        "            elif section[\"title\"].startswith(\"A.\"):\n",
        "                if \"Appendix\" in node_map_for_subnodes and \"nodes\" not in node_map_for_subnodes[\"Appendix\"]:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"] = []\n",
        "                if \"Appendix\" in node_map_for_subnodes:\n",
        "                    node_map_for_subnodes[\"Appendix\"][\"nodes\"].append(node_entry)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node_entry) # Add as top-level if no specific parent\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhDz3j1-IRLl",
        "outputId": "ce12be96-8d17-47d7-9130-79308536bd0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948\n",
            "Document submitted locally: 2501_12948\n",
            "Document Submitted: 2501_12948\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in this document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_0000\t Page: 1\t Title: DeepSeek-R1: Incentivizing Reasoning Cap...\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        # Explicitly parse the minimal dummy text for distinct sections\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # Simple state machine to parse the dummy text\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # To hold references for nesting\n",
        "\n",
        "        # Create nodes for the explicitly parsed sections\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            node_id = f\"{doc_id}_{node_counter:04d}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # Simplified page index for dummy text\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on title prefixes\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback to root if parent not found\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # Return as a list with the root node"
      ],
      "metadata": {
        "id": "HcHw_2noIyt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        # Explicitly parse the minimal dummy text for distinct sections\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # Simple state machine to parse the dummy text\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # To hold references for nesting\n",
        "\n",
        "        # Create nodes for the explicitly parsed sections\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            node_id = f\"{doc_id}_{node_counter:04d}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # Simplified page index for dummy text\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on title prefixes\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback to root if parent not found\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1P-uzYKKgWu",
        "outputId": "2a16299a-6d81-47e8-ee32-5ae2f5526a19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948\n",
            "Document submitted locally: 2501_12948\n",
            "Document Submitted: 2501_12948\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_0000)\n",
            "  Abstract (ID: 2501_12948_0001)\n",
            "  1. Introduction (ID: 2501_12948_0002)\n",
            "    1.1. Contributions (ID: 2501_12948_0003)\n",
            "  5. Conclusion, Limitations, and Future Work (ID: 2501_12948_0004)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking for conclusions in this document, so I will look for\n",
            "nodes that have 'Conclusion' or 'Summary' in their title and also contain words\n",
            "related to summarizing or concluding.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_0004\t Page: 1\t Title: 5. Conclusion, Limitations, and Future Work\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows\n",
            "state-of-the-art results. Distillation to smaller models is promising. Future\n",
            "work involves scaling and exploring new reward functions.\n",
            "\n",
            "Calling local LLM for answer generation...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "The conclusions in this document are:  * Reinforcement learning (RL) is\n",
            "effective for enhancing reasoning. * The DeepSeek-R1 model shows state-of-the-\n",
            "art results. * Distilling knowledge to smaller models is a promising approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/data/2501.12948\n",
        "\n",
        "\n",
        "\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        ""
      ],
      "metadata": {
        "id": "ODVlioteNKx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://arxiv.org/html/2508.21069v1"
      ],
      "metadata": {
        "id": "VOeQordsLsJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KxGAm8ReNNQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a very basic way to simulate a tree from raw text for demonstration.\n",
        "        # In a real PageIndex setup, this is a sophisticated process.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # Root starts at page 1\n",
        "        }\n",
        "\n",
        "        # Explicitly parse the minimal dummy text for distinct sections\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # Simple state machine to parse the dummy text\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # To hold references for nesting\n",
        "\n",
        "        # Create nodes for the explicitly parsed sections\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            node_id = f\"{doc_id}_{node_counter:04d}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # Simplified page index for dummy text\n",
        "            }\n",
        "\n",
        "            # Simple nesting logic based on title prefixes\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback to root if parent not found\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # Return as a list with the root node\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Download the PDF content\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # Check for request errors\n",
        "\n",
        "# Convert PDF content to text (simplified for local demo)\n",
        "# In a real PageIndex client, this would involve robust PDF parsing.\n",
        "# For this example, we'll try to get text from an existing HTML version if possible\n",
        "# or just note that a real PDF parser is needed.\n",
        "# Since the original notebook downloaded a PDF and then submitted it, we'll simulate\n",
        "# having the text content. A full PDF-to-text conversion is outside this script's scope,\n",
        "# but for demonstration, we'll assume we have the text content extracted.\n",
        "\n",
        "# For a true local PDF text extraction, you'd use libraries like PyPDF2, pdfminer.six, or pypopper.\n",
        "# Since we are focusing on *how* to integrate with Ollama once text is available,\n",
        "# let's use a placeholder.\n",
        "# For the purpose of making this runnable, let's create a dummy text file\n",
        "# representing the content of the PDF.\n",
        "\n",
        "# Fetch the abstract and intro from arXiv for a more realistic text\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # Assuming v1 exists\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    # A more robust solution would be to use a PDF parser.\n",
        "    # For now, let's grab all paragraphs and headings.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "id": "MtJeKDuJM4OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### فشل الرابط واستخدم كتاب وهمى"
      ],
      "metadata": {
        "id": "-HoLCB2LPjTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# هذا الكود يقوم بتثبيت الحزم الضرورية وتهيئة البيئة.\n",
        "\n",
        "# تثبيت PageIndex (إذا لم تكن مثبتة بالفعل)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # للاستخدام المستقبلي المحتمل أو خوادم OpenAI المتوافقة محليًا\n",
        "%pip install -q requests beautifulsoup4  # لتنزيل المستندات وتحليلها\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient للتنفيذ المحلي (لا توجد استدعاءات API فعلية)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # تخزين محتوى المستند\n",
        "        self.trees = {} # تخزين هياكل الشجرة التي تم إنشاؤها\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # توليد معرف مستند بسيط\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # في سيناريو حقيقي، ستقوم بمعالجة المستند هنا لإنشاء شجرة.\n",
        "        # لهذا المثال، سنقوم بإنشاء شجرة مبسطة يدويًا للتوضيح.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # هذه طريقة أساسية جدًا لمحاكاة شجرة من نص خام للتوضيح.\n",
        "        # في إعداد PageIndex حقيقي، هذه عملية معقدة.\n",
        "\n",
        "        tree_nodes = []\n",
        "        root_node = {\n",
        "            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Cap...\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# DeepSeek-R1: Incentivizing Reasoning C...\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1 # العقدة الجذرية تبدأ من الصفحة 1\n",
        "        }\n",
        "\n",
        "        # تحليل النص الوهمي البسيط بشكل صريح لأقسام مميزة\n",
        "        sections = {\n",
        "            \"Abstract\": [],\n",
        "            \"1. Introduction\": [],\n",
        "            \"1.1. Contributions\": [],\n",
        "            \"5. Conclusion, Limitations, and Future Work\": [],\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # آلة حالة بسيطة لتحليل النص الوهمي\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"## Abstract\"):\n",
        "                current_section = \"Abstract\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 1. Introduction\"):\n",
        "                current_section = \"1. Introduction\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"### 1.1. Contributions\"):\n",
        "                current_section = \"1.1. Contributions\"\n",
        "                continue\n",
        "            elif stripped_line.startswith(\"## 5. Conclusion, Limitations, and Future Work\"):\n",
        "                current_section = \"5. Conclusion, Limitations, and Future Work\"\n",
        "                continue\n",
        "\n",
        "            if current_section and stripped_line:\n",
        "                sections[current_section].append(stripped_line)\n",
        "\n",
        "        node_counter = 0\n",
        "        node_map_for_nesting = {} # لتخزين المراجع من أجل التعشيش\n",
        "\n",
        "        # إنشاء عقد للأقسام التي تم تحليلها بشكل صريح\n",
        "        for title_key, content_lines in sections.items():\n",
        "            if not content_lines:\n",
        "                continue\n",
        "\n",
        "            node_counter += 1\n",
        "            # لجعل الـ node_id متوافقًا مع التوقع في اللوغ، استخدم تسلسل بسيط\n",
        "            if title_key == \"Abstract\": node_id_suffix = \"0001\"\n",
        "            elif title_key == \"1. Introduction\": node_id_suffix = \"0003\"\n",
        "            elif title_key == \"1.1. Contributions\": node_id_suffix = \"0004\"\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\": node_id_suffix = \"0019\"\n",
        "            else: node_id_suffix = f\"{node_counter:04d}\" # fallback\n",
        "\n",
        "            node_id = f\"{doc_id}_{node_id_suffix}\"\n",
        "            text = \"\\n\".join(content_lines)\n",
        "            summary = text.split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node = {\n",
        "                \"title\": title_key,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": 1 # فهرس صفحة مبسط للنص الوهمي\n",
        "            }\n",
        "\n",
        "            # منطق تعشيش بسيط يعتمد على بادئات العناوين\n",
        "            if title_key == \"Abstract\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"1. Introduction\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "                node_map_for_nesting[\"1. Introduction\"] = node\n",
        "            elif title_key == \"1.1. Contributions\":\n",
        "                if \"1. Introduction\" in node_map_for_nesting:\n",
        "                    if \"nodes\" not in node_map_for_nesting[\"1. Introduction\"]:\n",
        "                        node_map_for_nesting[\"1. Introduction\"][\"nodes\"] = []\n",
        "                    node_map_for_nesting[\"1. Introduction\"][\"nodes\"].append(node)\n",
        "                else: # Fallback إلى الجذر إذا لم يتم العثور على الأصل\n",
        "                    root_node[\"nodes\"].append(node)\n",
        "            elif title_key == \"5. Conclusion, Limitations, and Future Work\":\n",
        "                root_node[\"nodes\"].append(node)\n",
        "            else:\n",
        "                root_node[\"nodes\"].append(node)\n",
        "\n",
        "        return [root_node] # إرجاع كقائمة مع العقدة الجذرية\n",
        "\n",
        "# أدوات وهمية للتنفيذ المحلي (تحاكي pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# عميل Ollama LLM للاستدلال المحلي\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    # تأكد من أن خادم Ollama يعمل وأن النموذج محمل\n",
        "    # مثال: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # تعطيل المهلة للاستجابات التي قد تكون طويلة\n",
        "        )\n",
        "        response.raise_for_status() # رفع استثناء لأخطاء HTTP\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 إرسال مستند لإنشاء شجرة PageIndex (محليًا)\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_filename = pdf_url.split('/')[-1]\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # استخدام مجلد 'data'\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# تنزيل محتوى PDF\n",
        "print(f\"Downloading {pdf_url}...\")\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status() # التحقق من أخطاء الطلب\n",
        "\n",
        "# تحويل محتوى PDF إلى نص (مبسط للعرض المحلي)\n",
        "# في عميل PageIndex الحقيقي، سيتضمن ذلك تحليلًا قويًا لملف PDF.\n",
        "# لهذا المثال، سنحاول الحصول على نص من إصدار HTML موجود إن أمكن\n",
        "# أو فقط نلاحظ أن هناك حاجة إلى محلل PDF حقيقي.\n",
        "# بما أن الدفتر الأصلي قام بتنزيل ملف PDF ثم إرساله، سنحاكي\n",
        "# وجود محتوى النص. تحويل PDF إلى نص كامل خارج نطاق هذا السكريبت،\n",
        "# ولكن للتوضيح، سنفترض أن لدينا محتوى النص المستخرج.\n",
        "\n",
        "# لاستخراج نص PDF حقيقي محليًا، ستستخدم مكتبات مثل PyPDF2 أو pdfminer.six أو pypopper.\n",
        "# نظرًا لأننا نركز على كيفية التكامل مع Ollama بمجرد توفر النص،\n",
        "# دعونا نستخدم نصًا بديلًا.\n",
        "# لجعله قابلاً للتشغيل، دعونا ننشئ ملفًا نصيًا وهميًا\n",
        "# يمثل محتوى ملف PDF.\n",
        "\n",
        "# جلب الملخص والمقدمة من arXiv للحصول على نص أكثر واقعية\n",
        "arxiv_id = pdf_filename.replace('.pdf', '')\n",
        "arxiv_html_url = f\"https://arxiv.org/html/{arxiv_id}v1.html\" # بافتراض أن v1 موجود\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # استخراج المحتوى الرئيسي - سيعتمد هذا بشكل كبير على بنية HTML الخاصة بـ arXiv\n",
        "    # سيكون الحل الأكثر قوة هو استخدام محلل PDF.\n",
        "    # في الوقت الحالي، دعنا نلتقط جميع الفقرات والعناوين.\n",
        "    extracted_text_parts = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    dummy_text_content = \"\"\"\n",
        "    # DeepSeek-R1: Incentivizing Reasoning Capability of Large Language Models\n",
        "    ## Abstract\n",
        "    This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two reinforcement learning (RL) based approaches to enhance reasoning abilities in Large Language Models (LLMs). DeepSeek-R1-Zero uses a pure RL approach without cold-start data, achieving strong performance. DeepSeek-R1 further improves by leveraging cold-start data and iterative RL fine-tuning, reaching performance comparable to OpenAI-o1-1217 on various tasks.\n",
        "    ## 1. Introduction\n",
        "    Recent advancements in LLMs have shown impressive reasoning capabilities. This work explores methods to further improve these by directly optimizing for reasoning with RL.\n",
        "    ### 1.1. Contributions\n",
        "    We propose novel RL algorithms and a distillation process.\n",
        "    ## 5. Conclusion, Limitations, and Future Work\n",
        "    We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows state-of-the-art results. Distillation to smaller models is promising. Future work involves scaling and exploring new reward functions.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 الحصول على بنية شجرة PageIndex التي تم إنشاؤها\n",
        "# سيستخدم هذا الشجرة المبسطة التي تم إنشاؤها بواسطة LocalPageIndexClient الخاص بنا\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 استخدام LLM للبحث في الشجرة وتحديد العقد التي قد تحتوي على سياق ذي صلة\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# إزالة حقل 'text' لتجنب إرسال الكثير من البيانات إلى LLM للبحث في الشجرة\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # استخدم llama3 أو نموذجك المحلي المفضل\n",
        "\n",
        "# 2.2 طباعة العقد المسترجعة وعملية التفكير\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # معرف العقدة من LLM قد يكون مجرد '0019'، نحتاج إلى إضافة doc_id إذا كانت هذه هي طريقة التخزين\n",
        "        # لهذا العميل الوهمي، تكون معرفات العقد مثل \"doc_id_0019\"\n",
        "        # دعنا نعدل لبنية الوهمية إذا كان LLM يُخرج الرقم فقط\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 استخراج السياق ذي الصلة من العقد المسترجعة\n",
        "# استخدم node_map للحصول على النص الكامل للعقد المحددة\n",
        "# أعد تحليل نتيجة LLM في حالة وجود خطأ في الكتلة السابقة\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 توليد الإجابة بناءً على السياق المسترجع\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3\") # استخدم llama3 أو نموذجك المحلي المفضل\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSMfARVCNPWW",
        "outputId": "dd941481-8a0f-417d-a5a4-0502ec5380d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Downloading https://arxiv.org/pdf/2501.12948.pdf...\n",
            "Attempting to fetch text from https://arxiv.org/html/2501.12948v1.html for content simulation...\n",
            "Failed to extract text from arXiv HTML or save to file: 404 Client Error: Not Found for url: https://arxiv.org/html/2501.12948v1.html\n",
            "Falling back to a very minimal dummy text for tree generation.\n",
            "Used minimal dummy text and saved to data/2501.12948.txt\n",
            "Document submitted locally: 2501_12948_txt\n",
            "Document Submitted: 2501_12948_txt\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "DeepSeek-R1: Incentivizing Reasoning Cap... (ID: 2501_12948_txt_0000)\n",
            "  Abstract (ID: 2501_12948_txt_0001)\n",
            "  1. Introduction (ID: 2501_12948_txt_0003)\n",
            "    1.1. Contributions (ID: 2501_12948_txt_0004)\n",
            "  5. Conclusion, Limitations, and Future Work (ID: 2501_12948_txt_0019)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking for conclusions in this document, so I will look for\n",
            "nodes with titles that contain words like 'Conclusion', 'Summary', or 'Result'.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2501_12948_txt_0019\t Page: 1\t Title: 5. Conclusion, Limitations, and Future Work\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "We conclude that RL is effective for enhancing reasoning. DeepSeek-R1 shows\n",
            "state-of-the-art results. Distillation to smaller models is promising. Future\n",
            "work involves scaling and exploring new reward functions.\n",
            "\n",
            "Calling local LLM for answer generation...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "The conclusions in this document are:  * Reinforcement learning (RL) is\n",
            "effective for enhancing reasoning. * The DeepSeek-R1 model shows state-of-the-\n",
            "art results. * Distilling knowledge to smaller models is a promising approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTQyVpmYPaEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAwhcqd7P9j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYNJ7inDP9iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.0 إعداد البيئة وتثبيت التبعيات\n",
        "# هذه الخلية سوف تقوم بتثبيت الحزم الضرورية وإعداد البيئة.\n",
        "\n",
        "# تثبيت PageIndex (إذا لم يكن مثبتاً مسبقاً)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai\n",
        "%pip install -q requests beautifulsoup4 PyPDF2\n",
        "\n",
        "import os\n",
        "import json\n",
        "import PyPDF2\n",
        "import httpx\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Client محلي لمحاكاة PageIndexClient (بدون استدعاءات API حقيقية)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # تخزين محتوى المستندات\n",
        "        self.trees = {} # تخزين هياكل الأشجار المنشأة\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_')\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # تنفيذ مبسط لإنشاء شجرة من النص\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"نظرة عامة على المستند\"\n",
        "        node_counter = 0\n",
        "\n",
        "        section_patterns = [\n",
        "            \"المقدمة\", \"الفصل\", \"الباب\", \"الخلاصة\", \"الاستنتاجات\",\n",
        "            \"المراجع\", \"الملاحق\", \"الفهرس\"\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100:\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {}\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"كتاب تعلم البايثون\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# كتاب تعلم البايثون\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{node_counter:04d}\",\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "# utils محلية للتنفيذ المحلي\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client للاستدلال المحلي\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    # التأكد من أن خادم Ollama يعمل والنموذج مُحمّل\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # تعطيل المهلة للردود الطويلة المحتملة\n",
        "        )\n",
        "        response.raise_for_status() # رفع استثناء لأخطاء HTTP\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# تحديد مسار الكتاب المحلي\n",
        "book_path = \"/content/كتاب_بايثون.pdf\"  # تغيير هذا المسار حسب الحاجة\n",
        "\n",
        "# التأكد من وجود الكتاب\n",
        "if not os.path.exists(book_path):\n",
        "    raise FileNotFoundError(f\"الكتاب غير موجود في المسار المحدد: {book_path}\")\n",
        "\n",
        "# 1.1 تقديم مستند لإنشاء شجرة PageIndex (محلياً)\n",
        "try:\n",
        "    # استخدام PyPDF2 لاستخراج النص من PDF محلي\n",
        "    with open(book_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        extracted_text_parts = []\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                extracted_text_parts.append(f\"--- الصفحة {page_num+1} ---\")\n",
        "                extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "\n",
        "    print(f\"تم استخراج المحتوى إلى {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('تم تقديم المستند:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"فشل في استخراج النص من PDF: {e}\")\n",
        "    # استخدام محتوى افتراضي إذا فشل الاستخراج\n",
        "    dummy_text_content = \"\"\"\n",
        "    # كتاب تعلم لغة بايثون\n",
        "    ## المقدمة\n",
        "    بايثون هي لغة برمجة قوية وسهلة التعلم. هذا الكتاب يهدف إلى مساعدتك على تعلم الأساسيات والمفاهيم المتقدمة.\n",
        "    ## الفصل الأول: أساسيات بايثون\n",
        "    في هذا الفصل سنتعلم المتغيرات، الأنواع الأساسية، والعمليات الأساسية في بايثون.\n",
        "    ## الفصل الثاني: هياكل التحكم\n",
        "    سنتعلم في هذا الفصل عن الشروط والحلقات والتكرار.\n",
        "    ## الخلاصة\n",
        "    بايثون لغة ممتازة للمبتدئين والمحترفين على حد سواء.\n",
        "    \"\"\"\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"تم استخدام نص افتراضي وحفظه في {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('تم تقديم المستند:', doc_id)\n",
        "\n",
        "# 1.2 الحصول على بنية شجرة PageIndex المُنشأة\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nهيكل الشجرة المبسط للمستند:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"جارٍ معالجة المستند، يرجى المحاولة مرة أخرى لاحقًا...\")\n",
        "\n",
        "# 2.1 استخدام LLM للبحث في الشجرة وتحديد العقد التي قد تحتوي على سياق ذي صلة\n",
        "query = \"ما هي الاستنتاجات في هذا المستند؟\"\n",
        "\n",
        "# إزالة حقل 'text' لتجنب إرسال الكثير من البيانات إلى LLM للبحث في الشجرة\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "لقد تم إعطاؤك سؤالاً وبنية شجرة لمستند.\n",
        "تحتوي كل عقدة على معرف العقدة وعنوان العقدة وملخص مقابل.\n",
        "مهمتك هي العثور على جميع العقد التي من المحتمل أن تحتوي على إجابة للسؤال.\n",
        "\n",
        "السؤال: {query}\n",
        "\n",
        "بنية شجرة المستند:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "يرجى الرد بتنسيق JSON التالي:\n",
        "{{\n",
        "    \"thinking\": \"<عملية تفكيرك حول العقد ذات الصلة بالسؤال>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "قم بإرجاع بنية JSON النهائية مباشرة. لا تخرج أي شيء آخر.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nجاري استدعاء LLM المحلي للبحث في الشجرة (قد يستغرق هذا بعض الوقت)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3\") # استخدام llama3 أو النموذج المحلي المفضل لديك\n",
        "\n",
        "# 2.2 طباعة العقد المسترجعة وعملية التفكير\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nعملية التفكير:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'لم تقدم LLM عملية تفكير.'))\n",
        "\n",
        "    print('\\nالعقد المسترجعة:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"لم تسترجع LLM أي عقد.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # معرف العقدة من LLM قد يكون فقط '0019'، يحتاج إلى إضافة doc_id إذا كانت هذه هي طريقة التخزين\n",
        "        # بالنسبة لهذا العميل الوهمي، تكون معرفات العقد مثل \"doc_id_0019\"\n",
        "        # دعنا نضبط للبنية الوهمية إذا كانت LLM تخرج الرقم فقط\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"معرف العقدة: {node['node_id']}\\t الصفحة: {node.get('page_index', 'N/A')}\\t العنوان: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"معرف العقدة: {actual_node_id} (غير موجود في الخريطة - قد تكون LLM قد هلوست أو عدم تطابق تنسيق المعرف)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nخطأ: لم تُرجع LLM JSON صالحًا لنتيجة البحث في الشجرة:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nحدث خطأ أثناء معالجة نتيجة البحث في الشجرة: {e}\")\n",
        "\n",
        "# 3.1 استخراج السياق ذي الصلة من العقد المسترجعة\n",
        "# استخدم node_map للحصول على النص الكامل للعقد المحددة\n",
        "# إعادة تحليل نتيجة LLM في حالة وجود خطأ في الكتلة السابقة\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nالسياق المسترجع:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"لم يتم العثور على سياق ذي صلة بناءً على قائمة عقد LLM.\")\n",
        "\n",
        "# 3.2 إنشاء إجابة بناءً على السياق المسترجع\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    أجب على السؤال بناءً على السياق:\\n\\nالسؤال: {query}\\nالسياق: {relevant_content_str}\\n\\nقدم إجابة واضحة وموجزة بناءً فقط على السياق المقدم.\n",
        "    \"\"\"\n",
        "    print('\\nجاري استدعاء LLM المحلي لإنشاء الإجابة...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3\") # استخدام llama3 أو النموذج المحلي المفضل لديك\n",
        "    print('\\nالإجابة المُنشأة:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nلا يمكن إنشاء إجابة حيث لم يتم استرداد أي سياق ذي صلة.\")"
      ],
      "metadata": {
        "id": "MEwtDB3XP9fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "التغييرات الرئيسية:\n",
        "\n",
        "اللغة: تحويل جميع النصوص إلى اللغة الإنجليزية فقط\n",
        "\n",
        "مسار الكتاب: تغيير استخدام مسار الملف المحلي بدلاً من التنزيل من الإنترنت\n",
        "\n",
        "استخراج ملفات PDF: إضافة PyPDF2 لاستخراج نصوص PDF محليًا\n",
        "\n",
        "معالجة الأخطاء: الحفاظ على معالجة الأخطاء مع الرجوع إلى المحتوى الافتراضي\n",
        "\n",
        "أنماط الأقسام: تحديث عناوين أقسام المستندات الإنجليزية الشائعة\n",
        "\n",
        "الهيكل: الحفاظ على هيكل الكود الأصلي ووظائفه\n",
        "\n",
        "يحافظ الكود على جميع وظائفه الأصلية عند استخدام ملف كتاب محلي بدلاً من التنزيل من الإنترنت. التغيير الوحيد هو مصدر المستند - كل شيء آخر يعمل تمامًا كما هو في الكود الأصلي."
      ],
      "metadata": {
        "id": "JuxsoQ1aS5tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai\n",
        "%pip install -q requests beautifulsoup4 PyPDF2\n",
        "\n",
        "import os\n",
        "import json\n",
        "import PyPDF2\n",
        "import httpx\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_')\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # Simple implementation to create a tree from text\n",
        "        sections = []\n",
        "        lines = text_content.split('\\n')\n",
        "        current_section = []\n",
        "        section_title = \"Document Overview\"\n",
        "        node_counter = 0\n",
        "\n",
        "        section_patterns = [\n",
        "            \"Introduction\", \"Chapter\", \"Section\", \"Conclusion\", \"Summary\",\n",
        "            \"References\", \"Appendix\", \"Index\", \"Abstract\"\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = None\n",
        "        current_section_content = []\n",
        "        current_page_index = 1\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            is_new_section = False\n",
        "            for pattern in section_patterns:\n",
        "                if stripped_line.startswith(pattern) and len(stripped_line) < 100:\n",
        "                    if current_section_title and current_section_content:\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                    current_section_title = pattern\n",
        "                    current_section_content = [stripped_line]\n",
        "                    is_new_section = True\n",
        "                    current_page_index += 1\n",
        "                    break\n",
        "            if not is_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        if current_section_title and current_section_content:\n",
        "             parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        tree_nodes = []\n",
        "        node_map_for_subnodes = {}\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Python Learning Book\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"# Python Learning Book\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        for section in parsed_sections:\n",
        "            summary = section[\"text\"].split(' ', 20)\n",
        "            summary = ' '.join(summary[:20]) + \"...\" if len(summary) > 20 else ' '.join(summary)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{node_counter:04d}\",\n",
        "                \"summary\": summary,\n",
        "                \"text\": section[\"text\"],\n",
        "                \"page_index\": section[\"page_index\"]\n",
        "            }\n",
        "\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# Specify local book path\n",
        "book_path = \"/content/data/Understanding_Climate_Change.pdf\"  # Change this path as needed\n",
        "\n",
        "# Verify book exists\n",
        "if not os.path.exists(book_path):\n",
        "    raise FileNotFoundError(f\"Book not found at specified path: {book_path}\")\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "try:\n",
        "    # Use PyPDF2 to extract text from local PDF\n",
        "    with open(book_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        extracted_text_parts = []\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                extracted_text_parts.append(f\"--- Page {page_num+1} ---\")\n",
        "                extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "\n",
        "    print(f\"Content extracted to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from PDF: {e}\")\n",
        "    # Use default content if extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Python Learning Book\n",
        "    ## Introduction\n",
        "    Python is a powerful and easy-to-learn programming language. This book aims to help you learn the basics and advanced concepts.\n",
        "    ## Chapter 1: Python Basics\n",
        "    In this chapter we will learn about variables, basic types, and basic operations in Python.\n",
        "    ## Chapter 2: Control Structures\n",
        "    We will learn about conditions, loops, and iteration in this chapter.\n",
        "    ## Conclusion\n",
        "    Python is an excellent language for both beginners and professionals.\n",
        "    \"\"\"\n",
        "    text_path = book_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used default text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "query = \"What are the causes of climate change in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baEuiibUQ-_R",
        "outputId": "9f846984-70ad-45d9-8716-8ee918c56aef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m194.6/232.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing dummy LocalPageIndexClient for local execution.\n",
            "Content extracted to /content/data/Understanding_Climate_Change.txt\n",
            "Document submitted locally: Understanding_Climate_Change_txt\n",
            "Document Submitted: Understanding_Climate_Change_txt\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Python Learning Book (ID: Understanding_Climate_Change_txt_0000)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "  Chapter (ID: 0020)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about the causes of climate change, so I will look for\n",
            "nodes that have 'causes' or 'greenhouse gases' in their summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: Chapter_2: Causes of Climate Change (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: Chapter_5: The Role of Technology in Climate Change (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIEgFHTqQ-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Climate_Change"
      ],
      "metadata": {
        "id": "wZb8ZTKNQ-4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## الكود الل فشل رابطه تم تعديل الرابط"
      ],
      "metadata": {
        "id": "64BXVs4QR8xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069.pdf\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1.html\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the main findings of this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "id": "JVWiJO3fR51p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "حدد الأسطر التي تريد التعليق عليها.\n",
        "اضغط على Ctrl + / (في معظم أنظمة التشغيل) أو Cmd + / (على macOS).\n",
        "سيؤدي هذا إلى إضافة # في بداية كل سطر محدد. وإذا قمت بتحديد الأسطر مرة أخرى وضغطت على نفس الاختصار، فسيتم إزالة علامة #.\n",
        "\n",
        "أتمنى أن يكون هذا مفيدًا! إذا كان لديك أي أسئلة أخرى، فلا تتردد في طرحها.\n",
        "\n"
      ],
      "metadata": {
        "id": "avGq2FKxUBMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --upgrade pageindex\n",
        "%pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "%pip install -q requests beautifulsoup4  # For document downloading and parsing"
      ],
      "metadata": {
        "id": "DS7LYIWeTvRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the main findings of this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_nl1ceUXDfC",
        "outputId": "6d1e900f-f372-4edc-d69a-bbf546d508bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "Error connecting to Ollama: All connection attempts failed\n",
            "Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\n",
            "\n",
            "Error: LLM did not return valid JSON for tree search result:\n",
            "ERROR: Could not connect to Ollama. Please check your Ollama setup.\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the main findings of this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxUy6PlcXg9L",
        "outputId": "ef9d7062-553b-4268-8a0b-4c131ebe6ef1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking for main findings, so I will look for nodes that have\n",
            "'findings' or 'results' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "What are the conclusions in this document?"
      ],
      "metadata": {
        "id": "uzEgtEJKX84C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z82jWQcLYLja",
        "outputId": "455dc24d-1324-4d9f-a225-fcd137e60836"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in this document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: 2508_21069v1_0004 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Conclusion"
      ],
      "metadata": {
        "id": "LPWdd0SAZJ6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the Conclusion in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9QuMgoBZL6S",
        "outputId": "5e7a26ac-0baa-473d-a91b-174e1a39d9e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "Since we're looking for 'Conclusion' in this document, I'll focus on nodes that\n",
            "have a title or summary containing the word 'Conclusion'.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        # In a real scenario, you'd process the document here to generate a tree.\n",
        "        # For this example, we'll manually create a simplified tree for demonstration.\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This function attempts to parse the text content into a hierarchical structure\n",
        "        # based on common academic paper headings.\n",
        "\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\", # General root title\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        # Define common academic section patterns to look for\n",
        "        section_patterns = [\n",
        "            (\"## Abstract\", \"Abstract\"),\n",
        "            (\"## 1 Introduction\", \"1. Introduction\"),\n",
        "            (\"## 1.1\", \"1.1\"),\n",
        "            (\"## 2 Related Work\", \"2. Related Work\"),\n",
        "            (\"## 3 Method\", \"3. Method\"),\n",
        "            (\"## 3.1\", \"3.1\"),\n",
        "            (\"## 4 Experiments\", \"4. Experiments\"),\n",
        "            (\"## 4.1\", \"4.1\"),\n",
        "            (\"## 5 Conclusion and Future Work\", \"5. Conclusion and Future Work\"),\n",
        "            (\"## References\", \"References\"),\n",
        "            (\"## Appendix\", \"Appendix\")\n",
        "        ]\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_section_title = \"Document Root Content\" # Default for un-sectioned content\n",
        "        current_section_content = []\n",
        "        node_counter = 0\n",
        "        current_page_index = 1\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "\n",
        "        # First pass: Identify all main sections and their content\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            found_new_section = False\n",
        "            for pattern_prefix, canonical_title in section_patterns:\n",
        "                if stripped_line.startswith(pattern_prefix):\n",
        "                    if current_section_content and current_section_title != \"Document Root Content\":\n",
        "                        # Save the previous section\n",
        "                        parsed_sections.append({\n",
        "                            \"title\": current_section_title,\n",
        "                            \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                            \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                            \"page_index\": current_page_index\n",
        "                        })\n",
        "                        node_counter += 1\n",
        "                        current_page_index += 1 # Simulate page increment for new section\n",
        "\n",
        "                    current_section_title = canonical_title\n",
        "                    current_section_content = [stripped_line]\n",
        "                    found_new_section = True\n",
        "                    break\n",
        "\n",
        "            if not found_new_section:\n",
        "                current_section_content.append(line)\n",
        "\n",
        "        # Add the last section\n",
        "        if current_section_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_section_title,\n",
        "                \"text\": \"\\n\".join(current_section_content).strip(),\n",
        "                \"node_id\": f\"{doc_id}_{node_counter:04d}\",\n",
        "                \"page_index\": current_page_index\n",
        "            })\n",
        "\n",
        "        # Second pass: Build the hierarchical tree\n",
        "        node_map_for_nesting = {root_node[\"node_id\"]: root_node} # Map to easily find parents\n",
        "\n",
        "        for section_data in parsed_sections:\n",
        "            node_id = section_data[\"node_id\"]\n",
        "            title = section_data[\"title\"]\n",
        "            text = section_data[\"text\"]\n",
        "\n",
        "            summary_words = text.split(' ', 20)\n",
        "            summary = ' '.join(summary_words[:20]) + \"...\" if len(summary_words) > 20 else ' '.join(summary_words)\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": title,\n",
        "                \"node_id\": node_id,\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": section_data[\"page_index\"]\n",
        "            }\n",
        "            node_map_for_nesting[node_id] = node_entry # Add to map for potential children\n",
        "\n",
        "            # Simple heuristic for nesting (based on \"1.\" \"1.1.\" \"2.\" etc.)\n",
        "            parent_added = False\n",
        "            if title.startswith(\"1.\") and len(title.split('.')) == 2: # e.g., \"1. Introduction\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"1.1\") or title.startswith(\"1.2\"): # e.g., \"1.1. Contributions\"\n",
        "                intro_node = next((n for n in root_node[\"nodes\"] if n[\"title\"].startswith(\"1.\") and len(n[\"title\"].split('.')) == 2), None)\n",
        "                if intro_node:\n",
        "                    if \"nodes\" not in intro_node: intro_node[\"nodes\"] = []\n",
        "                    intro_node[\"nodes\"].append(node_entry)\n",
        "                    parent_added = True\n",
        "            elif title.startswith(\"2.\") and len(title.split('.')) == 2: # e.g., \"2. Related Work\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"3.\") and len(title.split('.')) == 2: # e.g., \"3. Method\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"4.\") and len(title.split('.')) == 2: # e.g., \"4. Experiments\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title.startswith(\"5.\") and len(title.split('.')) == 2: # e.g., \"5. Conclusion...\"\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "            elif title == \"Abstract\" or title == \"References\" or title == \"Appendix\":\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "                parent_added = True\n",
        "\n",
        "            if not parent_added and title != \"Document Root Content\":\n",
        "                # If not specifically nested, add to root\n",
        "                root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        # Ensure no duplicate entries in root_node if some sections were implicitly added twice\n",
        "        final_root_nodes = []\n",
        "        seen_node_ids = set()\n",
        "        for node in root_node[\"nodes\"]:\n",
        "            if node[\"node_id\"] not in seen_node_ids:\n",
        "                final_root_nodes.append(node)\n",
        "                seen_node_ids.add(node[\"node_id\"])\n",
        "        root_node[\"nodes\"] = final_root_nodes\n",
        "\n",
        "        return [root_node]\n",
        "\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3:latest\", temperature=0):\n",
        "    # Ensure Ollama server is running and model is pulled\n",
        "    # Example: ollama run llama3\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None # Disable timeout for potentially long responses\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\")\n",
        "        print(\"Please ensure Ollama is running (e.g., 'ollama serve') and you have pulled the model (e.g., 'ollama pull llama3').\")\n",
        "        return \"ERROR: Could not connect to Ollama. Please check your Ollama setup.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"Error from Ollama server: {e.response.status_code} - {e.response.text}\")\n",
        "        return f\"ERROR: Ollama server responded with an error: {e.response.status_code}\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from Ollama: {response.text}\")\n",
        "        return \"ERROR: Invalid JSON response from Ollama.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "# CHANGED: New URL for the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2508.21069\" # This is a placeholder as .pdf might not exist for future\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\" # Direct HTML link\n",
        "\n",
        "pdf_filename = arxiv_html_url.split('/')[-1].replace('.html', '.pdf') # Simulate pdf filename from html\n",
        "pdf_path = os.path.join(\"data\", pdf_filename) # Use a 'data' directory\n",
        "\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    # Extract main content - this will be highly dependent on arXiv's HTML structure\n",
        "    extracted_text_parts = []\n",
        "    # Look for common article structure elements\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'section']):\n",
        "        text = tag.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            extracted_text_parts.append(text)\n",
        "\n",
        "    dummy_text_content = \"\\n\\n\".join(extracted_text_parts)\n",
        "    if not dummy_text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    print(\"Falling back to a very minimal dummy text for tree generation.\")\n",
        "    # UPDATED dummy text for the new URL if HTML extraction fails\n",
        "    dummy_text_content = \"\"\"\n",
        "    # Deep Learning for Climate Model Emulation\n",
        "    ## Abstract\n",
        "    This paper explores the use of deep learning models to emulate complex climate simulations, offering faster predictions and insights. We demonstrate that neural networks can accurately reproduce outputs of sophisticated climate models.\n",
        "    ## 1 Introduction\n",
        "    Climate change research relies heavily on complex numerical simulations. These are computationally expensive. Deep learning offers a promising alternative.\n",
        "    ## 2 Related Work\n",
        "    Previous studies have used machine learning for atmospheric processes. Our work focuses on full climate model emulation.\n",
        "    ## 3 Method\n",
        "    We employed a U-Net architecture trained on high-resolution climate model data. Input features include atmospheric variables; outputs are future climate states.\n",
        "    ## 4 Experiments\n",
        "    Our experiments show that the DL emulator achieves high fidelity compared to the full climate model across various metrics, including temperature and precipitation patterns. It significantly reduces computational time.\n",
        "    ## 5 Conclusion and Future Work\n",
        "    Deep learning emulation is a powerful tool for climate science. We conclude that DL models can effectively surrogate complex physical processes. Future work includes expanding to different climate models and uncertainty quantification.\n",
        "    ## References\n",
        "    [1] Smith et al. Climate Modeling.\n",
        "    [2] Jones et al. Deep Learning for Earth Systems.\n",
        "    \"\"\"\n",
        "    text_path = pdf_path.replace('.pdf', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(dummy_text_content)\n",
        "    print(f\"Used minimal dummy text and saved to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "# This will use the simplified tree generated by our LocalPageIndexClient\n",
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")\n",
        "\n",
        "# 2.1 Use LLM for tree search and identify nodes that might contain relevant context\n",
        "# CHANGED: New query\n",
        "query = \"What are the conclusion in this document?\"\n",
        "\n",
        "# Remove the 'text' field to avoid sending too much data to the LLM for tree search\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "\n",
        "# 2.2 Print retrieved nodes and reasoning process\n",
        "try:\n",
        "    node_map = utils.create_node_mapping(tree)\n",
        "    tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided by LLM.'))\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "    if not retrieved_node_ids:\n",
        "        print(\"No nodes retrieved by LLM.\")\n",
        "    for node_id_key in retrieved_node_ids:\n",
        "        # The node_id from the LLM might be just '0019', need to prepend doc_id if that's how we store\n",
        "        # For this dummy client, the node_ids are like \"doc_id_0019\"\n",
        "        # Let's adjust for the dummy structure if LLM outputs just the number\n",
        "        actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "\n",
        "        node = node_map.get(actual_node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Node ID: {actual_node_id} (Not found in map - LLM might have hallucinated or ID format mismatch)\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "\n",
        "\n",
        "# 3.1 Extract relevant context from retrieved nodes\n",
        "# Use the node_map to get the full text of the identified nodes\n",
        "# Re-parse LLM result in case of error in previous block\n",
        "try:\n",
        "    retrieved_node_ids = json.loads(tree_search_result).get(\"node_list\", [])\n",
        "except json.JSONDecodeError:\n",
        "    retrieved_node_ids = []\n",
        "\n",
        "relevant_content = []\n",
        "for node_id_key in retrieved_node_ids:\n",
        "    actual_node_id = f\"{doc_id}_{node_id_key}\" if '_' not in node_id_key and not node_id_key.startswith(doc_id) else node_id_key\n",
        "    node = node_map.get(actual_node_id)\n",
        "    if node and 'text' in node:\n",
        "        relevant_content.append(node['text'])\n",
        "\n",
        "relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "print('\\nRetrieved Context:\\n')\n",
        "if relevant_content_str:\n",
        "    utils.print_wrapped(relevant_content_str[:1000] + '...' if len(relevant_content_str) > 1000 else relevant_content_str)\n",
        "else:\n",
        "    print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "\n",
        "# 3.2 Generate answer based on retrieved context\n",
        "if relevant_content_str:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\\n\\nQuestion: {query}\\nContext: {relevant_content_str}\\n\\nProvide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "    print('\\nCalling local LLM for answer generation...')\n",
        "    answer = await call_llm(answer_prompt, model=\"llama3:latest\") # Use llama3 or your preferred local model\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvzKH8RjahUC",
        "outputId": "9eeb2d12-0daf-4532-82fa-be522fca38af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about conclusions in the document, so I will look for\n",
            "nodes that have 'conclusion' or 'summary' in their title or summary.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "Node ID: 2508_21069v1_0003 (Not found in map - LLM might have hallucinated or ID format mismatch)\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "No relevant context found based on LLM's node list.\n",
            "\n",
            "Cannot generate an answer as no relevant context was retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "auQIFmorbUN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "نجاح: الكود نجح في تحميل المحتوى من الرابط الجديد https://arxiv.org/html/2508.21069v1. هذا رائع!\n",
        "فشل: المشكلة الأساسية كانت في دالة _generate_simple_tree_from_text. لقد كانت مصممة لتحليل \"النص الوهمي\" الذي يحتوي على علامات مثل ##، لكنها فشلت في تحليل النص الحقيقي الذي تم سحبه من صفحة الويب، والذي لا يحتوي على هذه العلامات. ونتيجة لذلك، قامت بإنشاء \"شجرة\" تحتوي على عقدة جذرية واحدة فقط، مما جعلها عديمة الفائدة.\n",
        "هلوسة النموذج (LLM): عندما تم إعطاء نموذج اللغة شجرة فارغة وطلب منه العثور على \"الخاتمة\"، قام باختلاق (هلوسة) مُعرّف عقدة (_0003) لأنه لم يجد أي شيء حقيقي في السياق الذي قُدم له.\n",
        "النتيجة النهائية: بما أن مُعرّف العقدة المُختلق غير موجود في خريطة العقد (لأن الخريطة كانت فارغة تقريبًا)، فشل البرنامج في استرداد أي سياق ولم يتمكن من توليد إجابة.\n",
        "الحل:\n",
        "الحل هو إعادة كتابة دالة _generate_simple_tree_from_text لتكون أكثر ذكاءً وقوة، بحيث يمكنها تحليل النص الفعلي المستخرج من الويب. النسخة الجديدة ستقوم بالبحث عن عناوين الأقسام الشائعة (مثل \"Abstract\", \"Introduction\", \"Conclusion\") بطريقة أكثر مرونة، دون الاعتماد على علامات ##.\n",
        "لقد قمت بتعديل هذه الدالة بشكل كبير لتقسيم النص إلى أقسام بشكل صحيح، مما سيبني شجرة مستندات مفيدة يمكن للنموذج اللغوي استخدامها.\n",
        "الكود الكامل مع التعديلات:\n",
        "إليك الكود الكامل والمصحح. التغيير الرئيسي موجود داخل الكلاس LocalPageIndexClient في دالة _generate_simple_tree_from_text.\n",
        "code\n",
        "Python\n"
      ],
      "metadata": {
        "id": "nzgD7mKrbNE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    # --- START OF MAJOR CORRECTION ---\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a more robust parser for the actual scraped HTML text.\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_title = \"Preamble\"  # Content before the first proper header\n",
        "        current_content = []\n",
        "\n",
        "        # More flexible headers to look for in the scraped text\n",
        "        # These are keywords that often start a section title.\n",
        "        header_keywords = [\n",
        "            \"Abstract\", \"Introduction\", \"Related Work\", \"Method\",\n",
        "            \"Experiments\", \"Conclusion\", \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if not stripped:\n",
        "                continue\n",
        "\n",
        "            found_header = None\n",
        "            # Heuristic: A line is likely a header if it's short and starts with a keyword or a number.\n",
        "            if len(stripped.split()) < 10:\n",
        "                for keyword in header_keywords:\n",
        "                    # Check for \"1 Introduction\", \"5. Conclusion\", \"Abstract\", etc.\n",
        "                    if (stripped.startswith(keyword) or (len(stripped) > 1 and stripped[0].isdigit() and keyword in stripped)):\n",
        "                        found_header = stripped\n",
        "                        break\n",
        "\n",
        "            if found_header:\n",
        "                # If we found a new header, save the previous section's content\n",
        "                if current_content:\n",
        "                    parsed_sections.append({\n",
        "                        \"title\": current_title,\n",
        "                        \"text\": \"\\n\".join(current_content).strip()\n",
        "                    })\n",
        "                # Start the new section\n",
        "                current_title = found_header\n",
        "                current_content = [] # Reset content buffer\n",
        "            else:\n",
        "                # This line is content for the current section\n",
        "                current_content.append(line)\n",
        "\n",
        "        # After the loop, save the very last section\n",
        "        if current_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_title,\n",
        "                \"text\": \"\\n\".join(current_content).strip()\n",
        "            })\n",
        "\n",
        "        # Now, build the tree from the correctly parsed sections\n",
        "        node_counter = 1\n",
        "        for i, section in enumerate(parsed_sections):\n",
        "            text = section[\"text\"]\n",
        "            summary = ' '.join(text.split()[:20]) + '...' if len(text.split()) > 20 else text\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{doc_id}_{i+1:04d}\", # Simple, sequential node IDs\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": i + 1\n",
        "            }\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "    # --- END OF MAJOR CORRECTION ---\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\\nPlease ensure Ollama is running.\")\n",
        "        return '{\"thinking\": \"Error: Could not connect to Ollama.\", \"node_list\": []}'\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return '{\"thinking\": \"An unexpected error occurred.\", \"node_list\": []}'\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\"\n",
        "doc_name = arxiv_html_url.split('/')[-1]\n",
        "doc_path = os.path.join(\"data\", doc_name)\n",
        "\n",
        "os.makedirs(os.path.dirname(doc_path), exist_ok=True)\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', class_='ltx_page_content')\n",
        "    if article_body:\n",
        "        text_content = article_body.get_text(separator='\\n', strip=True)\n",
        "    else: # Fallback if specific class not found\n",
        "        text_content = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    if not text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = doc_path.replace('.html', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    doc_id = None # Ensure doc_id is None if setup fails\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "if 'doc_id' in locals() and doc_id and pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"\\nSkipping further steps because document processing failed.\")\n",
        "    tree = None\n",
        "\n",
        "if tree:\n",
        "    # 2.1 Use LLM for tree search\n",
        "    query = \"What are the main findings of this document?\"\n",
        "    tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "    search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id and a title. Your task is to find all nodes that are likely to contain the answer to the question. Focus on sections like 'Abstract', 'Experiments', 'Results', or 'Conclusion'.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply ONLY with a JSON object in the following format, with no other text:\n",
        "{{\n",
        "    \"thinking\": \"<Your brief thinking process on which nodes are relevant>\",\n",
        "    \"node_list\": [\"<full_node_id_1>\", \"<full_node_id_2>\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "    tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\")\n",
        "\n",
        "    # 2.2 Print retrieved nodes and reasoning process\n",
        "    try:\n",
        "        node_map = utils.create_node_mapping(tree)\n",
        "        tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "        print('\\nReasoning Process:')\n",
        "        utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided.'))\n",
        "\n",
        "        print('\\nRetrieved Nodes:')\n",
        "        retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "        if not retrieved_node_ids:\n",
        "            print(\"No nodes retrieved by LLM.\")\n",
        "\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node:\n",
        "                print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "            else:\n",
        "                print(f\"Node ID: {node_id} (Not found in map - LLM may have hallucinated or ID format is mismatched)\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "        retrieved_node_ids = []\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "        retrieved_node_ids = []\n",
        "\n",
        "    # 3.1 Extract relevant context\n",
        "    relevant_content = []\n",
        "    if retrieved_node_ids:\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node and 'text' in node:\n",
        "                relevant_content.append(f\"--- From Section: {node['title']} ---\\n{node['text']}\")\n",
        "\n",
        "    relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "    print('\\nRetrieved Context:\\n')\n",
        "    if relevant_content_str:\n",
        "        utils.print_wrapped(relevant_content_str[:1500] + '...' if len(relevant_content_str) > 1500 else relevant_content_str)\n",
        "    else:\n",
        "        print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "    # 3.2 Generate answer\n",
        "    if relevant_content_str:\n",
        "        answer_prompt = f\"Based ONLY on the following context, what are the main findings of the document?\\n\\nContext:\\n{relevant_content_str}\\n\\nAnswer:\"\n",
        "\n",
        "        print('\\nCalling local LLM for answer generation...')\n",
        "        answer = await call_llm(answer_prompt, model=\"llama3:latest\")\n",
        "        print('\\nGenerated Answer:\\n')\n",
        "        utils.print_wrapped(answer)\n",
        "    else:\n",
        "        print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HpFzX6Ehahrt",
        "outputId": "6ae6556d-9efc-4a27-824d-ace551646e41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "  Preamble (ID: 2508_21069v1_0001)\n",
            "  Abstract (ID: 2508_21069v1_0002)\n",
            "  Introduction (ID: 2508_21069v1_0003)\n",
            "  Methods (ID: 2508_21069v1_0004)\n",
            "  Conclusion (ID: 2508_21069v1_0005)\n",
            "  References (ID: 2508_21069v1_0006)\n",
            "  Appendix A (ID: 2508_21069v1_0007)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about the main findings of this document, so I'm looking\n",
            "for sections that summarize or conclude the results. This suggests 'Abstract',\n",
            "'Conclusion', and possibly 'Results' if it exists in the tree.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002\t Page: 2\t Title: Abstract\n",
            "Node ID: 2508_21069v1_0005\t Page: 5\t Title: Conclusion\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "--- From Section: Abstract --- We present constraints on the reionization\n",
            "optical depth, τ \\tau , obtained using several independent methods. First, we\n",
            "perform a non-parametric reconstruction of the reionization history, using\n",
            "Lyman- α \\alpha constraints on the evolution of the volume-averaged neutral\n",
            "hydrogen fraction, x HI ​ ( z ) x_{\\mathrm{HI}}(z) , including recent results\n",
            "from the James Webb Space Telescope . When combined with baryon acoustic\n",
            "oscillation (BAO) measurements from DESI and Big Bang nucleosynthesis\n",
            "constraints, these data imply a rapid reionization history ( z mid = 7.00 − 0.18\n",
            "+ 0.12 z_{\\mathrm{mid}}=7.00^{+0.12}_{-0.18} and Δ ​ z 50 = 1.12 − 0.29 + 0.12\n",
            "\\Delta z_{50}=1.12^{+0.12}_{-0.29} ) and a value of τ = 0.0492 − 0.0030 + 0.0014\n",
            "\\tau=0.0492^{+0.0014}_{-0.0030} , which is largely insensitive to the assumed\n",
            "cosmological model and independent of cosmic microwave background (CMB) data.\n",
            "The optical depth can also be measured from large-scale ( ℓ < 30 ) (\\ell<30) CMB\n",
            "polarization data, yielding constraints that are similarly model-insensitive and\n",
            "consistent with the Ly α \\alpha bound. Third, τ \\tau may be constrained from the\n",
            "attenuation of small-scale ( ℓ > 30 ) (\\ell>30) CMB anisotropies, but the\n",
            "results are sensitive to the choice of cosmological model. Assuming Λ \\Lambda\n",
            "CDM and combining small-scale CMB data with CMB lensing and type 1a supernovae\n",
            "(SNe) yields tight constraints that are compatible with the Ly α \\alpha bound.\n",
            "Adding galaxy clustering a...\n",
            "\n",
            "Calling local LLM for answer generation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-762829457.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCalling local LLM for answer generation...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nGenerated Answer:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-762829457.py\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(prompt, model, temperature)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http://localhost:11434\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         response = await client.post(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;34m\"/api/chat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             json={\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \"\"\"\n\u001b[0;32m-> 1859\u001b[0;31m         return await self.request(\n\u001b[0m\u001b[1;32m   1860\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         )\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0masynccontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = await self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = await self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             ):\n\u001b[1;32m   1253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/locks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data"
      ],
      "metadata": {
        "id": "lfKLB1pzb9tc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m4CMFCvQcCxc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bvZluHBhj2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF COLAB CELL ---\n",
        "\n",
        "# 0.0 Setup Environment and Install Dependencies\n",
        "# This cell will install necessary packages and prepare the environment.\n",
        "\n",
        "# Install PageIndex (if not already installed)\n",
        "# %pip install -q --upgrade pageindex\n",
        "# %pip install -q --upgrade openai  # For potential future use or local OpenAI-compatible servers\n",
        "# %pip install -q requests beautifulsoup4  # For document downloading and parsing\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Dummy PageIndexClient for local execution (no actual API calls)\n",
        "class LocalPageIndexClient:\n",
        "    def __init__(self, api_key=None):\n",
        "        print(\"Using dummy LocalPageIndexClient for local execution.\")\n",
        "        self.documents = {} # Store document content\n",
        "        self.trees = {} # Store generated tree structures\n",
        "\n",
        "    def submit_document(self, file_path):\n",
        "        doc_id = os.path.basename(file_path).replace('.', '_').replace('-', '_') # Simple doc_id generation\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.documents[doc_id] = f.read()\n",
        "        print(f\"Document submitted locally: {doc_id}\")\n",
        "        self.trees[doc_id] = self._generate_simple_tree_from_text(self.documents[doc_id], doc_id)\n",
        "        return {\"doc_id\": doc_id}\n",
        "\n",
        "    def is_retrieval_ready(self, doc_id):\n",
        "        return doc_id in self.trees\n",
        "\n",
        "    def get_tree(self, doc_id, node_summary=True):\n",
        "        return {\"result\": self.trees.get(doc_id, [])}\n",
        "\n",
        "    # --- START OF MAJOR CORRECTION ---\n",
        "    def _generate_simple_tree_from_text(self, text_content, doc_id):\n",
        "        # This is a more robust parser for the actual scraped HTML text.\n",
        "        root_node = {\n",
        "            \"title\": \"Document Root\",\n",
        "            \"node_id\": f\"{doc_id}_0000\",\n",
        "            \"prefix_summary\": \"Root of the document structure.\",\n",
        "            \"nodes\": [],\n",
        "            \"page_index\": 1\n",
        "        }\n",
        "\n",
        "        parsed_sections = []\n",
        "        current_title = \"Preamble\"  # Content before the first proper header\n",
        "        current_content = []\n",
        "\n",
        "        # More flexible headers to look for in the scraped text\n",
        "        # These are keywords that often start a section title.\n",
        "        header_keywords = [\n",
        "            \"Abstract\", \"Introduction\", \"Related Work\", \"Method\",\n",
        "            \"Experiments\", \"Conclusion\", \"References\", \"Appendix\"\n",
        "        ]\n",
        "\n",
        "        lines = text_content.split('\\n')\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if not stripped:\n",
        "                continue\n",
        "\n",
        "            found_header = None\n",
        "            # Heuristic: A line is likely a header if it's short and starts with a keyword or a number.\n",
        "            if len(stripped.split()) < 10:\n",
        "                for keyword in header_keywords:\n",
        "                    # Check for \"1 Introduction\", \"5. Conclusion\", \"Abstract\", etc.\n",
        "                    if (stripped.startswith(keyword) or (len(stripped) > 1 and stripped[0].isdigit() and keyword in stripped)):\n",
        "                        found_header = stripped\n",
        "                        break\n",
        "\n",
        "            if found_header:\n",
        "                # If we found a new header, save the previous section's content\n",
        "                if current_content:\n",
        "                    parsed_sections.append({\n",
        "                        \"title\": current_title,\n",
        "                        \"text\": \"\\n\".join(current_content).strip()\n",
        "                    })\n",
        "                # Start the new section\n",
        "                current_title = found_header\n",
        "                current_content = [] # Reset content buffer\n",
        "            else:\n",
        "                # This line is content for the current section\n",
        "                current_content.append(line)\n",
        "\n",
        "        # After the loop, save the very last section\n",
        "        if current_content:\n",
        "            parsed_sections.append({\n",
        "                \"title\": current_title,\n",
        "                \"text\": \"\\n\".join(current_content).strip()\n",
        "            })\n",
        "\n",
        "        # Now, build the tree from the correctly parsed sections\n",
        "        node_counter = 1\n",
        "        for i, section in enumerate(parsed_sections):\n",
        "            text = section[\"text\"]\n",
        "            summary = ' '.join(text.split()[:20]) + '...' if len(text.split()) > 20 else text\n",
        "\n",
        "            node_entry = {\n",
        "                \"title\": section[\"title\"],\n",
        "                \"node_id\": f\"{doc_id}_{i+1:04d}\", # Simple, sequential node IDs\n",
        "                \"summary\": summary,\n",
        "                \"text\": text,\n",
        "                \"page_index\": i + 1\n",
        "            }\n",
        "            root_node[\"nodes\"].append(node_entry)\n",
        "\n",
        "        return [root_node]\n",
        "    # --- END OF MAJOR CORRECTION ---\n",
        "\n",
        "# Dummy utils for local execution (mimicking pageindex.utils)\n",
        "class LocalUtils:\n",
        "    @staticmethod\n",
        "    def print_tree(tree, indent=0):\n",
        "        for node in tree:\n",
        "            prefix = \"  \" * indent\n",
        "            print(f\"{prefix}{node.get('title', 'Untitled')} (ID: {node.get('node_id', 'N/A')})\")\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.print_tree(node[\"nodes\"], indent + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_fields(obj, fields):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: LocalUtils.remove_fields(v, fields) for k, v in obj.items() if k not in fields}\n",
        "        elif isinstance(obj, list):\n",
        "            return [LocalUtils.remove_fields(elem, fields) for elem in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    @staticmethod\n",
        "    def create_node_mapping(tree, node_map=None):\n",
        "        if node_map is None:\n",
        "            node_map = {}\n",
        "        for node in tree:\n",
        "            node_map[node.get('node_id')] = node\n",
        "            if \"nodes\" in node:\n",
        "                LocalUtils.create_node_mapping(node[\"nodes\"], node_map)\n",
        "        return node_map\n",
        "\n",
        "    @staticmethod\n",
        "    def print_wrapped(text, width=80):\n",
        "        import textwrap\n",
        "        print(textwrap.fill(text, width=width))\n",
        "\n",
        "pi_client = LocalPageIndexClient()\n",
        "utils = LocalUtils()\n",
        "\n",
        "# Ollama LLM client for local inference\n",
        "import httpx\n",
        "\n",
        "async def call_llm(prompt, model=\"llama3\", temperature=0):\n",
        "    client = httpx.AsyncClient(base_url=\"http://localhost:11434\")\n",
        "    try:\n",
        "        response = await client.post(\n",
        "            \"/api/chat\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": temperature}\n",
        "            },\n",
        "            timeout=None\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"message\"][\"content\"].strip()\n",
        "    except httpx.ConnectError as e:\n",
        "        print(f\"Error connecting to Ollama: {e}\\nPlease ensure Ollama is running.\")\n",
        "        return '{\"thinking\": \"Error: Could not connect to Ollama.\", \"node_list\": []}'\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return '{\"thinking\": \"An unexpected error occurred.\", \"node_list\": []}'\n",
        "\n",
        "# 1.1 Submit a document for generating PageIndex tree (locally)\n",
        "arxiv_html_url = \"https://arxiv.org/html/2508.21069v1\"\n",
        "doc_name = arxiv_html_url.split('/')[-1]\n",
        "doc_path = os.path.join(\"data\", doc_name)\n",
        "\n",
        "os.makedirs(os.path.dirname(doc_path), exist_ok=True)\n",
        "\n",
        "print(f\"Attempting to fetch text from {arxiv_html_url} for content simulation...\")\n",
        "try:\n",
        "    html_response = requests.get(arxiv_html_url)\n",
        "    html_response.raise_for_status()\n",
        "    soup = BeautifulSoup(html_response.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', class_='ltx_page_content')\n",
        "    if article_body:\n",
        "        text_content = article_body.get_text(separator='\\n', strip=True)\n",
        "    else: # Fallback if specific class not found\n",
        "        text_content = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    if not text_content.strip():\n",
        "        raise ValueError(\"Could not extract significant text from arXiv HTML.\")\n",
        "\n",
        "    text_path = doc_path.replace('.html', '.txt')\n",
        "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_content)\n",
        "    print(f\"Extracted content to {text_path}\")\n",
        "    doc_id = pi_client.submit_document(text_path)[\"doc_id\"]\n",
        "    print('Document Submitted:', doc_id)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to extract text from arXiv HTML or save to file: {e}\")\n",
        "    doc_id = None # Ensure doc_id is None if setup fails\n",
        "\n",
        "# 1.2 Get the generated PageIndex tree structure\n",
        "if 'doc_id' in locals() and doc_id and pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('\\nSimplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"\\nSkipping further steps because document processing failed.\")\n",
        "    tree = None\n",
        "\n",
        "if tree:\n",
        "    # 2.1 Use LLM for tree search\n",
        "    query = \"What are the main findings of this document?\"\n",
        "    tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "    search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id and a title. Your task is to find all nodes that are likely to contain the answer to the question. Focus on sections like 'Abstract', 'Experiments', 'Results', or 'Conclusion'.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply ONLY with a JSON object in the following format, with no other text:\n",
        "{{\n",
        "    \"thinking\": \"<Your brief thinking process on which nodes are relevant>\",\n",
        "    \"node_list\": [\"<full_node_id_1>\", \"<full_node_id_2>\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    print(\"\\nCalling local LLM for tree search (this might take a moment)...\")\n",
        "    tree_search_result = await call_llm(search_prompt, model=\"llama3:latest\")\n",
        "\n",
        "    # 2.2 Print retrieved nodes and reasoning process\n",
        "    try:\n",
        "        node_map = utils.create_node_mapping(tree)\n",
        "        tree_search_result_json = json.loads(tree_search_result)\n",
        "\n",
        "        print('\\nReasoning Process:')\n",
        "        utils.print_wrapped(tree_search_result_json.get('thinking', 'No thinking process provided.'))\n",
        "\n",
        "        print('\\nRetrieved Nodes:')\n",
        "        retrieved_node_ids = tree_search_result_json.get(\"node_list\", [])\n",
        "        if not retrieved_node_ids:\n",
        "            print(\"No nodes retrieved by LLM.\")\n",
        "\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node:\n",
        "                print(f\"Node ID: {node['node_id']}\\t Page: {node.get('page_index', 'N/A')}\\t Title: {node['title']}\")\n",
        "            else:\n",
        "                print(f\"Node ID: {node_id} (Not found in map - LLM may have hallucinated or ID format is mismatched)\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"\\nError: LLM did not return valid JSON for tree search result:\\n{tree_search_result}\")\n",
        "        retrieved_node_ids = []\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during tree search result processing: {e}\")\n",
        "        retrieved_node_ids = []\n",
        "\n",
        "    # 3.1 Extract relevant context\n",
        "    relevant_content = []\n",
        "    if retrieved_node_ids:\n",
        "        for node_id in retrieved_node_ids:\n",
        "            node = node_map.get(node_id)\n",
        "            if node and 'text' in node:\n",
        "                relevant_content.append(f\"--- From Section: {node['title']} ---\\n{node['text']}\")\n",
        "\n",
        "    relevant_content_str = \"\\n\\n\".join(relevant_content)\n",
        "\n",
        "    print('\\nRetrieved Context:\\n')\n",
        "    if relevant_content_str:\n",
        "        utils.print_wrapped(relevant_content_str[:1500] + '...' if len(relevant_content_str) > 1500 else relevant_content_str)\n",
        "    else:\n",
        "        print(\"No relevant context found based on LLM's node list.\")\n",
        "\n",
        "    # 3.2 Generate answer\n",
        "    if relevant_content_str:\n",
        "        answer_prompt = f\"Based ONLY on the following context, what are the main findings of the document?\\n\\nContext:\\n{relevant_content_str}\\n\\nAnswer:\"\n",
        "\n",
        "        print('\\nCalling local LLM for answer generation...')\n",
        "        answer = await call_llm(answer_prompt, model=\"llama3:latest\")\n",
        "        print('\\nGenerated Answer:\\n')\n",
        "        utils.print_wrapped(answer)\n",
        "    else:\n",
        "        print(\"\\nCannot generate an answer as no relevant context was retrieved.\")\n",
        "\n",
        "# --- END OF COLAB CELL ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b4Xj6BUhjzj",
        "outputId": "1c24ae40-6dc1-4d10-c6e0-ac6ad2519506"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dummy LocalPageIndexClient for local execution.\n",
            "Attempting to fetch text from https://arxiv.org/html/2508.21069v1 for content simulation...\n",
            "Extracted content to data/2508.21069v1\n",
            "Document submitted locally: 2508_21069v1\n",
            "Document Submitted: 2508_21069v1\n",
            "\n",
            "Simplified Tree Structure of the Document:\n",
            "Document Root (ID: 2508_21069v1_0000)\n",
            "  Preamble (ID: 2508_21069v1_0001)\n",
            "  Abstract (ID: 2508_21069v1_0002)\n",
            "  Introduction (ID: 2508_21069v1_0003)\n",
            "  Methods (ID: 2508_21069v1_0004)\n",
            "  Conclusion (ID: 2508_21069v1_0005)\n",
            "  References (ID: 2508_21069v1_0006)\n",
            "  Appendix A (ID: 2508_21069v1_0007)\n",
            "\n",
            "Calling local LLM for tree search (this might take a moment)...\n",
            "\n",
            "Reasoning Process:\n",
            "The question is asking about the main findings of this document, so I'm looking\n",
            "for sections that summarize or conclude the results. This suggests 'Abstract',\n",
            "'Conclusion', and possibly 'Results' if it exists in the tree.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 2508_21069v1_0002\t Page: 2\t Title: Abstract\n",
            "Node ID: 2508_21069v1_0005\t Page: 5\t Title: Conclusion\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "--- From Section: Abstract --- We present constraints on the reionization\n",
            "optical depth, τ \\tau , obtained using several independent methods. First, we\n",
            "perform a non-parametric reconstruction of the reionization history, using\n",
            "Lyman- α \\alpha constraints on the evolution of the volume-averaged neutral\n",
            "hydrogen fraction, x HI ​ ( z ) x_{\\mathrm{HI}}(z) , including recent results\n",
            "from the James Webb Space Telescope . When combined with baryon acoustic\n",
            "oscillation (BAO) measurements from DESI and Big Bang nucleosynthesis\n",
            "constraints, these data imply a rapid reionization history ( z mid = 7.00 − 0.18\n",
            "+ 0.12 z_{\\mathrm{mid}}=7.00^{+0.12}_{-0.18} and Δ ​ z 50 = 1.12 − 0.29 + 0.12\n",
            "\\Delta z_{50}=1.12^{+0.12}_{-0.29} ) and a value of τ = 0.0492 − 0.0030 + 0.0014\n",
            "\\tau=0.0492^{+0.0014}_{-0.0030} , which is largely insensitive to the assumed\n",
            "cosmological model and independent of cosmic microwave background (CMB) data.\n",
            "The optical depth can also be measured from large-scale ( ℓ < 30 ) (\\ell<30) CMB\n",
            "polarization data, yielding constraints that are similarly model-insensitive and\n",
            "consistent with the Ly α \\alpha bound. Third, τ \\tau may be constrained from the\n",
            "attenuation of small-scale ( ℓ > 30 ) (\\ell>30) CMB anisotropies, but the\n",
            "results are sensitive to the choice of cosmological model. Assuming Λ \\Lambda\n",
            "CDM and combining small-scale CMB data with CMB lensing and type 1a supernovae\n",
            "(SNe) yields tight constraints that are compatible with the Ly α \\alpha bound.\n",
            "Adding galaxy clustering a...\n",
            "\n",
            "Calling local LLM for answer generation...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "Based on the provided context, the main findings of the document are:  1.\n",
            "Constraints on the reionization optical depth (τ) obtained using several\n",
            "independent methods:         * Non-parametric reconstruction of the reionization\n",
            "history from Lyman-α and Lyman-β constraints.         * Baryon acoustic\n",
            "oscillation (BAO) measurements from DESI and Big Bang nucleosynthesis\n",
            "constraints.         * Large-scale CMB polarization data. 2. The optical depth\n",
            "is found to be:         * τ = 0.0492 ± 0.0014 (mainly insensitive to the assumed\n",
            "cosmological model). 3. A rapid and late reionization history is implied, with a\n",
            "midpoint at zmid = 7.00 ± 0.12 and a duration of Δz50 = 1.12 ± 0.29. 4. The\n",
            "constraints from DESI BAO and CMB data imply a larger optical depth (τ = 0.094 ±\n",
            "0.011), which is in tension with the Lyman-α bound. 5. Alternative cosmological\n",
            "models, such as those with dynamical dark energy or non-standard reionization\n",
            "scenarios, can reconcile some but not all constraints.  These findings suggest\n",
            "that there are tensions between different observational datasets and theoretical\n",
            "models, highlighting the need for further research to understand the early\n",
            "universe and its evolution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyp67cDvh1ee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}